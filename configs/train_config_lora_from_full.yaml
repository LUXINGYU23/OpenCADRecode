# CAD-Recode v2.0 LoRA从Full模型微调配置文件
# 从已训练的full模型开始进行LoRA微调

# 模型配置 - 使用已训练的full模型作为基座
base_model_name: "checkpoints_qwen3_sft"  # 使用已训练的full模型路径
model_save_path: "checkpoints_qwen3_lora_from_full"

# 数据配置
train_data_path: "data/train"
val_data_path: "data/val" 
max_seq_length: 1024  # NUM_POINT_TOKENS(256) + MAX_CODE_TOKENS(768)
n_points: 256

# 训练配置 - LoRA微调通常需要更少的步数和较低的学习率
max_steps: 1000  # 减少步数，因为是在已训练模型基础上微调
batch_size: 10    # LoRA可以使用更大的batch size
gradient_accumulation_steps: 2
learning_rate: 0.0001  # 降低学习率，避免破坏已学到的知识
weight_decay: 0.01
warmup_steps: 500  # 减少warmup步数

# 评估和保存配置
eval_steps: 100
save_steps: 500
logging_steps: 50
seed: 42

# 硬件配置
device: "auto"
mixed_precision: "bf16"
num_workers: 4

# 数据增强配置 - 轻微的数据增强
noise_probability: 0.3  # 降低噪声概率
noise_std: 0.005       # 降低噪声强度

# LoRA配置 - 针对微调优化
use_lora: true
lora_r: 16              # 适中的LoRA秩
lora_alpha: 32          # alpha = 2 * r
lora_dropout: 0.05      # 较低的dropout，保持稳定性
lora_target_modules:    # 精确选择关键模块
  - "q_proj"
  - "k_proj" 
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# 自动合并配置
auto_merge_lora: true
auto_merge_final: true
keep_lora_only: true
keep_lora_final: true
merge_final_only: false

# 量化配置 (禁用，因为使用已训练模型)
use_qlora: false
load_in_4bit: false
load_in_8bit: false

# 实验跟踪配置
experiment_name: "cad-recode-lora-from-full-qwen3"
use_swanlab: true

# 特殊配置：从full模型开始
load_from_full_model: true  # 标识这是从full模型开始的LoRA训练
freeze_base_model: false    # 不冻结基础模型，允许LoRA微调

# 训练策略说明：
# 1. 使用已训练的full模型作为起点，避免从头开始
# 2. 较低的学习率确保不会破坏已学到的知识
# 3. 较少的训练步数，重点是微调而非重新训练
# 4. 精选的LoRA目标模块，专注于最重要的组件
# 5. 轻微的数据增强，保持训练稳定性
