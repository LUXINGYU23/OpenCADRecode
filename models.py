#!/usr/bin/env python3
"""
CAD-Recode æ¨¡å‹å®šä¹‰æ¨¡å—
åŒ…å«ç‚¹äº‘ç¼–ç å™¨å’ŒCAD-Recodeå¤šæ¨¡æ€æ¨¡å‹å®šä¹‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union, Any

from transformers import (
    AutoModelForCausalLM,
    AutoConfig,
    PreTrainedModel,
    Qwen3ForCausalLM,  
    Qwen2ForCausalLM,  
    AutoModel,
    AutoTokenizer
)
from transformers.modeling_outputs import CausalLMOutputWithPast

# å¯¼å…¥Qwen2Modelç”¨äºå…¼å®¹æ€§
try:
    from transformers.models.qwen2.modeling_qwen2 import Qwen2Model
except ImportError:
    print("Warning: Could not import Qwen2Model, falling back to AutoModel")
    Qwen2Model = None

try:
    from transformers.models.qwen3.modeling_qwen3 import Qwen3Model
except ImportError:
    print("Warning: Could not import Qwen3Model, using Qwen3ForCausalLM.model instead")
    Qwen3Model = None

from utils import NUM_POINT_TOKENS


class FourierPointEncoder(nn.Module):
    """å‚…é‡Œå¶ç‚¹äº‘ç¼–ç å™¨ - ä¸å®˜æ–¹demo.ipynbå®Œå…¨å¯¹é½"""
    def __init__(self, hidden_size: int):
        super().__init__()
        frequencies = 2.0 ** torch.arange(8, dtype=torch.float32)
        self.register_buffer('frequencies', frequencies, persistent=False)
        self.projection = nn.Linear(51, hidden_size)

    def forward(self, points: torch.Tensor) -> torch.Tensor:
        """
        Args:
            points: (batch_size, num_points, 3)
        Returns:
            encoded_points: (batch_size, num_points, hidden_size)
        """
        if self.projection.weight.dtype != points.dtype:
            points = points.to(self.projection.weight.dtype)
        x = points
        x = (x.unsqueeze(-1) * self.frequencies).view(*x.shape[:-1], -1)
        x = torch.cat((points, x.sin(), x.cos()), dim=-1)
        x = self.projection(x)
        return x


class CADRecode(Qwen3ForCausalLM):
    
    def __init__(self, config):
        # ğŸš¨ å…³é”®ä¿®å¤ï¼šæŒ‰ç…§å®˜æ–¹demoçš„åˆå§‹åŒ–æ–¹å¼
        # é¦–å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(config)
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        self._hidden_size = config.hidden_size
        self.num_point_tokens = NUM_POINT_TOKENS
        
        # ğŸš¨ å…³é”®ä¿®å¤ï¼šæŒ‰ç…§å®˜æ–¹demoçš„æ–¹å¼åˆ›å»ºç‚¹äº‘ç¼–ç å™¨
        # å®˜æ–¹demoï¼štorch.set_default_dtype(torch.float32) -> åˆ›å»ºç¼–ç å™¨ -> torch.set_default_dtype(torch.bfloat16)
        current_dtype = torch.get_default_dtype()
        torch.set_default_dtype(torch.float32)
        self.point_encoder = FourierPointEncoder(config.hidden_size)
        torch.set_default_dtype(torch.bfloat16)  # è®¾ç½®ä¸ºbfloat16ï¼Œä¸å®˜æ–¹demoä¸€è‡´

    def enable_input_require_grads(self):
        super().enable_input_require_grads()
        self.point_encoder.requires_grad_(True)

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        point_cloud: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # past_key_values is None or past_key_values.get_seq_length() == 0
        if past_key_values is None or (hasattr(past_key_values, 'get_seq_length') and past_key_values.get_seq_length() == 0):
            assert inputs_embeds is None, "ä¸èƒ½åŒæ—¶æä¾›inputs_embedså’Œpoint_cloud"
            
            # 1. è·å–æ–‡æœ¬tokençš„åµŒå…¥
            inputs_embeds = self.model.embed_tokens(input_ids)
            if inputs_embeds.dtype != torch.bfloat16:
                inputs_embeds = inputs_embeds.to(torch.bfloat16)
            # 2. è·å–ç‚¹äº‘åµŒå…¥ - æŒ‰ç…§å®˜æ–¹demoçš„æ–¹å¼
            if point_cloud is not None:
                point_embeds = self.point_encoder(point_cloud)
            
                point_embeds = point_embeds.bfloat16()        
                # 3. æ›¿æ¢è¾“å…¥åµŒå…¥ä¸­çš„ç‚¹äº‘ä½ç½® 
                inputs_embeds[attention_mask == -1] = point_embeds.reshape(-1, point_embeds.shape[2])
            
            # 4. è°ƒæ•´attention_maskï¼ˆå°†-1æ›¿æ¢ä¸º1ï¼‰
            attention_mask[attention_mask == -1] = 1
            input_ids = None 
            position_ids = None  

        # --- ä¼ é€’ç»™Qwenæ¨¡å‹ ---
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )

        # --- è®¡ç®—logitså’Œloss ---
        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)
        logits = logits.float()  # ğŸš¨ å®˜æ–¹demoæ˜¾å¼è½¬æ¢ä¸ºfloat

        loss = None
        if labels is not None:
            # æ ‡å‡†çš„å› æœè¯­è¨€æ¨¡å‹æŸå¤±
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, point_cloud=None, attention_mask=None, **kwargs):
        """ä¸ºç”Ÿæˆå‡†å¤‡è¾“å…¥ - æŒ‰ç…§å®˜æ–¹demoçš„æ–¹å¼"""
        model_inputs = super().prepare_inputs_for_generation(
            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, attention_mask=attention_mask, **kwargs
        )
        model_inputs['point_cloud'] = kwargs.get('point_cloud', point_cloud)
        
        return model_inputs


def create_model_and_tokenizer(config):
    """åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨"""
    from transformers import AutoTokenizer
    
    print(f"Loading model and tokenizer from {config.base_model_name}")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        config.base_model_name,
        pad_token='<|im_end|>',
        padding_side='left',
        trust_remote_code=True
    )
    
    # åŠ è½½æ¨¡å‹é…ç½®
    model_config = AutoConfig.from_pretrained(config.base_model_name, trust_remote_code=True)
    
    # ç¦ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
    if hasattr(model_config, 'sliding_window'):
        model_config.sliding_window = None
    
    # åˆ›å»ºæ¨¡å‹
    model = CADRecode.from_pretrained(
        config.base_model_name,
        config=model_config,
        torch_dtype=torch.bfloat16 if config.mixed_precision == "bf16" else torch.float16,
        trust_remote_code=True
    )
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    model.gradient_checkpointing_enable()
    
    return model, tokenizer
