#!/usr/bin/env python3
"""
é«˜æ•ˆæ‰¹é‡é¢„ç¼“å­˜ç‚¹äº‘æ•°æ®è„šæœ¬
é‡‡ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼Œé€šè¿‡éšæœºåŒ–å’Œè¿­ä»£é‡å¯æœºåˆ¶ç­›é€‰é”™è¯¯æ ·æœ¬

ä¸»è¦åŠŸèƒ½ï¼š
1. å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæ— å•æ ·æœ¬è¿›ç¨‹éš”ç¦»
2. éšæœºåŒ–æ ·æœ¬é¡ºåºï¼Œæ¯æ¬¡è¿è¡Œé‡æ–°æ‰“ä¹±
3. è¿­ä»£é‡å¯æœºåˆ¶ï¼Œå´©æºƒæ—¶è®°å½•ç¬¬ä¸€ä¸ªå¤±è´¥æ ·æœ¬
4. è‡ªåŠ¨è·³è¿‡å·²ç¼“å­˜å’Œå·²çŸ¥é”™è¯¯çš„æ ·æœ¬
5. æ”¯æŒå¤„ç†éªŒè¯é›†çš„æ‰¹é‡å­æ–‡ä»¶å¤¹ç»“æ„

ä½¿ç”¨æ–¹æ³•:
python precache_point_clouds_efficient.py --config configs/train_config.yaml
python precache_point_clouds_efficient.py --train_data_path data/train --val_data_path data/val
"""

import os
import sys
import json
import yaml
import argparse
import logging
import warnings
import time
import random
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor, as_completed
import traceback

import numpy as np
from tqdm import tqdm

# å¯¼å…¥è®­ç»ƒé…ç½®å’Œå·¥å…·å‡½æ•°
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from train_cad_recode import TrainingConfig, setup_logger

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

@dataclass
class PrecacheConfig:
    """é¢„ç¼“å­˜é…ç½®"""
    train_data_path: str = "./data/train"
    val_data_path: str = "./data/val"
    n_points: int = 256
    max_workers: int = 8  # è¿›ç¨‹æ•°é‡
    timeout: int = 15  # å•ä¸ªæ ·æœ¬çš„è¶…æ—¶æ—¶é—´
    force_recache: bool = False  # æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆç¼“å­˜
    log_level: str = "INFO"
    max_retries: int = 10  # æœ€å¤§é‡è¯•æ¬¡æ•°


def execute_cadquery_code(code_file: str, n_points: int) -> Tuple[Optional[np.ndarray], Optional[str]]:
    """æ‰§è¡ŒCadQueryä»£ç ç”Ÿæˆç‚¹äº‘ï¼ˆé«˜æ•ˆç‰ˆæœ¬ï¼Œæ— è¿›ç¨‹éš”ç¦»ï¼‰"""
    try:
        import cadquery as cq
        import trimesh
        import torch
        from pytorch3d.ops import sample_farthest_points
        
        # è¯»å–ä»£ç æ–‡ä»¶
        if not os.path.exists(code_file):
            return None, f"code_file_not_found|{code_file}"
        
        with open(code_file, 'r', encoding='utf-8') as f:
            code_content = f.read().strip()
        
        if not code_content:
            return None, "empty_code_file"
        
        # æ‰§è¡Œä»£ç 
        exec_globals = {
            'cadquery': cq,
            'cq': cq,
            '__builtins__': __builtins__
        }
        
        exec(code_content, exec_globals)
        
        if 'r' not in exec_globals or exec_globals['r'] is None:
            return None, "no_result_variable"
            
        compound = exec_globals['r'].val()
        
        # ç”Ÿæˆä¸´æ—¶STEPæ–‡ä»¶
        temp_step_path = f"/tmp/temp_model_{os.getpid()}_{hash(code_content) % 10000}.step"
        
        try:
            # å¯¼å‡ºSTEPæ–‡ä»¶
            cq.exporters.export(compound, temp_step_path)
            
            # è½¬æ¢ä¸ºç‚¹äº‘
            if temp_step_path.lower().endswith('.stl'):
                mesh = trimesh.load_mesh(temp_step_path)
            else:
                try:
                    mesh = trimesh.load_mesh(temp_step_path, force='mesh')
                except:
                    # tessellation fallback
                    mesh_data = compound.tessellate(0.1)
                    if len(mesh_data[0]) > 0:
                        vertices = np.array([(v.x, v.y, v.z) for v in mesh_data[0]])
                        faces = np.array(mesh_data[1])
                        mesh = trimesh.Trimesh(vertices=vertices, faces=faces)
                    else:
                        return None, "tessellation_failed"
            
            # æ ‡å‡†åŒ–ç½‘æ ¼
            if not mesh.is_empty and mesh.vertices is not None and len(mesh.vertices) > 0:
                # ç§»åŠ¨åˆ°åŸç‚¹
                mesh.apply_translation(-(mesh.bounds[0] + mesh.bounds[1]) / 2.0)
                # ç¼©æ”¾åˆ°å•ä½ç«‹æ–¹ä½“
                if max(mesh.extents) > 0:
                    mesh.apply_scale(2.0 / max(mesh.extents))
                
                # é‡‡æ ·ç‚¹äº‘
                vertices, _ = trimesh.sample.sample_surface(mesh, 8192)
                
                # ä½¿ç”¨æœ€è¿œç‚¹é‡‡æ ·
                if len(vertices) >= n_points:
                    vertices_tensor = torch.tensor(vertices, dtype=torch.float32).unsqueeze(0)
                    _, ids = sample_farthest_points(vertices_tensor, K=n_points)
                    ids = ids[0].numpy()
                    selected_points = vertices[ids]
                else:
                    indices = np.random.choice(len(vertices), n_points, replace=True)
                    selected_points = vertices[indices]
                
                return selected_points.astype(np.float32), None
            else:
                return None, "empty_mesh"
                
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_step_path):
                os.remove(temp_step_path)
                
    except Exception as e:
        return None, f"{type(e).__name__}: {str(e)[:200]}"


def process_single_sample(args: Tuple[str, str, str, int, bool]) -> Tuple[str, bool, str]:
    """å¤„ç†å•ä¸ªæ ·æœ¬ï¼ˆé«˜æ•ˆç‰ˆæœ¬ï¼‰"""
    sample_id, code_file, cache_file, n_points, force_recache = args
    
    try:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å·²æœ‰ç¼“å­˜
        if not force_recache and os.path.exists(cache_file):
            try:
                # éªŒè¯ç¼“å­˜æ–‡ä»¶çš„æœ‰æ•ˆæ€§
                cached_pc = np.load(cache_file)
                if cached_pc.shape == (n_points, 3):
                    return sample_id, True, "cached"
                else:
                    # ç¼“å­˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ é™¤é‡æ–°ç”Ÿæˆ
                    os.remove(cache_file)
            except Exception as e:
                # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤é‡æ–°ç”Ÿæˆ
                if os.path.exists(cache_file):
                    os.remove(cache_file)
                return sample_id, False, f"corrupted_cache|{type(e).__name__}: {str(e)[:100]}"
        
        # æ‰§è¡ŒCadQueryä»£ç 
        point_cloud, error_msg = execute_cadquery_code(code_file, n_points)
        
        if point_cloud is not None:
            # ä¿å­˜ç‚¹äº‘
            os.makedirs(os.path.dirname(cache_file), exist_ok=True)
            np.save(cache_file, point_cloud)
            return sample_id, True, "generated"
        else:
            return sample_id, False, error_msg
            
    except Exception as e:
        return sample_id, False, f"unexpected_error|{type(e).__name__}: {str(e)[:100]}"


def load_data_index(data_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½æ•°æ®ç´¢å¼• - æ”¯æŒéªŒè¯é›†çš„æ‰¹é‡å­æ–‡ä»¶å¤¹"""
    data_path = Path(data_path)
    
    # æ”¯æŒå¤šç§ç´¢å¼•æ–‡ä»¶å
    possible_index_files = [
        data_path / "train_index.json",
        data_path / "val_index.json", 
        data_path / "test_index.json",
        data_path / "index.json"
    ]
    
    for index_file in possible_index_files:
        if index_file.exists():
            try:
                with open(index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"Warning: Failed to load {index_file}: {e}")
                continue
    
    # å¦‚æœæ²¡æœ‰ç´¢å¼•æ–‡ä»¶ï¼Œå°è¯•è‡ªåŠ¨æ‰«æ
    print(f"No index file found, scanning directory: {data_path}")
    data_items = []
    
    if data_path.exists():
        # æ£€æŸ¥æ˜¯å¦æ˜¯éªŒè¯é›†ç»“æ„ï¼ˆæœ‰batch_xxå­æ–‡ä»¶å¤¹ï¼‰
        batch_dirs = [d for d in data_path.iterdir() if d.is_dir() and d.name.startswith('batch_')]
        
        if batch_dirs:
            # éªŒè¯é›†ç»“æ„ï¼šæ‰«ææ‰€æœ‰batch_xxå­æ–‡ä»¶å¤¹
            print(f"Found {len(batch_dirs)} batch directories")
            for batch_dir in batch_dirs:
                for code_file in batch_dir.rglob("*.py"):
                    if code_file.is_file():
                        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä½œä¸ºsample_id
                        relative_path = code_file.relative_to(data_path)
                        sample_id = str(relative_path.with_suffix(''))
                        data_items.append({
                            "id": sample_id,
                            "code_file": str(code_file)
                        })
        else:
            # å¸¸è§„ç»“æ„ï¼šç›´æ¥æ‰«æç›®å½•
            for code_file in data_path.rglob("*.py"):
                if code_file.is_file():
                    sample_id = code_file.stem
                    data_items.append({
                        "id": sample_id,
                        "code_file": str(code_file)
                    })
    
    return data_items


class EfficientPrecacheManager:
    """é«˜æ•ˆé¢„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: PrecacheConfig):
        self.config = config
        self.logger = setup_logger()
        self.logger.setLevel(getattr(logging, config.log_level.upper()))
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.error_samples = set()
        self.retry_count = 0
    
    def _get_cache_dir_and_error_file(self, data_path: str) -> Tuple[Path, Path]:
        """è·å–ç¼“å­˜ç›®å½•å’Œé”™è¯¯æ–‡ä»¶è·¯å¾„"""
        data_path = Path(data_path)
        cache_dir = data_path / "point_cloud_cache"
        cache_dir.mkdir(exist_ok=True)
        error_file = cache_dir / "error_samples.json"
        return cache_dir, error_file
    
    def _load_error_samples(self, error_file: Path):
        """åŠ è½½å·²çŸ¥é”™è¯¯æ ·æœ¬åˆ—è¡¨"""
        if error_file.exists():
            try:
                with open(error_file, 'r') as f:
                    error_list = json.load(f)
                    self.error_samples = set(error_list)
                self.logger.info(f"Loaded {len(self.error_samples)} known error samples")
            except Exception as e:
                self.logger.warning(f"Failed to load error samples: {e}")
                self.error_samples = set()
    
    def _save_error_samples(self, error_file: Path):
        """ä¿å­˜é”™è¯¯æ ·æœ¬åˆ—è¡¨"""
        try:
            with open(error_file, 'w') as f:
                json.dump(list(self.error_samples), f, indent=2)
            self.logger.info(f"Saved {len(self.error_samples)} error samples to {error_file}")
        except Exception as e:
            self.logger.warning(f"Failed to save error samples: {e}")
    
    def _get_processing_stats(self, data_items: List[Dict[str, Any]], cache_dir: Path) -> Dict[str, int]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        total_samples = len(data_items)
        cached_samples = 0
        error_samples = len(self.error_samples)
        
        # è®¡ç®—å·²ç¼“å­˜çš„æ ·æœ¬æ•°
        for item in data_items:
            sample_id = item.get('sample_id', item.get('id', 'unknown'))
            
            # ç¼“å­˜æ–‡ä»¶è·¯å¾„ - å°†è·¯å¾„åˆ†éš”ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œç›´æ¥æ”¾åœ¨ç¼“å­˜ç›®å½•ä¸‹
            cache_filename = sample_id.replace('/', '_').replace('\\', '_') + '.npy'
            cache_file = cache_dir / cache_filename
            
            if cache_file.exists():
                try:
                    cached_pc = np.load(cache_file)
                    if cached_pc.shape == (self.config.n_points, 3):
                        cached_samples += 1
                except:
                    pass
        
        remaining_samples = total_samples - cached_samples - error_samples
        
        return {
            "total": total_samples,
            "cached": cached_samples,
            "error": error_samples,
            "remaining": remaining_samples,
            "completion_rate": (cached_samples / total_samples) * 100 if total_samples > 0 else 0
        }
    
    def _prepare_tasks(self, data_items: List[Dict[str, Any]], data_path: str, cache_dir: Path) -> List[Tuple[str, str, str, int, bool]]:
        """å‡†å¤‡å¤„ç†ä»»åŠ¡åˆ—è¡¨"""
        tasks = []
        
        for item in data_items:
            sample_id = item.get('sample_id', item.get('id', 'unknown'))
            code_file = item.get('code_path', item.get('code_file', ''))
            
            # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•æ„å»ºç›¸å¯¹è·¯å¾„
            if not os.path.exists(code_file):
                relative_path = item.get('relative_path', '')
                if relative_path:
                    code_file = os.path.join(data_path, relative_path)
            
            # è·³è¿‡å·²çŸ¥é”™è¯¯æ ·æœ¬
            if sample_id in self.error_samples:
                self.logger.debug(f"Skipping known error sample: {sample_id}")
                continue
            
            # ç¼“å­˜æ–‡ä»¶è·¯å¾„ - å°†è·¯å¾„åˆ†éš”ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œç›´æ¥æ”¾åœ¨ç¼“å­˜ç›®å½•ä¸‹
            cache_filename = sample_id.replace('/', '_').replace('\\', '_') + '.npy'
            cache_file = cache_dir / cache_filename
            
            tasks.append((
                sample_id,
                code_file,
                str(cache_file),
                self.config.n_points,
                self.config.force_recache
            ))
        
        return tasks
    
    def _process_tasks_batch(self, tasks: List[Tuple[str, str, str, int, bool]], split_name: str) -> Tuple[Dict[str, int], List[Tuple[str, str]]]:
        """å¤„ç†ä¸€æ‰¹ä»»åŠ¡"""
        results = {"total": len(tasks), "successful": 0, "cached": 0, "failed": 0}
        failed_samples = []
        first_error_recorded = False  # æ ‡è®°æ˜¯å¦å·²è®°å½•ç¬¬ä¸€ä¸ªé”™è¯¯
        
        if not tasks:
            return results, failed_samples
        
        # éšæœºæ‰“ä¹±ä»»åŠ¡é¡ºåº
        random.shuffle(tasks)
        
        self.logger.info(f"Processing {len(tasks)} samples (retry {self.retry_count + 1}/{self.config.max_retries})")
        
        with ProcessPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_task = {executor.submit(process_single_sample, task): task for task in tasks}
            
            # å¤„ç†ç»“æœ
            with tqdm(total=len(tasks), desc=f"Precaching {split_name}") as pbar:
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    sample_id = task[0]
                    
                    try:
                        sample_id, success, message = future.result()
                        
                        if success:
                            if message == "cached":
                                results["cached"] += 1
                            else:
                                results["successful"] += 1
                            pbar.set_postfix({
                                "Success": results["successful"], 
                                "Cached": results["cached"],
                                "Failed": results["failed"]
                            })
                        else:
                            results["failed"] += 1
                            # åªè®°å½•ç¬¬ä¸€ä¸ªé”™è¯¯æ ·æœ¬
                            if not first_error_recorded:
                                failed_samples.append((sample_id, message))
                                first_error_recorded = True
                                self.logger.info(f"Recording first error sample: {sample_id}")
                            pbar.set_postfix({
                                "Success": results["successful"], 
                                "Cached": results["cached"],
                                "Failed": results["failed"]
                            })
                            
                    except Exception as e:
                        results["failed"] += 1
                        # åªè®°å½•ç¬¬ä¸€ä¸ªé”™è¯¯æ ·æœ¬
                        if not first_error_recorded:
                            failed_samples.append((sample_id, f"process_error: {e}"))
                            first_error_recorded = True
                            self.logger.info(f"Recording first error sample: {sample_id}")
                    
                    pbar.update(1)
        
        return results, failed_samples
    
    def precache_dataset(self, data_path: str, split_name: str) -> Dict[str, int]:
        """é¢„ç¼“å­˜å•ä¸ªæ•°æ®é›†ï¼ˆæ”¯æŒè¿­ä»£é‡å¯ï¼‰"""
        self.logger.info(f"Starting precache for {split_name} dataset: {data_path}")
        
        # è·å–ç¼“å­˜ç›®å½•å’Œé”™è¯¯æ–‡ä»¶
        cache_dir, error_file = self._get_cache_dir_and_error_file(data_path)
        self._load_error_samples(error_file)
        
        # åŠ è½½æ•°æ®ç´¢å¼•
        data_items = load_data_index(data_path)
        if not data_items:
            self.logger.warning(f"No data items found in {data_path}")
            return {"total": 0, "successful": 0, "cached": 0, "failed": 0}
        
        self.logger.info(f"Found {len(data_items)} samples in {split_name} dataset")
        
        # è·å–åˆå§‹ç»Ÿè®¡ä¿¡æ¯
        initial_stats = self._get_processing_stats(data_items, cache_dir)
        self.logger.info(f"Initial status: {initial_stats['cached']} cached, {initial_stats['error']} errors, {initial_stats['remaining']} remaining")
        self.logger.info(f"Current completion rate: {initial_stats['completion_rate']:.1f}%")
        
        # è¿­ä»£é‡å¯å¤„ç†
        overall_results = {"total": len(data_items), "successful": 0, "cached": 0, "failed": 0}
        
        for retry in range(self.config.max_retries):
            self.retry_count = retry
            
            # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨ï¼ˆè·³è¿‡å·²çŸ¥é”™è¯¯æ ·æœ¬ï¼‰
            tasks = self._prepare_tasks(data_items, data_path, cache_dir)
            
            if not tasks:
                self.logger.info(f"No more tasks to process for {split_name}")
                break
            
            self.logger.info(f"Retry {retry + 1}/{self.config.max_retries}: Processing {len(tasks)} remaining samples")
            
            try:
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_results, failed_samples = self._process_tasks_batch(tasks, split_name)
                
                # æ›´æ–°æ€»ä½“ç»“æœ
                overall_results["successful"] += batch_results["successful"]
                overall_results["cached"] += batch_results["cached"]
                overall_results["failed"] += batch_results["failed"]
                
                # å¦‚æœæ²¡æœ‰å¤±è´¥æ ·æœ¬ï¼Œå®Œæˆå¤„ç†
                if not failed_samples:
                    self.logger.info(f"All samples processed successfully for {split_name}")
                    break
                
                # è®°å½•å¤±è´¥æ ·æœ¬ï¼ˆæ¯æ¬¡é‡å¯åªè®°å½•ç¬¬ä¸€ä¸ªï¼‰
                for sample_id, error_msg in failed_samples:
                    self.error_samples.add(sample_id)
                    self.logger.info(f"Added error sample to blacklist: {sample_id} - {error_msg}")
                
                # ä¿å­˜é”™è¯¯æ ·æœ¬åˆ—è¡¨
                self._save_error_samples(error_file)
                
                self.logger.info(f"Retry {retry + 1} completed: {batch_results['successful']} successful, {batch_results['cached']} cached, {batch_results['failed']} failed")
                self.logger.info(f"Added {len(failed_samples)} error sample(s) to blacklist (first error only per retry)")
                
                # å¦‚æœå¤±è´¥æ ·æœ¬è¿‡å¤šï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç­–ç•¥
                if batch_results['failed'] > batch_results['successful'] + batch_results['cached']:
                    self.logger.warning(f"High failure rate in retry {retry + 1}, consider adjusting timeout or worker count")
                
            except Exception as e:
                self.logger.error(f"Retry {retry + 1} crashed: {e}")
                # å¦‚æœè¿›ç¨‹æ± å´©æºƒï¼Œè®°å½•å½“å‰å¤„ç†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ä¸ºé”™è¯¯æ ·æœ¬
                if tasks:
                    first_sample_id = tasks[0][0]
                    self.error_samples.add(first_sample_id)
                    self._save_error_samples(error_file)
                    self.logger.info(f"Added potentially problematic sample to error list: {first_sample_id}")
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
                time.sleep(2)
                continue
        
        # ä¿å­˜æœ€ç»ˆå¤±è´¥æ ·æœ¬æ—¥å¿—
        if self.error_samples:
            fail_log_file = cache_dir / f"failed_samples_final.json"
            try:
                with open(fail_log_file, 'w') as f:
                    json.dump(list(self.error_samples), f, indent=2)
                self.logger.info(f"Saved final failed samples to: {fail_log_file}")
            except Exception as e:
                self.logger.warning(f"Failed to save final failed samples: {e}")
        
        # ç»Ÿè®¡æŠ¥å‘Š
        total = overall_results["total"]
        successful = overall_results["successful"]
        cached = overall_results["cached"]
        failed = overall_results["failed"]
        
        self.logger.info(f"\n{split_name.upper()} Dataset Precaching Results:")
        self.logger.info(f"  Total samples: {total}")
        self.logger.info(f"  Successfully generated: {successful} ({successful/total*100:.1f}%)")
        self.logger.info(f"  Already cached: {cached} ({cached/total*100:.1f}%)")
        self.logger.info(f"  Failed: {failed} ({failed/total*100:.1f}%)")
        self.logger.info(f"  Error samples blacklisted: {len(self.error_samples)}")
        self.logger.info(f"  Cache directory: {cache_dir}")
        self.logger.info(f"  Retries completed: {self.retry_count + 1}/{self.config.max_retries}")
        
        # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
        if failed > 0:
            remaining_failed = failed - len(self.error_samples)
            if remaining_failed > 0:
                self.logger.info(f"  Note: {remaining_failed} samples failed but were not blacklisted (only first error per retry is recorded)")
                self.logger.info(f"  Recommendation: Run the script again to continue processing remaining samples")
        
        return overall_results
    
    def precache_all(self) -> Dict[str, Dict[str, int]]:
        """é¢„ç¼“å­˜æ‰€æœ‰æ•°æ®é›†"""
        self.logger.info("Starting efficient precache process for all datasets...")
        self.logger.info(f"Configuration: {self.config}")
        
        all_results = {}
        
        # å¤„ç†è®­ç»ƒæ•°æ®
        if os.path.exists(self.config.train_data_path):
            train_results = self.precache_dataset(self.config.train_data_path, "train")
            all_results["train"] = train_results
        else:
            self.logger.warning(f"Train data path not found: {self.config.train_data_path}")
        
        # å¤„ç†éªŒè¯æ•°æ®
        if os.path.exists(self.config.val_data_path):
            val_results = self.precache_dataset(self.config.val_data_path, "val")
            all_results["val"] = val_results
        else:
            self.logger.warning(f"Val data path not found: {self.config.val_data_path}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_samples = sum(results["total"] for results in all_results.values())
        total_successful = sum(results["successful"] for results in all_results.values())
        total_cached = sum(results["cached"] for results in all_results.values())
        total_failed = sum(results["failed"] for results in all_results.values())
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info("OVERALL EFFICIENT PRECACHING RESULTS:")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"Total samples processed: {total_samples}")
        self.logger.info(f"Successfully generated: {total_successful} ({total_successful/total_samples*100:.1f}%)")
        self.logger.info(f"Already cached: {total_cached} ({total_cached/total_samples*100:.1f}%)")
        self.logger.info(f"Failed: {total_failed} ({total_failed/total_samples*100:.1f}%)")
        self.logger.info(f"Error samples: {len(self.error_samples)}")
        self.logger.info(f"{'='*60}")
        
        return all_results


def main():
    parser = argparse.ArgumentParser(description="é«˜æ•ˆé¢„ç¼“å­˜CAD-Recodeè®­ç»ƒæ•°æ®çš„ç‚¹äº‘")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--train_data_path", type=str, default="./data/train", help="è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--val_data_path", type=str, default="./data/val", help="éªŒè¯æ•°æ®è·¯å¾„")
    parser.add_argument("--n_points", type=int, default=256, help="ç‚¹äº‘ç‚¹æ•°")
    parser.add_argument("--max_workers", type=int, default=8, help="æœ€å¤§å¹¶è¡Œè¿›ç¨‹æ•°")
    parser.add_argument("--timeout", type=int, default=15, help="å•ä¸ªæ ·æœ¬è¶…æ—¶æ—¶é—´(ç§’)")
    parser.add_argument("--force_recache", action="store_true", help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰ç¼“å­˜")
    parser.add_argument("--log_level", type=str, default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"])
    parser.add_argument("--max_retries", type=int, default=10, help="æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    if args.config and os.path.exists(args.config):
        # ä»è®­ç»ƒé…ç½®æ–‡ä»¶åŠ è½½
        with open(args.config, 'r') as f:
            train_config_dict = yaml.safe_load(f)
        
        # åˆ›å»ºé¢„ç¼“å­˜é…ç½®
        config = PrecacheConfig(
            train_data_path=train_config_dict.get('train_data_path', args.train_data_path),
            val_data_path=train_config_dict.get('val_data_path', args.val_data_path),
            n_points=train_config_dict.get('n_points', args.n_points),
            max_workers=args.max_workers,
            timeout=args.timeout,
            force_recache=args.force_recache,
            log_level=args.log_level,
            max_retries=args.max_retries
        )
    else:
        # ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»º
        config = PrecacheConfig(
            train_data_path=args.train_data_path,
            val_data_path=args.val_data_path,
            n_points=args.n_points,
            max_workers=args.max_workers,
            timeout=args.timeout,
            force_recache=args.force_recache,
            log_level=args.log_level,
            max_retries=args.max_retries
        )
    
    # è®¾ç½®éšæœºç§å­
    random.seed(int(time.time()))
    
    # æ‰§è¡Œé¢„ç¼“å­˜
    try:
        manager = EfficientPrecacheManager(config)
        results = manager.precache_all()
        
        print("\nğŸ‰ é«˜æ•ˆé¢„ç¼“å­˜å®Œæˆï¼")
        print("ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒï¼Œæ‰€æœ‰æœ‰æ•ˆçš„ç‚¹äº‘æ•°æ®å·²é¢„å…ˆç”Ÿæˆã€‚")
        print("ç¼“å­˜ä¿å­˜åœ¨å„æ•°æ®é›†ç›®å½•çš„ point_cloud_cache å­ç›®å½•ä¸­")
        print("é”™è¯¯æ ·æœ¬å·²è¢«è®°å½•å¹¶å°†åœ¨æœªæ¥çš„è®­ç»ƒä¸­è·³è¿‡ã€‚")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâŒ é¢„ç¼“å­˜è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ é¢„ç¼“å­˜è¿‡ç¨‹å‡ºé”™: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    print("ğŸš€ CAD-Recode é«˜æ•ˆç‚¹äº‘é¢„ç¼“å­˜å·¥å…·")
    print("="*60)
    exit(main())
