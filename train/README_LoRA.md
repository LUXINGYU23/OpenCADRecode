# CAD-Recode LoRA è®­ç»ƒä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

CAD-Recode LoRAç‰ˆæœ¬æä¾›äº†å†…å­˜å‹å¥½çš„æ¨¡å‹å¾®è°ƒæ–¹æ¡ˆï¼Œæ”¯æŒåœ¨è¾ƒå°æ˜¾å­˜ç¯å¢ƒä¸­è®­ç»ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚

## ä¸»è¦ç‰¹æ€§

- ğŸš€ **å†…å­˜æ•ˆç‡**: LoRAåªè®­ç»ƒå°‘é‡å‚æ•°(<1%)ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜éœ€æ±‚
- âš¡ **è®­ç»ƒé€Ÿåº¦**: æ›´å¿«çš„è®­ç»ƒå’Œæ”¶æ•›é€Ÿåº¦
- ğŸ’¾ **å­˜å‚¨ä¼˜åŒ–**: åªéœ€ä¿å­˜LoRAæƒé‡(å‡ MB vs å‡ GB)
- ğŸ”„ **çµæ´»æ€§**: å¯ä»¥è½»æ¾åˆ‡æ¢å’Œåˆå¹¶ä¸åŒçš„é€‚é…å™¨
- ğŸ¯ **QLoRAæ”¯æŒ**: æ”¯æŒ4ä½é‡åŒ–ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜éœ€æ±‚

## æ–‡ä»¶ç»“æ„

```
â”œâ”€â”€ train_cad_recode_lora.py      # LoRAè®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference_lora.py             # LoRAæ¨ç†è„šæœ¬  
â”œâ”€â”€ merge_lora_weights.py         # LoRAæƒé‡åˆå¹¶è„šæœ¬
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ train_config_lora.yaml    # æ ‡å‡†LoRAé…ç½®
â”‚   â””â”€â”€ train_config_qlora.yaml   # QLoRAé…ç½®(ä½æ˜¾å­˜)
```

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

ç¡®ä¿å®‰è£…äº†ä»¥ä¸‹å…³é”®ä¾èµ–ï¼š
- `peft>=0.7.0` (LoRAæ”¯æŒ)
- `bitsandbytes>=0.41.0` (é‡åŒ–æ”¯æŒ)
- `accelerate>=0.24.0` (è®­ç»ƒåŠ é€Ÿ)

### 2. æ ‡å‡†LoRAè®­ç»ƒ

é€‚ç”¨äº16GB+æ˜¾å­˜ç¯å¢ƒï¼š

```bash
python train_cad_recode_lora.py --config configs/train_config_lora.yaml
```

### 3. QLoRAè®­ç»ƒ(ä½æ˜¾å­˜)

é€‚ç”¨äº12GBåŠä»¥ä¸‹æ˜¾å­˜ç¯å¢ƒï¼š

```bash
python train_cad_recode_lora.py --config configs/train_config_qlora.yaml
```

æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼š

```bash
python train_cad_recode_lora.py --config configs/train_config_lora.yaml --use_qlora
```

### 4. LoRAæ¨ç†

ä½¿ç”¨è®­ç»ƒå¥½çš„LoRAæƒé‡è¿›è¡Œæ¨ç†ï¼š

```bash
# ä½¿ç”¨ç‚¹äº‘æ–‡ä»¶æ¨ç†
python inference_lora.py \
    --base_model Qwen/Qwen3-1.7B-Base \
    --lora_path checkpoints_qwen3_lora \
    --point_cloud_file data/test/sample.npy

# ä½¿ç”¨ç¤ºä¾‹ç‚¹äº‘æ¨ç†
python inference_lora.py \
    --base_model Qwen/Qwen3-1.7B-Base \
    --lora_path checkpoints_qwen3_lora \
    --shape cube \
    --output generated_code.py
```

### 5. åˆå¹¶LoRAæƒé‡

è®­ç»ƒå®Œæˆåï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆä¸¤ç§å½¢å¼çš„æ¨¡å‹ï¼š

#### è‡ªåŠ¨åˆå¹¶ï¼ˆæ¨èï¼‰

è®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨åˆå¹¶LoRAæƒé‡ï¼š

```bash
# è®­ç»ƒå®Œæˆåä¼šç”Ÿæˆï¼š
checkpoints_qwen3_lora/
â”œâ”€â”€ adapter_config.json          # LoRAé…ç½®
â”œâ”€â”€ adapter_model.safetensors    # LoRAæƒé‡
â”œâ”€â”€ tokenizer_config.json        # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ config.yaml                  # è®­ç»ƒé…ç½®
â””â”€â”€ merged_model/                # ğŸ¯ è‡ªåŠ¨åˆå¹¶çš„å®Œæ•´æ¨¡å‹
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ ...
```

#### æ‰‹åŠ¨åˆå¹¶

å¦‚æœéœ€è¦æ‰‹åŠ¨åˆå¹¶LoRAæƒé‡ï¼š

```bash
python merge_lora_weights.py \
    --base_model Qwen/Qwen3-1.7B-Base \
    --lora_path checkpoints_qwen3_lora \
    --output_path merged_model
```

#### æ¨ç†æ—¶è‡ªåŠ¨æ£€æµ‹

æ¨ç†è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä¼˜å…ˆä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹ï¼š

```bash
# ä¼šè‡ªåŠ¨ä½¿ç”¨ checkpoints_qwen3_lora/merged_modelï¼ˆå¦‚æœå­˜åœ¨ï¼‰
python inference_lora.py \
    --base_model Qwen/Qwen3-1.7B-Base \
    --lora_path checkpoints_qwen3_lora \
    --shape cube
```

### 6. æ¨¡å‹ç»“æ„

è®­ç»ƒåçš„ç›®å½•ç»“æ„ï¼š

```
checkpoints_qwen3_lora/
â”œâ”€â”€ ğŸ“ LoRAæƒé‡æ–‡ä»¶
â”‚   â”œâ”€â”€ adapter_config.json      # LoRAé€‚é…å™¨é…ç½®
â”‚   â”œâ”€â”€ adapter_model.safetensors # LoRAæƒé‡(é€šå¸¸å‡ MB)
â”‚   â””â”€â”€ tokenizer_config.json    # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ ğŸ“ åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ (è‡ªåŠ¨ç”Ÿæˆ)
â”‚   â””â”€â”€ merged_model/
â”‚       â”œâ”€â”€ config.json          # æ¨¡å‹é…ç½®
â”‚       â”œâ”€â”€ model.safetensors    # å®Œæ•´æ¨¡å‹æƒé‡(å‡ GB)
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â””â”€â”€ generation_config.json
â””â”€â”€ ğŸ“„ è®­ç»ƒé…ç½®
    â””â”€â”€ config.yaml              # è®­ç»ƒæ—¶çš„é…ç½®å‚æ•°
```

**ä¼˜åŠ¿ï¼š**
- ğŸ”„ **çµæ´»æ€§**: ä¿ç•™LoRAæƒé‡ä¾¿äºåç»­å¾®è°ƒ
- âš¡ **æ¨ç†é€Ÿåº¦**: åˆå¹¶æ¨¡å‹æ— éœ€åŠ¨æ€åŠ è½½é€‚é…å™¨
- ğŸ’¾ **å­˜å‚¨ä¼˜åŒ–**: LoRAæƒé‡æ–‡ä»¶å°ï¼Œä¾¿äºåˆ†å‘
- ğŸ¯ **è‡ªåŠ¨åŒ–**: æ— éœ€æ‰‹åŠ¨æ“ä½œï¼Œè®­ç»ƒå®Œæˆå³å¯ä½¿ç”¨

## é…ç½®è¯´æ˜

### LoRAå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `lora_r` | 16 | LoRAç§©ï¼Œæ§åˆ¶å‚æ•°é‡å’Œæ€§èƒ½å¹³è¡¡ |
| `lora_alpha` | 32 | LoRAç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ä¸º2*lora_r |
| `lora_dropout` | 0.1 | LoRA dropoutç‡ |
| `lora_target_modules` | [q_proj, k_proj, v_proj, ...] | ç›®æ ‡æ¨¡å—åˆ—è¡¨ |

### è‡ªåŠ¨åˆå¹¶é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `auto_merge_lora` | true | æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨åˆå¹¶æƒé‡ |
| `auto_merge_final` | true | æ˜¯å¦åœ¨æœ€ç»ˆä¿å­˜æ—¶åˆå¹¶æƒé‡ |
| `keep_lora_only` | true | è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¦ä¿ç•™LoRAæƒé‡æ–‡ä»¶ |
| `keep_lora_final` | true | æœ€ç»ˆä¿å­˜æ˜¯å¦ä¿ç•™LoRAæƒé‡æ–‡ä»¶ |
| `merge_final_only` | false | æ˜¯å¦åªåœ¨æœ€ç»ˆä¿å­˜æ—¶åˆå¹¶ï¼ˆèŠ‚çœè®­ç»ƒæ—¶é—´ï¼‰ |

### QLoRAå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `use_qlora` | false | æ˜¯å¦å¯ç”¨QLoRA |
| `load_in_4bit` | false | 4ä½é‡åŒ– |
| `bnb_4bit_compute_dtype` | "bfloat16" | è®¡ç®—æ•°æ®ç±»å‹ |
| `bnb_4bit_quant_type` | "nf4" | é‡åŒ–ç±»å‹ |

## æ˜¾å­˜éœ€æ±‚ä¼°ç®—

| é…ç½® | æ¨¡å‹å¤§å° | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒå‚æ•°é‡ |
|------|----------|----------|------------|
| Qwen3-1.7B + LoRA | 1.7B | ~8GB | ~0.8M (0.05%) |
| Qwen3-1.7B + QLoRA | 1.7B | ~6GB | ~0.8M (0.05%) |
| Qwen2-7B + LoRA | 7B | ~16GB | ~3.1M (0.04%) |
| Qwen2-7B + QLoRA | 7B | ~12GB | ~3.1M (0.04%) |

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„é…ç½®

- **æ˜¾å­˜ â‰¥ 16GB**: ä½¿ç”¨æ ‡å‡†LoRAé…ç½®
- **æ˜¾å­˜ < 16GB**: ä½¿ç”¨QLoRAé…ç½®
- **æ˜¾å­˜ < 12GB**: è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é™ä½batch size

### 2. LoRAå‚æ•°è°ƒä¼˜

- **é«˜æ€§èƒ½**: `r=64, alpha=128`
- **å¹³è¡¡**: `r=16, alpha=32` (æ¨è)
- **ä½å‚æ•°**: `r=8, alpha=16`

### 3. è®­ç»ƒç­–ç•¥

- **å­¦ä¹ ç‡**: LoRAé€šå¸¸ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡(2e-4 ~ 5e-4)
- **Batch Size**: LoRAå¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch size
- **è®­ç»ƒæ­¥æ•°**: LoRAæ”¶æ•›æ›´å¿«ï¼Œå¯ä»¥å‡å°‘è®­ç»ƒæ­¥æ•°

### 4. æ¨¡å‹é€‰æ‹©

- **å¿«é€Ÿå®éªŒ**: Qwen3-1.7B
- **æ›´å¥½æ€§èƒ½**: Qwen2-7B (éœ€è¦æ›´å¤šæ˜¾å­˜)
- **å¹³è¡¡æ–¹æ¡ˆ**: Qwen2.5-3B

## æ•…éšœæ’é™¤

### 1. æ˜¾å­˜ä¸è¶³

```bash
# é™ä½batch size
per_device_train_batch_size: 4

# å¢åŠ æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps: 8

# å¯ç”¨QLoRA
use_qlora: true
```

### 2. è®­ç»ƒä¸æ”¶æ•›

```bash
# å¢åŠ LoRAç§©
lora_r: 32
lora_alpha: 64

# è°ƒæ•´å­¦ä¹ ç‡
learning_rate: 0.0001

# å¢åŠ warmupæ­¥æ•°
warmup_steps: 2000
```

### 3. æ¨ç†é€Ÿåº¦æ…¢

```bash
# åˆå¹¶LoRAæƒé‡
python merge_lora_weights.py --base_model ... --lora_path ... --output_path merged_model

# ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œæ¨ç†
```

## é«˜çº§ç”¨æ³•

### 1. å¤šLoRAé€‚é…å™¨

å¯ä»¥ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒå¤šä¸ªLoRAé€‚é…å™¨ï¼š

```python
# åŠ è½½ä¸åŒçš„LoRAé€‚é…å™¨
model.load_adapter("task1_lora", adapter_name="task1")
model.load_adapter("task2_lora", adapter_name="task2")

# åˆ‡æ¢é€‚é…å™¨
model.set_adapter("task1")
```

### 2. å¢é‡è®­ç»ƒ

åœ¨å·²æœ‰LoRAåŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼š

```bash
python train_cad_recode_lora.py \
    --config configs/train_config_lora.yaml \
    --resume_from_checkpoint checkpoints_qwen3_lora/checkpoint-10000
```

### 3. æ¨¡å‹è¯„ä¼°

```bash
# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
python train_cad_recode_lora.py \
    --config configs/train_config_lora.yaml \
    --do_eval \
    --eval_steps 1000
```

## å‚è€ƒèµ„æº

- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRAè®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [PEFTåº“æ–‡æ¡£](https://huggingface.co/docs/peft)
- [BitsAndBytesæ–‡æ¡£](https://github.com/TimDettmers/bitsandbytes)
