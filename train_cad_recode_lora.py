#!/usr/bin/env python3
"""
CAD-Recode LoRAå¾®è°ƒè®­ç»ƒè„šæœ¬ v2.0
æ”¯æŒä»ç‚¹äº‘ç”ŸæˆCadQueryä»£ç çš„å¤šæ¨¡æ€æ¨¡å‹LoRAå¾®è°ƒ

ä¸»è¦ç‰¹æ€§ï¼š
1. åŸºäºPEFTåº“çš„LoRAå¾®è°ƒ
2. æ”¯æŒåˆ‡æ¢åŸºåº§æ¨¡å‹ï¼ˆQwen2/Qwen3ç­‰ï¼‰
3. ä½¿ç”¨æ ‡å‡†Trainerè¿›è¡Œç›‘ç£å¾®è°ƒ
4. é›†æˆSwanLabè¿›è¡Œå®éªŒè·Ÿè¸ª
5. å†…å­˜å‹å¥½çš„LoRAè®­ç»ƒæ–¹æ¡ˆ
6. æ”¯æŒQLoRAï¼ˆé‡åŒ–LoRAï¼‰

ä½¿ç”¨æ–¹æ³•:
python train_cad_recode_lora.py --config configs/train_config_lora.yaml
"""

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import warnings
import argparse
import yaml
from typing import Tuple
from pathlib import Path

import torch
from transformers import TrainingArguments, Trainer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils import TrainingConfig, setup_environment, set_random_seeds
from models import create_model_and_tokenizer
from data_utils import CADRecodeDataset, DataCollatorForCADRecode

# å¯¼å…¥SwanLabç”¨äºå®éªŒè·Ÿè¸ª
try:
    import swanlab
    SWANLAB_AVAILABLE = True
except ImportError:
    SWANLAB_AVAILABLE = False
    print("Warning: SwanLab not installed. Install with: pip install swanlab")

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


class LoRATrainingConfig(TrainingConfig):
    """LoRAè®­ç»ƒé…ç½®ç±»ï¼Œç»§æ‰¿åŸºç¡€é…ç½®å¹¶æ·»åŠ LoRAç‰¹å®šé…ç½®"""
    
    def __init__(self, **kwargs):
        # LoRAç‰¹å®šå‚æ•°åˆ—è¡¨
        lora_specific_params = {
            'use_lora', 'lora_r', 'lora_alpha', 'lora_dropout', 'lora_target_modules',
            'use_qlora', 'load_in_4bit', 'load_in_8bit', 'bnb_4bit_compute_dtype',
            'bnb_4bit_use_double_quant', 'bnb_4bit_quant_type',
            'auto_merge_lora', 'auto_merge_final', 'keep_lora_only', 
            'keep_lora_final', 'merge_final_only', 'load_from_full_model', 'freeze_base_model'
        }
        
        # åˆ†ç¦»åŸºç¡€é…ç½®å’ŒLoRAé…ç½®
        base_kwargs = {k: v for k, v in kwargs.items() if k not in lora_specific_params}
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(**base_kwargs)
        
        # LoRAé…ç½®
        self.use_lora: bool = kwargs.get('use_lora', True)
        self.lora_r: int = kwargs.get('lora_r', 16)
        self.lora_alpha: int = kwargs.get('lora_alpha', 32)
        self.lora_dropout: float = kwargs.get('lora_dropout', 0.1)
        self.lora_target_modules: list = kwargs.get('lora_target_modules', [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ])
        
        # é‡åŒ–é…ç½® (QLoRA)
        self.use_qlora: bool = kwargs.get('use_qlora', False)
        self.load_in_4bit: bool = kwargs.get('load_in_4bit', False)
        self.load_in_8bit: bool = kwargs.get('load_in_8bit', False)
        self.bnb_4bit_compute_dtype: str = kwargs.get('bnb_4bit_compute_dtype', "bfloat16")
        self.bnb_4bit_use_double_quant: bool = kwargs.get('bnb_4bit_use_double_quant', True)
        self.bnb_4bit_quant_type: str = kwargs.get('bnb_4bit_quant_type', "nf4")
        
        # è‡ªåŠ¨åˆå¹¶é…ç½®
        self.auto_merge_lora: bool = kwargs.get('auto_merge_lora', True)
        self.auto_merge_final: bool = kwargs.get('auto_merge_final', True)
        self.keep_lora_only: bool = kwargs.get('keep_lora_only', True)
        self.keep_lora_final: bool = kwargs.get('keep_lora_final', True)
        self.merge_final_only: bool = kwargs.get('merge_final_only', False)
        
        # ä»fullæ¨¡å‹è®­ç»ƒçš„ç‰¹æ®Šé…ç½®
        self.load_from_full_model: bool = kwargs.get('load_from_full_model', False)
        self.freeze_base_model: bool = kwargs.get('freeze_base_model', False)


def create_quantization_config(config: LoRATrainingConfig):
    """åˆ›å»ºé‡åŒ–é…ç½®"""
    if not (config.use_qlora or config.load_in_4bit or config.load_in_8bit):
        return None
    
    if config.load_in_4bit:
        compute_dtype = getattr(torch, config.bnb_4bit_compute_dtype)
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_use_double_quant=config.bnb_4bit_use_double_quant,
            bnb_4bit_quant_type=config.bnb_4bit_quant_type,
        )
    elif config.load_in_8bit:
        return BitsAndBytesConfig(load_in_8bit=True)
    
    return None


def create_lora_model(model, tokenizer, config: LoRATrainingConfig):
    """åˆ›å»ºLoRAæ¨¡å‹"""
    if not config.use_lora:
        return model
    
    # å¦‚æœä½¿ç”¨é‡åŒ–ï¼Œå‡†å¤‡æ¨¡å‹
    if config.use_qlora or config.load_in_4bit or config.load_in_8bit:
        model = prepare_model_for_kbit_training(model)
    
    # ç¡®ä¿ç‚¹äº‘ç¼–ç å™¨éœ€è¦æ¢¯åº¦
    if hasattr(model, 'point_encoder'):
        for param in model.point_encoder.parameters():
            param.requires_grad = True
        print("âœ… Point encoder gradients enabled")
        
        # ç¡®ä¿ç‚¹äº‘ç¼–ç å™¨æ•°æ®ç±»å‹ä¸æ¨¡å‹ä¸€è‡´
        model_dtype = next(model.parameters()).dtype
        if model.point_encoder.projection.weight.dtype != model_dtype:
            model.point_encoder = model.point_encoder.to(model_dtype)
            print(f"âœ… Point encoder dtype set to {model_dtype}")
    
    # é…ç½®LoRA - æ·»åŠ ç‚¹äº‘ç¼–ç å™¨åˆ°ç›®æ ‡æ¨¡å—
    target_modules = config.lora_target_modules.copy()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ç‚¹äº‘ç¼–ç å™¨æ¨¡å—
    if hasattr(model, 'point_encoder'):
        print("ğŸ¯ Adding point encoder to LoRA target modules")
        # æ·»åŠ ç‚¹äº‘ç¼–ç å™¨çš„çº¿æ€§å±‚
        if hasattr(model.point_encoder, 'projection'):
            target_modules.append("point_encoder.projection")
    
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        target_modules=target_modules,
        lora_dropout=config.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=["point_encoder"],  # ä¿å­˜ç‚¹äº‘ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    # å†æ¬¡ç¡®ä¿ç‚¹äº‘ç¼–ç å™¨éœ€è¦æ¢¯åº¦ï¼ˆPEFTå¯èƒ½ä¼šå½±å“ï¼‰
    if hasattr(model.base_model, 'model') and hasattr(model.base_model.model, 'point_encoder'):
        for param in model.base_model.model.point_encoder.parameters():
            param.requires_grad = True
        print("âœ… Point encoder gradients re-enabled after LoRA")
    elif hasattr(model, 'point_encoder'):
        for param in model.point_encoder.parameters():
            param.requires_grad = True
        print("âœ… Point encoder gradients re-enabled after LoRA (direct access)")
    
    # å¯ç”¨è¾“å…¥æ¢¯åº¦ï¼ˆå¯¹äºCADRecodeæ¨¡å‹ï¼‰
    if hasattr(model, 'enable_input_require_grads'):
        model.enable_input_require_grads()
        print("âœ… Input gradients enabled")
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    model.print_trainable_parameters()
    
    return model


def create_lora_model_and_tokenizer(config: LoRATrainingConfig):
    """åˆ›å»ºLoRAæ¨¡å‹å’Œåˆ†è¯å™¨"""
    from transformers import AutoTokenizer, AutoConfig
    from models import CADRecode
    import os
    
    print(f"Loading model and tokenizer from {config.base_model_name}")
    
    # æ£€æŸ¥æ˜¯å¦ä»fullæ¨¡å‹åŠ è½½
    is_local_model = os.path.exists(config.base_model_name) and os.path.isdir(config.base_model_name)
    load_from_full = getattr(config, 'load_from_full_model', False) or is_local_model
    
    if load_from_full:
        print(f"ğŸ”„ Loading from trained full model: {config.base_model_name}")
        
        # ä»æœ¬åœ°å·²è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            config.base_model_name,
            pad_token='<|im_end|>',
            padding_side='left',
            trust_remote_code=True
        )
        
        # ä»æœ¬åœ°åŠ è½½æ¨¡å‹é…ç½®
        model_config = AutoConfig.from_pretrained(config.base_model_name, trust_remote_code=True)
        
        # ç¦ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
        if hasattr(model_config, 'sliding_window'):
            model_config.sliding_window = None
        
        # åˆ›å»ºé‡åŒ–é…ç½®
        quantization_config = create_quantization_config(config)
        
        # ç¡®å®šæ•°æ®ç±»å‹
        if quantization_config:
            torch_dtype = torch.float16  # é‡åŒ–æ—¶ä½¿ç”¨fp16
        else:
            torch_dtype = torch.bfloat16 if config.mixed_precision == "bf16" else torch.float16
        
        # ä»æœ¬åœ°å·²è®­ç»ƒæ¨¡å‹åŠ è½½
        model = CADRecode.from_pretrained(
            config.base_model_name,
            config=model_config,
            torch_dtype=torch_dtype,
            quantization_config=quantization_config,
            trust_remote_code=True
        )
        
        print(f"âœ… Successfully loaded trained CADRecode model from {config.base_model_name}")
        
    else:
        print(f"ğŸ”„ Loading base model: {config.base_model_name}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            config.base_model_name,
            pad_token='<|im_end|>',
            padding_side='left',
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹é…ç½®
        model_config = AutoConfig.from_pretrained(config.base_model_name, trust_remote_code=True)
        
        # ç¦ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
        if hasattr(model_config, 'sliding_window'):
            model_config.sliding_window = None
        
        # åˆ›å»ºé‡åŒ–é…ç½®
        quantization_config = create_quantization_config(config)
        
        # ç¡®å®šæ•°æ®ç±»å‹
        if quantization_config:
            torch_dtype = torch.float16  # é‡åŒ–æ—¶ä½¿ç”¨float16
        else:
            torch_dtype = torch.bfloat16 if config.mixed_precision == "bf16" else torch.float16
        
        # åˆ›å»ºæ¨¡å‹
        model = CADRecode.from_pretrained(
            config.base_model_name,
            config=model_config,
            torch_dtype=torch_dtype,
            quantization_config=quantization_config,
            trust_remote_code=True
        )
    
    # åº”ç”¨LoRA
    model = create_lora_model(model, tokenizer, config)
    
    # ä¸å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
    # if not quantization_config:
    #     model.gradient_checkpointing_enable()
    
    return model, tokenizer


def train_model(config: LoRATrainingConfig):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®ç¯å¢ƒå’Œéšæœºç§å­
    setup_environment()
    set_random_seeds(config.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.model_save_path, exist_ok=True)
    
    # åˆå§‹åŒ–SwanLab
    if config.use_swanlab and SWANLAB_AVAILABLE:
        swanlab.init(
            project="cad-recode-lora",
            experiment_name=config.experiment_name,
            config=config.__dict__
        )
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = create_lora_model_and_tokenizer(config)
    
    # è°ƒè¯•æ¨¡å‹æ¢¯åº¦çŠ¶æ€
    debug_model_gradients(model, "LoRA Model")
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print("Model structure:")
    print(model)
    
    # åˆ›å»ºæ•°æ®é›†
    print("Loading datasets...")
    train_dataset = CADRecodeDataset(config.train_data_path, tokenizer, config, "train")
    val_dataset = CADRecodeDataset(config.val_data_path, tokenizer, config, "val") if config.val_data_path else None
    
    # åˆ›å»ºæ•°æ®collator
    data_collator = DataCollatorForCADRecode(
        tokenizer=tokenizer,
        max_length=config.max_seq_length,
        pad_to_multiple_of=8  # å¯é€‰ï¼šæé«˜æ•ˆç‡
    )
    
    # é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.model_save_path,
        max_steps=config.max_steps,
        per_device_train_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        warmup_steps=config.warmup_steps,
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        bf16=config.mixed_precision == "bf16" and not (config.load_in_4bit or config.load_in_8bit),
        fp16=config.mixed_precision == "fp16" or (config.load_in_4bit or config.load_in_8bit),
        logging_steps=config.logging_steps,
        eval_steps=config.eval_steps,
        save_steps=config.save_steps,
        eval_strategy="steps" if val_dataset else "no",
        save_strategy="steps",
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        remove_unused_columns=False,  # ä¿ç•™point_cloudåˆ—
        gradient_checkpointing=False,  # ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
        dataloader_num_workers=config.num_workers,
        save_total_limit=3,
        report_to=["none"],  # æˆ‘ä»¬ä½¿ç”¨SwanLab
        ddp_find_unused_parameters=False,  # LoRAè®­ç»ƒä¼˜åŒ–
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # æ·»åŠ SwanLabå›è°ƒ
    if config.use_swanlab and SWANLAB_AVAILABLE:
        from transformers import TrainerCallback
        
        class SwanLabCallback(TrainerCallback):
            def on_log(self, args, state, control, logs=None, **kwargs):
                if logs:
                    swanlab.log(logs)
        
        trainer.add_callback(SwanLabCallback())
    
    # æ·»åŠ LoRAè‡ªåŠ¨åˆå¹¶å›è°ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if config.use_lora and hasattr(config, 'auto_merge_lora') and config.auto_merge_lora:
        from utils import LoRACheckpointCallback
        from transformers import TrainerCallback
        
        class LoRAMergeCallback(TrainerCallback):
            def __init__(self, base_model_name, auto_merge=True, keep_lora_only=True, merge_final_only=False):
                self.base_model_name = base_model_name
                self.auto_merge = auto_merge
                self.keep_lora_only = keep_lora_only
                self.merge_final_only = merge_final_only
            
            def on_save(self, args, state, control, **kwargs):
                # å¦‚æœè®¾ç½®äº†åªåœ¨æœ€ç»ˆä¿å­˜æ—¶åˆå¹¶ï¼Œä¸”å½“å‰ä¸æ˜¯æœ€ç»ˆä¿å­˜ï¼Œåˆ™è·³è¿‡
                is_final = (state.global_step >= args.max_steps)
                if self.merge_final_only and not is_final:
                    return
                
                if self.auto_merge:
                    from utils import merge_lora_weights_to_model
                    from pathlib import Path
                    
                    model = kwargs.get('model')
                    if model and hasattr(model, 'merge_and_unload'):
                        output_dir = Path(args.output_dir) / f"checkpoint-{state.global_step}"
                        merged_dir = output_dir / "merged_model"
                        
                        try:
                            print(f"ğŸ”„ Auto-merging LoRA weights for checkpoint-{state.global_step}...")
                            merge_lora_weights_to_model(
                                peft_model=model,
                                base_model_name=self.base_model_name,
                                save_path=str(merged_dir)
                            )
                            print(f"âœ… Merged model saved to: {merged_dir}")
                        except Exception as e:
                            print(f"âŒ Error merging LoRA weights: {e}")
        
        lora_callback = LoRAMergeCallback(
            base_model_name=config.base_model_name,
            auto_merge=config.auto_merge_lora,
            keep_lora_only=getattr(config, 'keep_lora_only', True),
            merge_final_only=getattr(config, 'merge_final_only', False)
        )
        
        trainer.add_callback(lora_callback)
    
    # å¼€å§‹è®­ç»ƒ
    print("Starting LoRA training...")
    trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("Saving final LoRA model...")
    if config.use_lora:
        # ç›´æ¥ä¿å­˜LoRAæƒé‡
        trainer.save_model()
        
        # å¦‚æœé…ç½®äº†è‡ªåŠ¨åˆå¹¶ï¼Œè¿›è¡Œæœ€ç»ˆåˆå¹¶
        auto_merge = getattr(config, 'auto_merge_final', True)
        if auto_merge:
            from utils import merge_lora_weights_to_model
            from pathlib import Path
            
            merged_dir = Path(config.model_save_path) / "merged_model"
            try:
                print(f"ğŸ”„ Creating final merged model...")
                merge_lora_weights_to_model(
                    peft_model=model,
                    base_model_name=config.base_model_name,
                    save_path=str(merged_dir)
                )
                print(f"âœ… Final merged model saved to: {merged_dir}")
            except Exception as e:
                print(f"âŒ Error creating final merged model: {e}")
        
        # é¢å¤–ä¿å­˜tokenizerï¼ˆç¡®ä¿åœ¨ä¸»ç›®å½•ï¼‰
        tokenizer.save_pretrained(config.model_save_path)
        
        print(f"âœ… LoRA weights saved to: {config.model_save_path}")
    else:
        # å¦‚æœä¸ä½¿ç”¨LoRAï¼Œä¿å­˜å®Œæ•´æ¨¡å‹
        trainer.save_model()
        tokenizer.save_pretrained(config.model_save_path)
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(config.model_save_path, "config.yaml"), 'w') as f:
        yaml.dump(config.__dict__, f)
    
    print("LoRA training completed!")
    
    # æ‰“å°è®­ç»ƒåçš„å‚æ•°ç»Ÿè®¡
    if config.use_lora:
        print("\nTrainable parameters after training:")
        model.print_trainable_parameters()
        
        # æ‰“å°ä¿å­˜çš„æ–‡ä»¶ä¿¡æ¯
        save_path = Path(config.model_save_path)
        lora_files = list(save_path.glob("adapter_*"))
        merged_path = save_path / "merged_model"
        
        print(f"\nğŸ“ Saved files:")
        print(f"  LoRA weights: {len(lora_files)} files in {save_path}")
        if merged_path.exists():
            merged_files = list(merged_path.glob("*.safetensors")) + list(merged_path.glob("*.bin"))
            print(f"  Merged model: {len(merged_files)} files in {merged_path}")
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            try:
                total_size = sum(f.stat().st_size for f in merged_files)
                print(f"  Merged model size: {total_size / (1024**3):.2f} GB")
            except:
                pass


def debug_model_gradients(model, name="Model"):
    """è°ƒè¯•æ¨¡å‹æ¢¯åº¦çŠ¶æ€"""
    print(f"\nğŸ” {name} gradient status:")
    
    total_params = 0
    trainable_params = 0
    
    for name, param in model.named_parameters():
        total_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
            if "point_encoder" in name:
                print(f"  âœ… {name}: requires_grad={param.requires_grad}, shape={param.shape}")
        else:
            if "point_encoder" in name:
                print(f"  âŒ {name}: requires_grad={param.requires_grad}, shape={param.shape}")
    
    print(f"  ğŸ“Š Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")
    return trainable_params, total_params


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CAD-Recode LoRA Training Script v2.0")
    parser.add_argument("--config", type=str, required=True, help="Training configuration file")
    parser.add_argument("--base_model", type=str, help="Override base model name")
    parser.add_argument("--experiment_name", type=str, help="Override experiment name")
    parser.add_argument("--lora_r", type=int, help="Override LoRA rank")
    parser.add_argument("--use_qlora", action="store_true", help="Use QLoRA (4-bit quantization)")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config_dict = yaml.safe_load(f)
    
    config = LoRATrainingConfig(**config_dict)
    
    # è¦†ç›–é…ç½®
    if args.base_model:
        config.base_model_name = args.base_model
    if args.experiment_name:
        config.experiment_name = args.experiment_name
    if args.lora_r:
        config.lora_r = args.lora_r
    if args.use_qlora:
        config.use_qlora = True
        config.load_in_4bit = True
    
    print(f"LoRA Training Configuration:")
    print(f"  Base Model: {config.base_model_name}")
    print(f"  Experiment: {config.experiment_name}")
    print(f"  Max Steps: {config.max_steps}")
    print(f"  Batch Size: {config.batch_size}")
    print(f"  Learning Rate: {config.learning_rate}")
    print(f"  Use LoRA: {config.use_lora}")
    if config.use_lora:
        print(f"  LoRA Rank: {config.lora_r}")
        print(f"  LoRA Alpha: {config.lora_alpha}")
        print(f"  LoRA Dropout: {config.lora_dropout}")
        print(f"  Target Modules: {config.lora_target_modules}")
        print(f"  Auto Merge: {config.auto_merge_lora}")
        print(f"  Auto Merge Final: {config.auto_merge_final}")
        print(f"  Merge Final Only: {config.merge_final_only}")
    print(f"  Use QLoRA: {config.use_qlora}")
    if config.use_qlora or config.load_in_4bit:
        print(f"  4-bit Quantization: {config.load_in_4bit}")
        print(f"  Compute Dtype: {config.bnb_4bit_compute_dtype}")
    
    # å¼€å§‹è®­ç»ƒ
    train_model(config)


if __name__ == "__main__":
    main()
