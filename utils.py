#!/usr/bin/env python3
"""
CAD-Recode å·¥å…·å‡½æ•°æ¨¡å—
åŒ…å«è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„å„ç§å·¥å…·å‡½æ•°
"""

import os
import json
import random
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass

# é…ç½®å¸¸é‡
NUM_POINT_TOKENS = 256  # è®ºæ–‡ä¸­æåˆ°çš„ç‚¹äº‘tokenæ•°é‡
MAX_CODE_TOKENS = 768   # ä»£ç tokenæœ€å¤§æ•°é‡
SPECIAL_TOKENS = 2      # <|im_start|> å’Œ <|endoftext|>
MAX_SEQ_LENGTH = NUM_POINT_TOKENS + MAX_CODE_TOKENS + SPECIAL_TOKENS  # æ€»åºåˆ—é•¿åº¦


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    # æ¨¡å‹é…ç½®
    base_model_name: str = "Qwen/Qwen2-1.5B"
    model_save_path: str = "checkpoints_v2"
    
    # æ•°æ®é…ç½®
    train_data_path: str = "data/train"
    val_data_path: str = "data/val"
    max_seq_length: int = MAX_SEQ_LENGTH
    n_points: int = NUM_POINT_TOKENS
    
    # è®­ç»ƒé…ç½®
    max_steps: int = 100000
    batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-4
    weight_decay: float = 0.01
    warmup_steps: int = 1000
    
    # è¯„ä¼°å’Œä¿å­˜é…ç½®
    eval_steps: int = 2000
    save_steps: int = 2000
    logging_steps: int = 50
    seed: int = 42
    
    # ç¡¬ä»¶é…ç½®
    device: str = "auto"
    mixed_precision: str = "bf16"
    num_workers: int = 0
    
    # æ•°æ®å¢å¼ºé…ç½®
    noise_probability: float = 0.5
    noise_std: float = 0.01
    
    # å®éªŒè·Ÿè¸ª
    experiment_name: str = "cad-recode-v2"
    use_swanlab: bool = True


def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'


def set_random_seeds(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)


def load_error_samples(data_root: Path) -> set:
    """åŠ è½½é”™è¯¯æ ·æœ¬åˆ—è¡¨"""
    error_file = data_root / "error_samples.json"
    if error_file.exists():
        with open(error_file, 'r') as f:
            return set(json.load(f))
    return set()


def load_data_index(data_root: Path, split: str) -> List[Dict[str, Any]]:
    """åŠ è½½æ•°æ®ç´¢å¼• - å…¼å®¹æ—§æ ¼å¼"""
    # é¦–å…ˆå°è¯•åŠ è½½splitç‰¹å®šçš„ç´¢å¼•æ–‡ä»¶ï¼ˆæ—§æ ¼å¼ï¼‰
    index_file = data_root / f"{split}_index.json"
    if index_file.exists():
        print(f"Loading index from {index_file}")
        with open(index_file, 'r') as f:
            return json.load(f)
    
    # å¦‚æœæ²¡æœ‰splitç‰¹å®šçš„ç´¢å¼•æ–‡ä»¶ï¼Œå°è¯•é€šç”¨ç´¢å¼•æ–‡ä»¶
    index_file = data_root / "index.json"
    if index_file.exists():
        print(f"Loading index from {index_file}")
        with open(index_file, 'r') as f:
            return json.load(f)
    
    # å¦‚æœæ²¡æœ‰ç´¢å¼•æ–‡ä»¶ï¼Œæ‰«æç›®å½•ç»“æ„
    print(f"No index file found, scanning directory: {data_root}")
    data_list = []
    for batch_dir in data_root.glob("batch_*"):
        if batch_dir.is_dir():
            for py_file in batch_dir.glob("*.py"):
                sample_id = f"{batch_dir.name}_{py_file.stem}"
                data_list.append({
                    "sample_id": sample_id,
                    "code_path": str(py_file),
                    "relative_path": f"{batch_dir.name}/{py_file.name}",
                    "split": split
                })
    return data_list


def get_cached_point_cloud(data_root: Path, sample_id: str, num_points: int) -> Optional[np.ndarray]:
    """ä»ç¼“å­˜è¯»å–ç‚¹äº‘ - æ”¯æŒå…¨å±€ç¼“å­˜ç›®å½•"""
    try:
        # å°è¯•æœ¬åœ°ç¼“å­˜
        cache_file = data_root / "point_cloud_cache" / f"{sample_id}.npy"
        
        if not cache_file.exists():
            # å°è¯•å…¨å±€ç¼“å­˜ç›®å½•
            global_cache = Path("/root/cad-recode/point_cloud_cache") 
            cache_file = global_cache / f"{sample_id}.npy"
        
        if cache_file.exists():
            point_cloud = np.load(cache_file)
            # ç¡®ä¿ç‚¹äº‘å½¢çŠ¶æ­£ç¡®
            if point_cloud.shape[0] != num_points:
                print(f"Warning: cached point cloud for {sample_id} has wrong shape: {point_cloud.shape}")
                return None
            return point_cloud.astype(np.float32)
        else:
            print(f"Warning: no cached point cloud found for {sample_id}")
            return None
            
    except Exception as e:
        print(f"Error loading cached point cloud for {sample_id}: {e}")
        return None


def apply_data_augmentation(point_cloud: np.ndarray, noise_probability: float, noise_std: float, is_training: bool = True) -> np.ndarray:
    """åº”ç”¨æ•°æ®å¢å¼º"""
    if is_training and random.random() < noise_probability:
        # æ·»åŠ é«˜æ–¯å™ªå£°
        noise = np.random.normal(0, noise_std, point_cloud.shape)
        point_cloud = point_cloud + noise
    return point_cloud


def create_default_point_cloud(num_points: int) -> np.ndarray:
    """åˆ›å»ºé»˜è®¤ç‚¹äº‘ï¼ˆå•ä½ç«‹æ–¹ä½“çš„8ä¸ªé¡¶ç‚¹ï¼‰"""
    default_points = np.array([
        [-1, -1, -1], [1, -1, -1], [1, 1, -1], [-1, 1, -1],
        [-1, -1, 1], [1, -1, 1], [1, 1, 1], [-1, 1, 1]
    ], dtype=np.float32)
    
    # æ‰©å±•åˆ°æ‰€éœ€çš„ç‚¹æ•°
    if len(default_points) < num_points:
        # é‡å¤ç‚¹ä»¥è¾¾åˆ°æ‰€éœ€æ•°é‡
        repeat_times = num_points // len(default_points) + 1
        default_points = np.tile(default_points, (repeat_times, 1))[:num_points]
    
    return default_points


def inference_single_sample(model, tokenizer, point_cloud: np.ndarray, max_new_tokens: int = 768) -> str:
    """å•æ ·æœ¬æ¨ç† - ä¸å®˜æ–¹demo.ipynbå®Œå…¨å¯¹é½"""
    device = next(model.parameters()).device
    
    # input_ids = [tokenizer.pad_token_id] * len(point_cloud) + [tokenizer('<|im_start|>')['input_ids'][0]]
    # attention_mask = [-1] * len(point_cloud) + [1]
    
    input_ids = [tokenizer.pad_token_id] * len(point_cloud) + [tokenizer('<|im_start|>')['input_ids'][0]]
    attention_mask = [-1] * len(point_cloud) + [1]
    
    # è½¬ç§»åˆ°è®¾å¤‡
    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)
    attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)
    point_cloud = torch.tensor(point_cloud.astype(np.float32)).unsqueeze(0).to(device)
    
    # ç”Ÿæˆ - ä¸demoä¸€è‡´
    with torch.no_grad():
        batch_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            point_cloud=point_cloud,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.pad_token_id
        )
    
    # è§£ç  - ä¸demoä¸€è‡´
    py_string = tokenizer.batch_decode(batch_ids)[0]
    begin = py_string.find('<|im_start|>') + 12
    end = py_string.find('<|endoftext|>')
    py_string = py_string[begin:end] if end != -1 else py_string[begin:]
    
    return py_string


def merge_lora_weights_to_model(peft_model, base_model_name: str = None, save_path: str = None, 
                               safe_serialization: bool = True, max_shard_size: str = "5GB"):
    """
    åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
    
    Args:
        peft_model: PEFTæ¨¡å‹å®ä¾‹
        base_model_name: åŸºç¡€æ¨¡å‹åç§°ï¼ˆç”¨äºä¿å­˜tokenizerå’Œconfigï¼‰
        save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›åˆå¹¶åçš„æ¨¡å‹
        safe_serialization: æ˜¯å¦ä½¿ç”¨safe_serialization
        max_shard_size: æœ€å¤§åˆ†ç‰‡å¤§å°
    
    Returns:
        merged_model: åˆå¹¶åçš„æ¨¡å‹ï¼ˆå¦‚æœsave_pathä¸ºNoneï¼‰
    """
    try:
        print("ğŸ”„ Starting LoRA weight merging...")
        
        # åˆå¹¶æƒé‡
        merged_model = peft_model.merge_and_unload()
        print("âœ… LoRA weights merged successfully")
        
        if save_path is not None:
            save_path = Path(save_path)
            save_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
            print(f"ğŸ’¾ Saving merged model to: {save_path}")
            merged_model.save_pretrained(
                save_path,
                safe_serialization=safe_serialization,
                max_shard_size=max_shard_size
            )
            
            # å¦‚æœæä¾›äº†åŸºç¡€æ¨¡å‹åç§°ï¼Œä¿å­˜tokenizerå’Œconfig
            if base_model_name:
                try:
                    from transformers import AutoTokenizer, AutoConfig
                    
                    print("ğŸ’¾ Saving tokenizer...")
                    tokenizer = AutoTokenizer.from_pretrained(
                        base_model_name,
                        pad_token='<|im_end|>',
                        padding_side='left',
                        trust_remote_code=True
                    )
                    tokenizer.save_pretrained(save_path)
                    
                    print("ğŸ’¾ Saving config...")
                    config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)
                    if hasattr(config, 'sliding_window'):
                        config.sliding_window = None
                    config.save_pretrained(save_path)
                    
                except Exception as e:
                    print(f"âš ï¸  Warning: Could not save tokenizer/config: {e}")
            
            # æ˜¾ç¤ºä¿å­˜ä¿¡æ¯
            try:
                model_files = list(save_path.glob("*.safetensors")) + list(save_path.glob("*.bin"))
                total_size = sum(f.stat().st_size for f in model_files)
                print(f"ğŸ“Š Merged model size: {total_size / (1024**3):.2f} GB")
                print(f"ğŸ“ Files saved: {len(model_files)} model files")
            except Exception as e:
                print(f"âš ï¸  Could not calculate model size: {e}")
                
            print(f"âœ… Merged model saved successfully to: {save_path}")
            
        return merged_model
        
    except Exception as e:
        print(f"âŒ Error merging LoRA weights: {e}")
        raise e


def save_lora_checkpoint_with_merge(trainer, output_dir: str, base_model_name: str = None, 
                                   auto_merge: bool = True, keep_lora_only: bool = True):
    """
    ä¿å­˜LoRAæ£€æŸ¥ç‚¹å¹¶å¯é€‰æ‹©æ€§åœ°è‡ªåŠ¨åˆå¹¶æƒé‡
    
    Args:
        trainer: Hugging Face Trainerå®ä¾‹
        output_dir: è¾“å‡ºç›®å½•
        base_model_name: åŸºç¡€æ¨¡å‹åç§°
        auto_merge: æ˜¯å¦è‡ªåŠ¨åˆå¹¶æƒé‡
        keep_lora_only: æ˜¯å¦ä¿ç•™LoRAæƒé‡æ–‡ä»¶
    """
    try:
        output_dir = Path(output_dir)
        
        # ä¿å­˜LoRAæƒé‡ - ç›´æ¥è°ƒç”¨åŸå§‹çš„ä¿å­˜æ–¹æ³•
        print(f"ğŸ’¾ Saving LoRA checkpoint to: {output_dir}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸å’Œé…ç½®
        if hasattr(trainer.model, 'save_pretrained'):
            trainer.model.save_pretrained(output_dir)
        
        # ä¿å­˜tokenizer
        if hasattr(trainer, 'tokenizer') and trainer.tokenizer:
            trainer.tokenizer.save_pretrained(output_dir)
        
        if auto_merge and hasattr(trainer.model, 'merge_and_unload'):
            # åˆ›å»ºåˆå¹¶æ¨¡å‹çš„å­ç›®å½•
            merged_dir = output_dir / "merged_model"
            
            print(f"ğŸ”„ Auto-merging LoRA weights...")
            merge_lora_weights_to_model(
                peft_model=trainer.model,
                base_model_name=base_model_name,
                save_path=str(merged_dir)
            )
            
            # å¦‚æœä¸ä¿ç•™LoRAæƒé‡ï¼Œç§»é™¤ç›¸å…³æ–‡ä»¶
            if not keep_lora_only:
                lora_files = ["adapter_config.json", "adapter_model.safetensors", "adapter_model.bin"]
                for lora_file in lora_files:
                    lora_path = output_dir / lora_file
                    if lora_path.exists():
                        lora_path.unlink()
                        print(f"ğŸ—‘ï¸  Removed LoRA file: {lora_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error saving LoRA checkpoint: {e}")
        return False


class LoRACheckpointCallback:
    """LoRAè®­ç»ƒè¿‡ç¨‹ä¸­çš„è‡ªåŠ¨åˆå¹¶å›è°ƒ"""
    
    def __init__(self, base_model_name: str = None, auto_merge: bool = True, 
                 keep_lora_only: bool = True, merge_final_only: bool = False):
        self.base_model_name = base_model_name
        self.auto_merge = auto_merge
        self.keep_lora_only = keep_lora_only
        self.merge_final_only = merge_final_only
    
    def on_save(self, trainer, output_dir: str, is_final: bool = False):
        """åœ¨ä¿å­˜æ—¶è°ƒç”¨"""
        # å¦‚æœè®¾ç½®äº†åªåœ¨æœ€ç»ˆä¿å­˜æ—¶åˆå¹¶ï¼Œä¸”å½“å‰ä¸æ˜¯æœ€ç»ˆä¿å­˜ï¼Œåˆ™è·³è¿‡
        if self.merge_final_only and not is_final:
            return
            
        if self.auto_merge and hasattr(trainer.model, 'merge_and_unload'):
            save_lora_checkpoint_with_merge(
                trainer=trainer,
                output_dir=output_dir,
                base_model_name=self.base_model_name,
                auto_merge=self.auto_merge,
                keep_lora_only=self.keep_lora_only or not is_final  # æœ€ç»ˆä¿å­˜æ—¶å¯ä»¥é€‰æ‹©ä¸ä¿ç•™LoRA
            )
