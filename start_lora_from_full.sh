#!/bin/bash

# CAD-Recode LoRAä»Fullæ¨¡å‹å¾®è°ƒå¯åŠ¨è„šæœ¬
# ä½¿ç”¨å·²è®­ç»ƒçš„fullæ¨¡å‹ä½œä¸ºèµ·ç‚¹è¿›è¡ŒLoRAå¾®è°ƒ

echo "==========================================="
echo "CAD-Recode LoRAä»Fullæ¨¡å‹å¾®è°ƒ"
echo "==========================================="

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
if command -v nvidia-smi &> /dev/null; then
    echo "ğŸ” GPUä¿¡æ¯:"
    nvidia-smi --query-gpu=index,name,memory.total,memory.used --format=csv,noheader,nounits
    echo ""
fi

# æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶å’Œç›®å½•
echo "ğŸ” æ£€æŸ¥è®­ç»ƒç¯å¢ƒ..."

# æ£€æŸ¥å·²è®­ç»ƒçš„fullæ¨¡å‹
FULL_MODEL_PATH="checkpoints_qwen3_sft"
if [ ! -d "$FULL_MODEL_PATH" ]; then
    echo "âŒ æœªæ‰¾åˆ°å·²è®­ç»ƒçš„fullæ¨¡å‹: $FULL_MODEL_PATH"
    echo "è¯·å…ˆå®Œæˆfullæ¨¡å‹è®­ç»ƒ"
    exit 1
fi

# æ£€æŸ¥æ•°æ®ç›®å½•
if [ ! -d "data/train" ] || [ ! -d "data/val" ]; then
    echo "âŒ æ•°æ®ç›®å½•ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ data/train å’Œ data/val"
    exit 1
fi

# æ£€æŸ¥é…ç½®æ–‡ä»¶
CONFIG_FILE="configs/train_config_lora_from_full.yaml"
if [ ! -f "$CONFIG_FILE" ]; then
    echo "âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $CONFIG_FILE"
    exit 1
fi

echo "âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ"
echo ""

# æ˜¾ç¤ºé…ç½®ä¿¡æ¯
echo "ğŸ”§ è®­ç»ƒé…ç½®:"
echo "  Fullæ¨¡å‹è·¯å¾„: $FULL_MODEL_PATH"
echo "  é…ç½®æ–‡ä»¶: $CONFIG_FILE"
echo "  è¾“å‡ºç›®å½•: checkpoints_qwen3_lora_from_full"
echo ""

# è¯¢é—®ç”¨æˆ·ç¡®è®¤
read -p "æ˜¯å¦å¼€å§‹LoRAå¾®è°ƒ? (y/N): " -n 1 -r
echo ""
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "è®­ç»ƒå·²å–æ¶ˆ"
    exit 0
fi

# è®¾ç½®Pythonè·¯å¾„å’Œç¯å¢ƒå˜é‡
export PYTHONPATH="$PWD:$PYTHONPATH"
export TOKENIZERS_PARALLELISM=false

# å¯åŠ¨è®­ç»ƒ
echo "ğŸš€ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ..."
echo "è¯¦ç»†æ—¥å¿—å°†ä¿å­˜åˆ° lora_from_full_training.log"
echo ""

# è¿è¡Œè®­ç»ƒè„šæœ¬å¹¶ä¿å­˜æ—¥å¿—
python train_cad_recode_lora.py \
    --config "$CONFIG_FILE" \
    --experiment_name "cad-recode-lora-from-full-$(date +%Y%m%d_%H%M%S)" \
    2>&1 | tee lora_from_full_training.log

# æ£€æŸ¥è®­ç»ƒç»“æœ
if [ $? -eq 0 ]; then
    echo ""
    echo "ğŸ‰ LoRAå¾®è°ƒè®­ç»ƒå®Œæˆ!"
    echo ""
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    OUTPUT_DIR="checkpoints_qwen3_lora_from_full"
    if [ -d "$OUTPUT_DIR" ]; then
        echo "ğŸ“ è®­ç»ƒè¾“å‡ºç›®å½•: $OUTPUT_DIR"
        echo "   æ–‡ä»¶ç»“æ„:"
        ls -la "$OUTPUT_DIR"
        
        # æ£€æŸ¥åˆå¹¶æ¨¡å‹
        if [ -d "$OUTPUT_DIR/merged_model" ]; then
            echo ""
            echo "âœ… æ‰¾åˆ°åˆå¹¶åçš„æ¨¡å‹: $OUTPUT_DIR/merged_model"
            echo "   å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œæ¨ç†"
        fi
        
        echo ""
        echo "ğŸ§ª è¿è¡Œæ¨ç†æµ‹è¯•:"
        echo "python inference_lora_from_full.py \\"
        echo "    --model_path $OUTPUT_DIR \\"
        echo "    --data_root data \\"
        echo "    --output_dir inference_results_lora_from_full \\"
        echo "    --max_samples 5 \\"
        echo "    --use_merged"
    fi
else
    echo ""
    echo "âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"
    echo "è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: lora_from_full_training.log"
fi
