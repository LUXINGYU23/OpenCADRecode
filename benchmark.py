#!/usr/bin/env python3
"""
CAD-Recode Benchmark Script
å¯¹è®­ç»ƒçš„æ¨¡å‹å’Œå®˜æ–¹æ¨¡å‹åœ¨éªŒè¯æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°å’Œæ¯”è¾ƒ

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½å¤šä¸ªæ¨¡å‹ï¼ˆå®˜æ–¹æ¨¡å‹ + è®­ç»ƒçš„æ¨¡å‹ï¼‰
2. åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè¿›è¡Œæ‰¹é‡æ¨ç†
3. ç”ŸæˆCadQueryä»£ç å’ŒSTL/STEPæ–‡ä»¶
4. è®¡ç®—å‡ ä½•æŒ‡æ ‡ï¼ˆIoUã€Chamferè·ç¦»ç­‰ï¼‰
5. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

æ”¯æŒçš„æ•°æ®é›†æ ¼å¼ï¼š
- legacy: åŸå§‹æ ¼å¼ï¼Œä½¿ç”¨data/valç›®å½•
- fusion360: Fusion360æ•°æ®é›†ï¼Œä½¿ç”¨train_test.jsonæ‹†åˆ†

ä½¿ç”¨æ–¹æ³•:
# ä½¿ç”¨legacyæ ¼å¼ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
python benchmark.py --models official checkpoints_qwen3_sft --num_samples 100

# ä½¿ç”¨fusion360æ•°æ®é›†çš„testæ‹†åˆ†
python benchmark.py --models official checkpoints_qwen3_sft --dataset_type fusion360 --split test --data_path fusion360dataset --train_test_json fusion360dataset/train_test.json --num_samples 100

# ä½¿ç”¨fusion360æ•°æ®é›†çš„trainæ‹†åˆ†è¿›è¡ŒéªŒè¯
python benchmark.py --models checkpoints_qwen3_sft \\
    --dataset_type fusion360 --split train --data_path fusion360dataset \\
    --num_samples 500
"""

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import sys
import json
import argparse
import traceback
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from datetime import datetime
from dataclasses import dataclass, asdict
from collections import defaultdict
import warnings
import shutil
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
import numpy as np
try:
    import OCC
    OCC_AVAILABLE = True
except ImportError:
    OCC_AVAILABLE = False
from tqdm import tqdm

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils import (
    setup_environment, 
    get_cached_point_cloud, 
    inference_single_sample,
    NUM_POINT_TOKENS
)
from models import CADRecode, create_model_and_tokenizer
from data_utils import load_data_index, load_error_samples
from metric import MetricsCalculator, SamplingConfig, MetricsResult
from batch_inference import CadQueryExecutor, BatchInferenceConfig

# å¯¼å…¥å®˜æ–¹æ¨¡å‹å®šä¹‰ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    import cadquery as cq
    CADQUERY_AVAILABLE = True
except ImportError:
    CADQUERY_AVAILABLE = False
    print("Warning: CadQuery not installed. STL generation will be disabled.")


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str
    path: str
    model_type: str  # 'official', 'sft', 'lora', 'lora_from_full'
    base_model: Optional[str] = None  # LoRAæ¨¡å‹éœ€è¦æŒ‡å®šbase model
    

@dataclass
class BenchmarkConfig:
    """Benchmarké…ç½®"""
    models: List[ModelConfig]
    data_path: str = "data/val"
    output_dir: str = "benchmark_results"
    num_samples: Optional[int] = None  # Noneè¡¨ç¤ºå…¨éƒ¨æ ·æœ¬
    batch_size: int = 1
    max_new_tokens: int = 768
    device: str = "auto"
    
    # æ•°æ®é›†ç›¸å…³é…ç½®
    dataset_type: str = "legacy"  # "legacy" or "fusion360" 
    split: str = "val"  # å¯¹äºfusion360: "train" or "test"
    train_test_json: Optional[str] = None  # fusion360æ•°æ®é›†çš„train/testæ‹†åˆ†æ–‡ä»¶
    
    # è¾“å‡ºæ§åˆ¶
    save_code: bool = True
    save_stl: bool = True
    save_step: bool = True
    
    # è¯„ä¼°å‚æ•°
    n_points_metric: int = 2000  # ç”¨äºè®¡ç®—æŒ‡æ ‡çš„ç‚¹æ•°
    cadquery_timeout: int = 30
    skip_failed_generations: bool = True
    
    # å¹¶è¡Œå‚æ•°
    use_multiprocessing: bool = True


class OfficialCADRecode:
    """å®˜æ–¹CAD-Recodeæ¨¡å‹åŒ…è£…ç±»"""
    
    def __init__(self):
        from transformers import AutoTokenizer
        import torch.nn as nn
        from transformers.modeling_outputs import CausalLMOutputWithPast
        from transformers import Qwen2ForCausalLM, Qwen2Model, PreTrainedModel
        
        class FourierPointEncoder(nn.Module):
            def __init__(self, hidden_size):
                super().__init__()
                frequencies = 2.0 ** torch.arange(8, dtype=torch.float32)
                self.register_buffer('frequencies', frequencies, persistent=False)
                self.projection = nn.Linear(51, hidden_size)

            def forward(self, points):
                x = points
                x = (x.unsqueeze(-1) * self.frequencies).view(*x.shape[:-1], -1)
                x = torch.cat((points, x.sin(), x.cos()), dim=-1)
                x = self.projection(x)
                return x
        
        class OfficialCADRecode(Qwen2ForCausalLM):
            def __init__(self, config):
                PreTrainedModel.__init__(self, config)
                self.model = Qwen2Model(config)
                self.vocab_size = config.vocab_size
                self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
                
                torch.set_default_dtype(torch.float32)
                self.point_encoder = FourierPointEncoder(config.hidden_size)
                torch.set_default_dtype(torch.bfloat16)

            def forward(self, input_ids=None, attention_mask=None, point_cloud=None, 
                       position_ids=None, past_key_values=None, inputs_embeds=None, 
                       labels=None, use_cache=None, output_attentions=None, 
                       output_hidden_states=None, return_dict=None, cache_position=None):
                
                output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
                output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
                return_dict = return_dict if return_dict is not None else self.config.use_return_dict

                # concatenate point and text embeddings
                if past_key_values is None or past_key_values.get_seq_length() == 0:
                    assert inputs_embeds is None
                    inputs_embeds = self.model.embed_tokens(input_ids)
                    point_embeds = self.point_encoder(point_cloud).bfloat16()
                    inputs_embeds[attention_mask == -1] = point_embeds.reshape(-1, point_embeds.shape[2])
                    attention_mask[attention_mask == -1] = 1
                    input_ids = None
                    position_ids = None

                # decoder outputs
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    inputs_embeds=inputs_embeds,
                    use_cache=use_cache,
                    output_attentions=output_attentions,
                    output_hidden_states=output_hidden_states,
                    return_dict=return_dict,
                    cache_position=cache_position)

                hidden_states = outputs[0]
                logits = self.lm_head(hidden_states)
                logits = logits.float()

                loss = None
                if labels is not None:
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    loss_fct = nn.CrossEntropyLoss()
                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
                    shift_labels = shift_labels.view(-1)
                    shift_labels = shift_labels.to(shift_logits.device)
                    loss = loss_fct(shift_logits, shift_labels)

                if not return_dict:
                    output = (logits,) + outputs[1:]
                    return (loss,) + output if loss is not None else output

                return CausalLMOutputWithPast(
                    loss=loss,
                    logits=logits,
                    past_key_values=outputs.past_key_values,
                    hidden_states=outputs.hidden_states,
                    attentions=outputs.attentions)

            def prepare_inputs_for_generation(self, *args, **kwargs):
                model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)
                model_inputs['point_cloud'] = kwargs['point_cloud']
                return model_inputs
        
        self.OfficialCADRecode = OfficialCADRecode


class BenchmarkRunner:
    """Benchmarkè¿è¡Œå™¨"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–CadQueryæ‰§è¡Œå™¨
        batch_config = BatchInferenceConfig(
            output_dir=str(self.output_dir),
            cadquery_timeout=config.cadquery_timeout,
            save_code=config.save_code,
            save_stl=config.save_stl,
            save_step=config.save_step
        )
        self.cadquery_executor = CadQueryExecutor(batch_config)
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
        sampling_config = SamplingConfig(n_points=config.n_points_metric)
        self.metrics_calculator = MetricsCalculator(sampling_config=sampling_config)
        
        # åŠ è½½æ•°æ®ç´¢å¼•
        if config.dataset_type == "fusion360":
            self.data_index = self._load_fusion360_data_index()
        else:
            # ä½¿ç”¨åŸæœ‰çš„åŠ è½½æ–¹å¼
            self.data_index = load_data_index(Path(config.data_path), config.split)
        
        self.error_samples = load_error_samples(Path(config.data_path))
        
        # å¤„ç†ç®€å•æ ¼å¼çš„æ•°æ®ç´¢å¼•ï¼ˆvalç›®å½•ç›´æ¥åŒ…å«.pyæ–‡ä»¶çš„æƒ…å†µï¼‰
        if not self.data_index:
            print("No index found, scanning val directory for .py files...")
            data_path = Path(config.data_path)
            self.data_index = []
            for py_file in sorted(data_path.glob("*.py")):
                sample_id = py_file.stem  # æ–‡ä»¶åå»æ‰.pyåç¼€
                self.data_index.append({
                    "id": sample_id,
                    "py_file": str(py_file),
                    "code_path": str(py_file)  # å…¼å®¹ä¸¤ç§æ ¼å¼
                })
        
        # è¿‡æ»¤é”™è¯¯æ ·æœ¬
        if self.error_samples:
            original_count = len(self.data_index)
            self.data_index = [item for item in self.data_index 
                             if item.get("sample_id", item.get("id")) not in self.error_samples]
            filtered_count = original_count - len(self.data_index)
            if filtered_count > 0:
                print(f"Filtered out {filtered_count} known error samples")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if config.num_samples:
            self.data_index = self.data_index[:config.num_samples]
        
        print(f"Loaded {len(self.data_index)} samples for benchmarking")
        
        self.models = {}
        self.tokenizers = {}
    
    def load_model(self, model_config: ModelConfig) -> Tuple[Any, Any]:
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"\\nğŸ”„ Loading model: {model_config.name} ({model_config.model_type})")
        
        if model_config.model_type == "official":
            return self._load_official_model(model_config)
        elif model_config.model_type == "sft":
            return self._load_sft_model(model_config)
        elif model_config.model_type in ["lora", "lora_from_full"]:
            return self._load_lora_model(model_config)
        else:
            raise ValueError(f"Unsupported model type: {model_config.model_type}")
    
    def _load_official_model(self, model_config: ModelConfig) -> Tuple[Any, Any]:
        """åŠ è½½å®˜æ–¹æ¨¡å‹"""
        from transformers import AutoTokenizer
        
        # æ£€æŸ¥å®˜æ–¹æ¨¡å‹æ˜¯å¦å­˜åœ¨
        model_path = Path(model_config.path)
        if not model_path.exists():
            # å°è¯•ç›´æ¥ä»HuggingFaceåŠ è½½
            print(f"Local model not found, trying to load from HuggingFace: filapro/cad-recode-v1.5")
            model_config.path = "filapro/cad-recode-v1.5"
        
        # ä½¿ç”¨å®˜æ–¹æ¨èçš„åŠ è½½æ–¹å¼
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        attn_implementation = None  # ä¸ä½¿ç”¨ flash_attention_2ï¼Œä½¿ç”¨æ™®é€šæ³¨æ„åŠ›æœºåˆ¶
        tokenizer = AutoTokenizer.from_pretrained(
            'Qwen/Qwen2-1.5B',
            pad_token='<|im_end|>',
            padding_side='left',
            trust_remote_code=True
        )
        
        # åŠ¨æ€åˆ›å»ºå®˜æ–¹æ¨¡å‹ç±»
        official_model_cls = OfficialCADRecode().OfficialCADRecode
        
        model = official_model_cls.from_pretrained(
            model_config.path,
            torch_dtype='auto',
            attn_implementation=attn_implementation,
            trust_remote_code=True
        ).eval().to(device)
        
        print(f"âœ… Official model loaded from {model_config.path}")
        return model, tokenizer
    
    def _load_sft_model(self, model_config: ModelConfig) -> Tuple[Any, Any]:
        """åŠ è½½SFTæ¨¡å‹"""
        from transformers import AutoTokenizer, AutoConfig
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_config.path,
            pad_token='<|im_end|>',
            padding_side='left',
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹
        model_config_obj = AutoConfig.from_pretrained(model_config.path, trust_remote_code=True)
        if hasattr(model_config_obj, 'sliding_window'):
            model_config_obj.sliding_window = None
        
        model = CADRecode.from_pretrained(
            model_config.path,
            config=model_config_obj,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        ).eval()
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() and self.config.device != "cpu" else "cpu")
        model = model.to(device)
        
        print(f"âœ… SFT model loaded from {model_config.path}")
        return model, tokenizer
    
    def _load_lora_model(self, model_config: ModelConfig) -> Tuple[Any, Any]:
        """åŠ è½½LoRAæ¨¡å‹"""
        from inference_lora import load_lora_model
        
        if not model_config.base_model:
            # æ ¹æ®æ¨¡å‹ç±»å‹æ¨æ–­base model
            if model_config.model_type == "lora":
                model_config.base_model = "Qwen/Qwen3-1.7B-Base"
            else:  # lora_from_full
                # éœ€è¦ç”¨æˆ·æŒ‡å®šæˆ–ä»é…ç½®æ¨æ–­
                model_config.base_model = "checkpoints_qwen3_sft"
        
        model, tokenizer = load_lora_model(
            model_config.base_model,
            model_config.path,
            device=self.config.device
        )
        
        print(f"âœ… LoRA model loaded from {model_config.path}")
        return model, tokenizer
    
    def inference_single_sample_official(self, model, tokenizer, point_cloud: np.ndarray, sample_id: str) -> str:
        """ä½¿ç”¨å®˜æ–¹æ¨¡å‹è¿›è¡Œå•æ ·æœ¬æ¨ç† - å®Œå…¨æŒ‰ç…§demo.ipynbçš„æ–¹å¼"""
        try:
            # æŒ‰ç…§å®˜æ–¹demoçš„æ–¹å¼æ„å»ºè¾“å…¥
            input_ids = [tokenizer.pad_token_id] * len(point_cloud) + [tokenizer('<|im_start|>')['input_ids'][0]]
            attention_mask = [-1] * len(point_cloud) + [1]
            
            with torch.no_grad():
                batch_ids = model.generate(
                    input_ids=torch.tensor(input_ids).unsqueeze(0).to(model.device),
                    attention_mask=torch.tensor(attention_mask).unsqueeze(0).to(model.device),
                    point_cloud=torch.tensor(point_cloud.astype(np.float32)).unsqueeze(0).to(model.device),
                    max_new_tokens=self.config.max_new_tokens,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            py_string = tokenizer.batch_decode(batch_ids)[0]
            begin = py_string.find('<|im_start|>') + 12
            end = py_string.find('<|endoftext|>')
            if end == -1:
                end = len(py_string)
            py_string = py_string[begin:end]
            
            return py_string.strip()
            
        except Exception as e:
            print(f"Error in official inference for {sample_id}: {e}")
            return f"# Error during inference: {e}\\nr = cq.Workplane().box(1, 1, 1)"
    
    def inference_single_sample_custom(self, model, tokenizer, point_cloud: np.ndarray, sample_id: str) -> str:
        """ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹è¿›è¡Œå•æ ·æœ¬æ¨ç†"""
        try:
            return inference_single_sample(
                model=model,
                tokenizer=tokenizer,
                point_cloud=point_cloud,
                max_new_tokens=self.config.max_new_tokens
            )
        except Exception as e:
            print(f"Error in custom inference for {sample_id}: {e}")
            return f"# Error during inference: {e}\\nr = cq.Workplane().box(1, 1, 1)"
    
    def run_benchmark(self):
        """è¿è¡Œbenchmark"""
        print(f"\\nğŸš€ Starting benchmark with {len(self.config.models)} models on {len(self.data_index)} samples")
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        for model_config in self.config.models:
            model, tokenizer = self.load_model(model_config)
            self.models[model_config.name] = model
            self.tokenizers[model_config.name] = tokenizer
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        results_dir = self.output_dir / "results"
        results_dir.mkdir(exist_ok=True)
        
        for model_config in self.config.models:
            model_dir = results_dir / model_config.name
            model_dir.mkdir(exist_ok=True)
            (model_dir / "generated_code").mkdir(exist_ok=True)
            (model_dir / "generated_stl").mkdir(exist_ok=True)
            (model_dir / "generated_step").mkdir(exist_ok=True)
        
        # è¿è¡Œæ¨ç†
        all_results = {}
        for model_config in self.config.models:
            print(f"\\nğŸ“Š Running inference with {model_config.name}...")
            model_results = self._run_model_inference(model_config)
            all_results[model_config.name] = model_results
        
        # è®¡ç®—å‡ ä½•æŒ‡æ ‡
        print(f"\\nğŸ“ Computing geometric metrics...")
        metrics_results = self._compute_metrics(all_results)
        
        # ç”ŸæˆæŠ¥å‘Š
        print(f"\\nğŸ“‹ Generating benchmark report...")
        self._generate_report(all_results, metrics_results)
        
        print(f"\\nâœ… Benchmark completed! Results saved to {self.output_dir}")
    
    def _run_model_inference(self, model_config: ModelConfig) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ¨¡å‹çš„æ¨ç†"""
        model = self.models[model_config.name]
        tokenizer = self.tokenizers[model_config.name]
        model_dir = self.output_dir / "results" / model_config.name
        
        results = {
            "model_config": asdict(model_config),
            "samples": {},
            "generation_stats": {
                "total_samples": len(self.data_index),
                "successful_generations": 0,
                "failed_generations": 0,
                "successful_stl": 0,
                "failed_stl": 0
            }
        }
        
        for i, sample_info in enumerate(tqdm(self.data_index, desc=f"Inference {model_config.name}")):
            sample_id = sample_info.get("sample_id", sample_info.get("id"))
            
            # åŠ è½½ç‚¹äº‘ - æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©ä¸åŒçš„æ–¹æ³•
            if sample_info.get("data_source") == "fusion360_step":
                # å¯¹äºfusion360æ•°æ®é›†ï¼Œä»STEPæ–‡ä»¶ç”Ÿæˆç‚¹äº‘
                step_file = sample_info.get("step_file")
                point_cloud = self._get_point_cloud_from_step(step_file, sample_id)
            else:
                # å¯¹äºä¼ ç»Ÿæ•°æ®é›†ï¼Œä»ç¼“å­˜è¯»å–ç‚¹äº‘
                point_cloud = get_cached_point_cloud(
                    Path(self.config.data_path), 
                    sample_id, 
                    num_points=NUM_POINT_TOKENS
                )
            
            if point_cloud is None:
                print(f"Warning: No point cloud found for sample {sample_id}")
                continue
            
            # æ¨ç†ç”Ÿæˆä»£ç 
            if model_config.model_type == "official":
                generated_code = self.inference_single_sample_official(model, tokenizer, point_cloud, sample_id)
            else:
                generated_code = self.inference_single_sample_custom(model, tokenizer, point_cloud, sample_id)
            
            # ä¿å­˜ç”Ÿæˆçš„ä»£ç 
            if self.config.save_code:
                code_path = model_dir / "generated_code" / f"{sample_id}.py"
                with open(code_path, 'w', encoding='utf-8') as f:
                    f.write(generated_code)
            
            # æ‰§è¡ŒCadQueryä»£ç ç”Ÿæˆå‡ ä½•ä½“
            stl_path = model_dir / "generated_stl" / f"{sample_id}.stl"
            step_path = model_dir / "generated_step" / f"{sample_id}.step"
            
            success, error_msg, file_status = self.cadquery_executor.execute_cadquery_code(
                generated_code,
                str(stl_path) if self.config.save_stl else None,
                str(step_path) if self.config.save_step else None
            )
            
            # è®°å½•ç»“æœ
            sample_result = {
                "sample_id": sample_id,
                "generated_code": generated_code,
                "execution_success": success,
                "error_message": error_msg if not success else None,
                "stl_generated": file_status.get("stl", False),
                "step_generated": file_status.get("step", False)
            }
            
            results["samples"][sample_id] = sample_result
            
            # æ›´æ–°ç»Ÿè®¡
            if success:
                results["generation_stats"]["successful_generations"] += 1
                if file_status.get("stl", False):
                    results["generation_stats"]["successful_stl"] += 1
                else:
                    results["generation_stats"]["failed_stl"] += 1
            else:
                results["generation_stats"]["failed_generations"] += 1
                results["generation_stats"]["failed_stl"] += 1
        
        # ä¿å­˜æ¨¡å‹ç»“æœ
        results_file = model_dir / "inference_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        return results
    
    def _load_fusion360_data_index(self) -> List[Dict[str, Any]]:
        """åŠ è½½fusion360æ•°æ®é›†çš„train/testæ‹†åˆ†"""
        # ç¡®å®štrain_test.jsonæ–‡ä»¶è·¯å¾„
        if self.config.train_test_json:
            train_test_file = Path(self.config.train_test_json)
        else:
            # é»˜è®¤åœ¨fusion360datasetç›®å½•ä¸‹æŸ¥æ‰¾
            train_test_file = Path(self.config.data_path).parent / "fusion360dataset" / "train_test.json"
        
        if not train_test_file.exists():
            raise FileNotFoundError(f"Train/test split file not found: {train_test_file}")
        
        print(f"Loading fusion360 {self.config.split} split from {train_test_file}")
        
        # åŠ è½½train/testæ‹†åˆ†
        with open(train_test_file, 'r', encoding='utf-8') as f:
            train_test_data = json.load(f)
        
        if self.config.split not in train_test_data:
            raise ValueError(f"Split '{self.config.split}' not found in {train_test_file}")
        
        sample_ids = train_test_data[self.config.split]
        print(f"Found {len(sample_ids)} samples in {self.config.split} split")
        
        # æ„å»ºæ•°æ®ç´¢å¼• - åŸºäºfusion360æ•°æ®é›†ç»“æ„
        data_index = []
        
        # fusion360æ•°æ®é›†çš„reconstructionç›®å½•è·¯å¾„
        reconstruction_dir = Path("cad-recode/fusion360dataset/reconstruction")
        if not reconstruction_dir.exists():
            # å°è¯•ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„è·¯å¾„
            reconstruction_dir = Path("fusion360dataset/reconstruction")
            if not reconstruction_dir.exists():
                # å°è¯•ç›¸å¯¹äºdata_pathçš„è·¯å¾„
                reconstruction_dir = Path(self.config.data_path).parent / "fusion360dataset" / "reconstruction"
                if not reconstruction_dir.exists():
                    raise FileNotFoundError(f"Fusion360 reconstruction directory not found")
        
        print(f"Using reconstruction directory: {reconstruction_dir}")
        
        found_samples = 0
        missing_samples = 0
        
        for sample_id in sample_ids:
            # åœ¨reconstructionç›®å½•ä¸­æŸ¥æ‰¾å¯¹åº”çš„.stepæ–‡ä»¶
            # æ–‡ä»¶åæ ¼å¼å¯èƒ½æ˜¯: {sample_id}.step æˆ–åŒ…å«sample_idçš„æ–‡ä»¶å
            step_file = None
            
            # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
            exact_match = reconstruction_dir / f"{sample_id}.step"
            if exact_match.exists():
                step_file = exact_match
            else:
                # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å«sample_idï¼‰
                for step_candidate in reconstruction_dir.glob(f"*{sample_id}*.step"):
                    step_file = step_candidate
                    break
            
            if step_file and step_file.exists():
                data_index.append({
                    "id": sample_id,
                    "sample_id": sample_id,
                    "step_file": str(step_file),
                    "split": self.config.split,
                    "data_source": "fusion360_step"  # æ ‡è®°æ•°æ®æ¥æº
                })
                found_samples += 1
            else:
                print(f"Warning: STEP file not found for sample {sample_id}")
                missing_samples += 1
        
        print(f"Successfully loaded {found_samples} samples from {self.config.split} split")
        if missing_samples > 0:
            print(f"Warning: {missing_samples} samples missing STEP files")
        
        return data_index
    
    def _get_point_cloud_from_step(self, step_file: str, sample_id: str) -> Optional[np.ndarray]:
        """ä»STEPæ–‡ä»¶ç”Ÿæˆç‚¹äº‘"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„ç‚¹äº‘
            cache_dir = Path("point_cloud_cache_fusion360")
            cache_dir.mkdir(exist_ok=True)
            cache_file = cache_dir / f"{sample_id}.npy"
            
            if cache_file.exists():
                try:
                    point_cloud = np.load(cache_file)
                    if point_cloud.shape[0] == NUM_POINT_TOKENS:
                        return point_cloud.astype(np.float32)
                except Exception as e:
                    print(f"Warning: Failed to load cached point cloud for {sample_id}: {e}")
            
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä»STEPæ–‡ä»¶ç”Ÿæˆç‚¹äº‘
            print(f"Generating point cloud from STEP file for {sample_id}")
            
            # # æ–¹æ³•1: ä½¿ç”¨OCCç›´æ¥ä»STEPæ–‡ä»¶é‡‡æ ·ç‚¹äº‘
            # if OCC_AVAILABLE:
            #     point_cloud = self._step_to_point_cloud_direct(step_file)
            #     if point_cloud is not None:
            #         # ä¿å­˜åˆ°ç¼“å­˜
            #         np.save(cache_file, point_cloud)
            #         return point_cloud
            
            # æ–¹æ³•2: STEP -> STL -> ç‚¹äº‘
            stl_file = self._step_to_stl(step_file, sample_id)
            if stl_file:
                point_cloud = self._stl_to_point_cloud(stl_file)
                if point_cloud is not None:
                    # ä¿å­˜åˆ°ç¼“å­˜
                    np.save(cache_file, point_cloud)
                    return point_cloud
            
            print(f"Warning: Failed to generate point cloud from STEP file {step_file}")
            return None
            
        except Exception as e:
            print(f"Error generating point cloud from STEP file {step_file}: {e}")
            return None
    
    def _step_to_point_cloud_direct(self, step_file: str) -> Optional[np.ndarray]:
        """ç›´æ¥ä»STEPæ–‡ä»¶ç”Ÿæˆç‚¹äº‘ï¼ˆä½¿ç”¨OCCï¼‰"""
        try:
            from OCC.Core.STEPControl import STEPControl_Reader
            from OCC.Core.IFSelect import IFSelect_RetDone
            from OCC.Core.BRepMesh import BRepMesh_IncrementalMesh
            from OCC.Core.TopExp import TopExp_Explorer
            from OCC.Core.TopAbs import TopAbs_FACE
            from OCC.Core.BRep import BRep_Tool
            from OCC.Extend.TopologyUtils import TopologyExplorer
            
            # è¯»å–STEPæ–‡ä»¶
            step_reader = STEPControl_Reader()
            status = step_reader.ReadFile(step_file)
            if status != IFSelect_RetDone:
                return None
            
            step_reader.TransferRoots()
            shape = step_reader.OneShape()
            
            # ç”Ÿæˆç½‘æ ¼
            mesh = BRepMesh_IncrementalMesh(shape, 0.1)
            mesh.Perform()
            
            # æå–ä¸‰è§’å½¢é¡¶ç‚¹
            points = []
            explorer = TopologyExplorer(shape)
            for face in explorer.faces():
                location_out = None
                location = BRep_Tool.Triangulation(face, location_out)
                if location:
                    triangulation = location.GetObject()
                    for i in range(1, triangulation.NbNodes() + 1):
                        node = triangulation.Node(i)
                        points.append([node.X(), node.Y(), node.Z()])
            
            if len(points) == 0:
                return None
            
            points = np.array(points, dtype=np.float32)
            
            # é‡‡æ ·åˆ°æŒ‡å®šæ•°é‡çš„ç‚¹
            if len(points) > NUM_POINT_TOKENS:
                indices = np.random.choice(len(points), NUM_POINT_TOKENS, replace=False)
                points = points[indices]
            elif len(points) < NUM_POINT_TOKENS:
                # é‡å¤é‡‡æ ·
                indices = np.random.choice(len(points), NUM_POINT_TOKENS, replace=True)
                points = points[indices]
            
            # å½’ä¸€åŒ–åˆ°[-1, 1]
            points = self._normalize_point_cloud(points)
            
            return points
            
        except Exception as e:
            print(f"Error in direct STEP to point cloud conversion: {e}")
            return None
    
    def _step_to_stl(self, step_file: str, sample_id: str) -> Optional[str]:
        """å°†STEPæ–‡ä»¶è½¬æ¢ä¸ºSTLæ–‡ä»¶"""
        try:
            # åˆ›å»ºä¸´æ—¶STLæ–‡ä»¶
            temp_stl_dir = Path("temp_stl")
            temp_stl_dir.mkdir(exist_ok=True)
            stl_file = temp_stl_dir / f"{sample_id}_temp.stl"
            
            if OCC_AVAILABLE:
                from OCC.Core.STEPControl import STEPControl_Reader
                from OCC.Core.IFSelect import IFSelect_RetDone
                from OCC.Extend.DataExchange import write_stl_file
                
                # è¯»å–STEPæ–‡ä»¶
                step_reader = STEPControl_Reader()
                status = step_reader.ReadFile(step_file)
                if status != IFSelect_RetDone:
                    return None
                
                step_reader.TransferRoots()
                shape = step_reader.OneShape()
                
                # å†™å…¥STLæ–‡ä»¶
                write_stl_file(shape, str(stl_file))
                
                if stl_file.exists():
                    return str(stl_file)
            
            return None
            
        except Exception as e:
            print(f"Error converting STEP to STL: {e}")
            return None
    
    def _stl_to_point_cloud(self, stl_file: str) -> Optional[np.ndarray]:
        """ä»STLæ–‡ä»¶ç”Ÿæˆç‚¹äº‘ï¼Œä½¿ç”¨ç®€å•çš„è¿œç‚¹é‡‡æ ·æ›¿ä»£pytorch3d"""
        import trimesh

        try:
            # åŠ è½½STLæ–‡ä»¶ä¸ºmesh
            mesh = trimesh.load(stl_file)

            # é‡‡æ ·8192ä¸ªç‚¹
            vertices, _ = mesh.sample(8192, return_index=True)

            # ä½¿ç”¨ç®€å•çš„è¿œç‚¹é‡‡æ ·ç®—æ³•é€‰å–256ä¸ªç‚¹
            sampled_points = self._farthest_point_sampling(vertices, NUM_POINT_TOKENS)

            # å½’ä¸€åŒ–ç‚¹äº‘
            sampled_points = self._normalize_point_cloud(sampled_points)

            return sampled_points.astype(np.float32)
        except Exception as e:
            print(f"Error in _stl_to_point_cloud: {e}")
            return None

    def _farthest_point_sampling(self, points: np.ndarray, n_samples: int) -> np.ndarray:
        """
        è¿œç‚¹é‡‡æ ·ç®—æ³•ï¼ˆFarthest Point Sampling, FPSï¼‰
        ä»ç‚¹äº‘ä¸­é‡‡æ ·n_samplesä¸ªç‚¹ï¼Œä½¿å¾—é‡‡æ ·ç‚¹ä¹‹é—´å°½å¯èƒ½åˆ†æ•£ã€‚
        """
        if points.shape[0] <= n_samples:
            return points.copy()

        sampled_indices = np.zeros(n_samples, dtype=np.int64)
        n_points = points.shape[0]
        distances = np.full(n_points, np.inf)

        # éšæœºé€‰æ‹©ç¬¬ä¸€ä¸ªç‚¹
        first_idx = np.random.randint(0, n_points)
        sampled_indices[0] = first_idx

        # è¿­ä»£é‡‡æ ·
        for i in range(1, n_samples):
            # æ›´æ–°æ‰€æœ‰ç‚¹åˆ°å·²é‡‡æ ·ç‚¹é›†çš„æœ€å°è·ç¦»
            last_sampled = points[sampled_indices[i - 1]]
            dist = np.linalg.norm(points - last_sampled, axis=1)
            distances = np.minimum(distances, dist)
            # é€‰æ‹©è·ç¦»å·²é‡‡æ ·ç‚¹é›†æœ€è¿œçš„ç‚¹
            next_idx = np.argmax(distances)
            sampled_indices[i] = next_idx

        return points[sampled_indices]
    
    def _normalize_point_cloud(self, points: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–ç‚¹äº‘åˆ°[-1, 1]èŒƒå›´"""
        # ä¸­å¿ƒåŒ–
        center = points.mean(axis=0)
        points = points - center
        
        # ç¼©æ”¾åˆ°[-1, 1]
        max_dist = np.abs(points).max()
        if max_dist > 0:
            points = points / max_dist
        
        return points

    def _prepare_gt_step_files(self) -> Path:
        """ä¸ºGTæ•°æ®ç”ŸæˆSTEPæ–‡ä»¶ï¼Œå¹¶ä¿å­˜meshï¼ˆSTLï¼‰"""
        gt_step_dir = self.output_dir / "gt_step_files"
        gt_step_dir.mkdir(exist_ok=True)
        gt_stl_dir = self.output_dir / "gt_stl_files"
        gt_stl_dir.mkdir(exist_ok=True)

        print(f"ğŸ”§ Preparing GT STEP files and mesh (STL)...")

        # åˆ›å»ºä¸´æ—¶çš„CadQueryæ‰§è¡Œå™¨ç”¨äºç”ŸæˆGT STEPå’ŒSTLæ–‡ä»¶
        temp_config = BatchInferenceConfig(
            output_dir=str(self.output_dir),
            cadquery_timeout=self.config.cadquery_timeout,
            save_code=False,
            save_stl=True,
            save_step=True
        )
        gt_executor = CadQueryExecutor(temp_config)

        generated_count = 0
        failed_count = 0

        for sample_info in tqdm(self.data_index, desc="Generating GT STEP files"):
            sample_id = sample_info.get("sample_id", sample_info.get("id"))
            gt_step_path = gt_step_dir / f"{sample_id}.step"
            gt_stl_path = gt_stl_dir / f"{sample_id}.stl"
            if "step_file" in sample_info:
                # fusion360 æ ¼å¼ï¼Œç›´æ¥å¤åˆ¶ step æ–‡ä»¶
                src_step = Path(sample_info["step_file"])
                if src_step.exists():
                    shutil.copy(src_step, gt_step_path)
                    # mesh(STL) ä¹Ÿå°è¯•ç”Ÿæˆ
                    try:
                        # åªåœ¨æœªå­˜åœ¨æ—¶ç”Ÿæˆ
                        if not gt_stl_path.exists():
                            # ç”¨ OCC æˆ–è€… trimesh è¿›è¡Œè½¬æ¢
                            # è¿™é‡Œç›´æ¥ç”¨ _step_to_stl
                            stl_file = self._step_to_stl(str(gt_step_path), sample_id)
                            if stl_file and Path(stl_file).exists():
                                shutil.copy(stl_file, gt_stl_path)
                    except Exception as e:
                        print(f"Warning: Failed to generate GT STL for {sample_id}: {e}")
                    generated_count += 1
                    continue
                else:
                    print(f"Warning: GT STEP file not found: {src_step}")
                    failed_count += 1
                    continue
            elif "code_path" in sample_info:
                gt_code_path = Path(sample_info["code_path"])
            else:
                gt_code_path = Path(self.config.data_path) / f"{sample_id}.py"

            # å¦‚æœSTEPå’ŒSTLæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if gt_step_path.exists() and gt_stl_path.exists():
                generated_count += 1
                continue

            # è¯»å–GTä»£ç 
            if not gt_code_path.exists():
                print(f"Warning: GT code file not found: {gt_code_path}")
                failed_count += 1
                continue

            try:
                with open(gt_code_path, 'r', encoding='utf-8') as f:
                    gt_code = f.read()

                # æ‰§è¡ŒGTä»£ç ç”ŸæˆSTEPå’ŒSTLæ–‡ä»¶
                success, error_msg, file_status = gt_executor.execute_cadquery_code(
                    gt_code,
                    stl_output_path=str(gt_stl_path),
                    step_output_path=str(gt_step_path)
                )

                if success and file_status.get("step", False):
                    generated_count += 1
                else:
                    print(f"Failed to generate GT STEP for {sample_id}: {error_msg}")
                    failed_count += 1

            except Exception as e:
                print(f"Error processing GT code for {sample_id}: {e}")
                failed_count += 1

        print(f"GT STEP generation completed: {generated_count} success, {failed_count} failed")
        return gt_step_dir
    
    def _compute_metrics(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å‡ ä½•æŒ‡æ ‡ - ä»…å¯¹æ‰€æœ‰æ¨¡å‹éƒ½æˆåŠŸç”Ÿæˆstepçš„æ ·æœ¬è¿›è¡ŒæŒ‡æ ‡è®¡ç®—"""
        metrics_results = {}
        # ä¸ºGTæ•°æ®ç”ŸæˆSTEPæ–‡ä»¶
        gt_step_dir = self._prepare_gt_step_files()

        # 1. æ‰¾åˆ°æ‰€æœ‰æ¨¡å‹éƒ½æˆåŠŸç”Ÿæˆstepçš„æ ·æœ¬idäº¤é›†
        all_model_success_ids = None
        model_pred_step_dirs = {}
        for model_name, model_results in all_results.items():
            model_dir = self.output_dir / "results" / model_name
            pred_step_dir = model_dir / "generated_step"
            model_pred_step_dirs[model_name] = pred_step_dir
            success_ids = set()
            for sample_id, sample_result in model_results["samples"].items():
                if sample_result["step_generated"]:
                    gt_step = gt_step_dir / f"{sample_id}.step"
                    pred_step = pred_step_dir / f"{sample_id}.step"
                    if gt_step.exists() and pred_step.exists():
                        success_ids.add(sample_id)
            if all_model_success_ids is None:
                all_model_success_ids = success_ids
            else:
                all_model_success_ids = all_model_success_ids & success_ids

        if not all_model_success_ids or len(all_model_success_ids) == 0:
            print("\nNo common successful samples for all models. Metrics cannot be computed.")
            for model_name in all_results.keys():
                metrics_results[model_name] = {"error": "No common successful samples for all models"}
            return metrics_results

        print(f"\nFound {len(all_model_success_ids)} samples with successful STEP for all models. Only these will be used for metrics.")

        # 2. å¯¹æ¯ä¸ªæ¨¡å‹ï¼Œä»…å¯¹äº¤é›†æ ·æœ¬è®¡ç®—æŒ‡æ ‡
        for model_name, model_results in all_results.items():
            print(f"  Computing metrics for {model_name}...")
            model_dir = self.output_dir / "results" / model_name
            pred_step_dir = model_pred_step_dirs[model_name]
            file_pairs = []
            for sample_id in all_model_success_ids:
                gt_step = gt_step_dir / f"{sample_id}.step"
                pred_step = pred_step_dir / f"{sample_id}.step"
                if gt_step.exists() and pred_step.exists():
                    file_pairs.append((str(gt_step), str(pred_step)))

            if not file_pairs:
                print(f"    No valid file pairs found for {model_name}")
                metrics_results[model_name] = {"error": "No valid file pairs"}
                continue

            print(f"    Found {len(file_pairs)} valid GT-Prediction pairs for metric calculation")

            # ç›´æ¥è®¡ç®—æŒ‡æ ‡ - ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
            try:
                batch_results = []
                print(f"    Computing metrics for each pair...")

                for i, (gt_file, pred_file) in enumerate(file_pairs):
                    try:
                        result = self.metrics_calculator.compute_files_metrics(
                            ext="step",
                            file1=gt_file,
                            file2=pred_file,
                            matching_threshold=None,
                            include_normals=True,
                            use_icp=True
                        )

                        if result is not None:
                            result_dict = result.to_dict()
                            result_dict['file1'] = gt_file.replace("\\", "/")
                            result_dict['file2'] = pred_file.replace("\\", "/")
                            result_dict['pair_index'] = i
                            batch_results.append(result_dict)
                            print(f"      âœ… Pair {i+1}/{len(file_pairs)}: Success")
                        else:
                            print(f"      âŒ Pair {i+1}/{len(file_pairs)}: Failed")
                    except Exception as e:
                        print(f"      âŒ Pair {i+1}/{len(file_pairs)}: Error - {e}")

                print(f"    Computed {len(batch_results)} successful metric results for {model_name}")
                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                if batch_results:
                    metrics = self._compute_average_metrics(batch_results)
                    metrics["total_pairs"] = len(file_pairs)
                    metrics["successful_pairs"] = len(batch_results)

                    print(f"    Metrics computed for {metrics['successful_pairs']}/{metrics['total_pairs']} pairs")
                else:
                    metrics = {"error": "No successful metric computations"}

                metrics_results[model_name] = metrics

                # ä¿å­˜è¯¦ç»†æŒ‡æ ‡
                metrics_file = model_dir / "detailed_metrics.json"
                with open(metrics_file, 'w') as f:
                    json.dump({
                        "summary": metrics,
                        "batch_results": batch_results,
                        "file_pairs": [(str(p[0]), str(p[1])) for p in file_pairs]
                    }, f, indent=2)

            except Exception as e:
                print(f"    Error computing metrics for {model_name}: {e}")
                traceback.print_exc()
                metrics_results[model_name] = {"error": str(e)}

        return metrics_results
    
    def _compute_average_metrics(self, batch_results: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if not batch_results:
            return {}
        
        # æ ¹æ®metric.pyå®šä¹‰çš„æŒ‡æ ‡
        metrics = [
            'chamfer_distance', 'hausdorff_distance', 'earth_mover_distance', 'rms_error',
            'iou_voxel', 'iou_brep', 'matching_rate', 'coverage_rate', 
            'normal_consistency', 'computation_time'
        ]
        
        def get_values(key):
            values = []
            for res in batch_results:
                value = res.get(key)
                # åªå¤„ç†æ•°å€¼ç±»å‹çš„å€¼
                if value is not None and isinstance(value, (int, float)) and not np.isnan(value):
                    values.append(value)
            return values
        
        result = {}
        for metric in metrics:
            vals = get_values(metric)
            if len(vals) > 0:
                result[f"{metric}_mean"] = float(np.mean(vals))
                result[f"{metric}_std"] = float(np.std(vals))
                result[f"{metric}_median"] = float(np.median(vals))
                result[f"{metric}_min"] = float(np.min(vals))
                result[f"{metric}_max"] = float(np.max(vals))
        
        return result
    
    def _generate_report(self, all_results: Dict[str, Any], metrics_results: Dict[str, Any]):
        """ç”ŸæˆbenchmarkæŠ¥å‘Š"""
        report = {
            "benchmark_info": {
                "timestamp": datetime.now().isoformat(),
                "config": asdict(self.config),
                "total_samples": len(self.data_index)
            },
            "models": {}
        }
        
        # æ±‡æ€»æ¯ä¸ªæ¨¡å‹çš„ç»“æœ
        for model_name in all_results.keys():
            model_data = all_results[model_name]
            model_metrics = metrics_results.get(model_name, {})
            
            # ç”Ÿæˆç»Ÿè®¡
            stats = model_data["generation_stats"]
            metrics_summary = {}
            
            if "error" not in model_metrics:
                # ä¸»è¦æŒ‡æ ‡
                main_metrics = [
                    "chamfer_distance", "hausdorff_distance", "earth_mover_distance",
                    "iou_voxel", "iou_brep", "matching_rate", "coverage_rate", "normal_consistency"
                ]
                
                for metric in main_metrics:
                    mean_key = f"{metric}_mean"
                    std_key = f"{metric}_std"
                    if mean_key in model_metrics:
                        metrics_summary[metric] = {
                            "mean": model_metrics[mean_key],
                            "std": model_metrics[std_key]
                        }
            
            report["models"][model_name] = {
                "config": model_data["model_config"],
                "generation_stats": stats,
                "metrics": metrics_summary if metrics_summary else model_metrics
            }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "benchmark_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(report)
        
        # æ‰“å°æ€»ç»“
        self._print_summary(report)
    
    def _generate_markdown_report(self, report: Dict):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        md_content = f"""# CAD-Recode Benchmark Report

**Generated:** {report['benchmark_info']['timestamp']}  
**Total Samples:** {report['benchmark_info']['total_samples']}

## Model Comparison

| Model | Type | Success Rate | STL Success | Chamferâ†“ | IoUâ†‘ | Matching Rateâ†‘ |
|-------|------|-------------|-------------|----------|------|----------------|
"""
        
        for model_name, model_data in report["models"].items():
            config = model_data["config"]
            stats = model_data["generation_stats"]
            metrics = model_data["metrics"]
            
            # è®¡ç®—æˆåŠŸç‡
            total = stats["total_samples"]
            success_rate = stats["successful_generations"] / total * 100 if total > 0 else 0
            stl_success_rate = stats["successful_stl"] / total * 100 if total > 0 else 0
            
            # æå–å…³é”®æŒ‡æ ‡
            chamfer = metrics.get("chamfer_distance", {})
            iou = metrics.get("iou_brep", {}) or metrics.get("iou_voxel", {})
            matching = metrics.get("matching_rate", {})
            
            chamfer_str = f"{chamfer.get('mean', 0):.4f}" if isinstance(chamfer, dict) else "N/A"
            iou_str = f"{iou.get('mean', 0):.4f}" if isinstance(iou, dict) else "N/A"
            matching_str = f"{matching.get('mean', 0):.4f}" if isinstance(matching, dict) else "N/A"
            
            md_content += f"| {model_name} | {config['model_type']} | {success_rate:.1f}% | {stl_success_rate:.1f}% | {chamfer_str} | {iou_str} | {matching_str} |\\n"
        
        md_content += f"""
## Detailed Results

"""
        
        for model_name, model_data in report["models"].items():
            metrics = model_data["metrics"]
            md_content += f"""### {model_name}

**Generation Statistics:**
- Total Samples: {model_data['generation_stats']['total_samples']}
- Successful Generations: {model_data['generation_stats']['successful_generations']}
- Failed Generations: {model_data['generation_stats']['failed_generations']}
- Successful STL: {model_data['generation_stats']['successful_stl']}

"""
            if isinstance(metrics, dict) and "error" not in metrics:
                md_content += """**Geometric Metrics:**
"""
                for metric_name, metric_data in metrics.items():
                    if isinstance(metric_data, dict) and "mean" in metric_data:
                        md_content += f"- {metric_name}: {metric_data['mean']:.6f} Â± {metric_data['std']:.6f}\\n"
            else:
                md_content += f"**Metrics Error:** {metrics.get('error', 'Unknown error')}\\n"
            
            md_content += "\\n"
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = self.output_dir / "benchmark_report.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    def _print_summary(self, report: Dict):
        """æ‰“å°ç»“æœæ€»ç»“"""
        print(f"\\n{'='*60}")
        print(f"ğŸ¯ BENCHMARK SUMMARY")
        print(f"{'='*60}")
        
        for model_name, model_data in report["models"].items():
            stats = model_data["generation_stats"]
            metrics = model_data["metrics"]
            
            print(f"\\nğŸ“Š {model_name} ({model_data['config']['model_type']})")
            print(f"   Generation Success: {stats['successful_generations']}/{stats['total_samples']} ({stats['successful_generations']/stats['total_samples']*100:.1f}%)")
            print(f"   STL Success: {stats['successful_stl']}/{stats['total_samples']} ({stats['successful_stl']/stats['total_samples']*100:.1f}%)")
            
            if isinstance(metrics, dict) and "error" not in metrics:
                # æ‰“å°å…³é”®æŒ‡æ ‡
                key_metrics = ["chamfer_distance", "iou_brep", "matching_rate"]
                for metric in key_metrics:
                    if metric in metrics:
                        data = metrics[metric]
                        if isinstance(data, dict) and "mean" in data:
                            print(f"   {metric}: {data['mean']:.6f} Â± {data['std']:.6f}")
            else:
                print(f"   Metrics: {metrics.get('error', 'Error occurred')}")
        
        print(f"\\nâœ… Complete results saved to: {self.output_dir}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="CAD-Recode Benchmark Script")
    
    parser.add_argument("--models", nargs="+", required=True,
                       help="List of models to benchmark. Use 'official' for the official model, or provide paths to your trained models.")
    parser.add_argument("--model_types", nargs="+", 
                       help="Model types corresponding to --models. Options: official, sft, lora, lora_from_full")
    parser.add_argument("--base_models", nargs="+",
                       help="Base models for LoRA models (required for lora type)")
    
    parser.add_argument("--data_path", default="data/val",
                       help="Path to validation data")
    parser.add_argument("--output_dir", default="benchmark_results_fusion360",
                       help="Output directory for results")
    
    # æ•°æ®é›†ç±»å‹å’Œæ‹†åˆ†å‚æ•°
    parser.add_argument("--dataset_type", choices=["legacy", "fusion360"], default="legacy",
                       help="Dataset type: 'legacy' for original format, 'fusion360' for fusion360 dataset")
    parser.add_argument("--split", default="val", 
                       help="Data split to use. For legacy: 'train'/'val'. For fusion360: 'train'/'test'")
    parser.add_argument("--train_test_json", 
                       help="Path to train_test.json file for fusion360 dataset (default: fusion360dataset/train_test.json)")
    
    parser.add_argument("--num_samples", type=int,
                       help="Number of samples to evaluate (default: all)")
    parser.add_argument("--batch_size", type=int, default=1,
                       help="Batch size for inference")
    parser.add_argument("--max_new_tokens", type=int, default=768,
                       help="Maximum new tokens for generation")
    
    parser.add_argument("--device", default="auto",
                       help="Device to use (auto, cuda, cpu)")
    parser.add_argument("--no_stl", action="store_true",
                       help="Skip STL generation")
    parser.add_argument("--no_step", action="store_true",
                       help="Skip STEP generation")
    parser.add_argument("--no_code", action="store_true",
                       help="Skip code saving")
    
    parser.add_argument("--timeout", type=int, default=30,
                       help="Timeout for CadQuery execution")
    parser.add_argument("--metric_points", type=int, default=2000,
                       help="Number of points for metric calculation")
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # æ„å»ºæ¨¡å‹é…ç½®
    model_configs = []
    for i, model_path in enumerate(args.models):
        if model_path == "official":
            model_config = ModelConfig(
                name="official",
                path="cad-recode-v1.5",  # å®˜æ–¹æ¨¡å‹è·¯å¾„
                model_type="official"
            )
        else:
            # æ¨æ–­æ¨¡å‹ç±»å‹
            if args.model_types and i < len(args.model_types):
                model_type = args.model_types[i]
            else:
                # æ ¹æ®è·¯å¾„æ¨æ–­
                if "lora" in model_path.lower():
                    if "from_full" in model_path.lower():
                        model_type = "lora_from_full"
                    else:
                        model_type = "lora"
                else:
                    model_type = "sft"
            
            # åŸºç¡€æ¨¡å‹
            base_model = None
            if model_type in ["lora", "lora_from_full"]:
                if args.base_models and i < len(args.base_models):
                    base_model = args.base_models[i]
                elif model_type == "lora":
                    base_model = "Qwen/Qwen3-1.7B-Base"
                elif model_type == "lora_from_full":
                    base_model = "checkpoints_qwen3_sft"  # é»˜è®¤å€¼ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´
            
            model_config = ModelConfig(
                name=Path(model_path).name,
                path=model_path,
                model_type=model_type,
                base_model=base_model
            )
        
        model_configs.append(model_config)
    
    # åˆ›å»ºbenchmarké…ç½®
    config = BenchmarkConfig(
        models=model_configs,
        data_path=args.data_path,
        output_dir=args.output_dir,
        num_samples=args.num_samples,
        batch_size=args.batch_size,
        max_new_tokens=args.max_new_tokens,
        device=args.device,
        dataset_type=args.dataset_type,
        split=args.split,
        train_test_json=args.train_test_json,
        save_code=not args.no_code,
        save_stl=not args.no_stl,
        save_step=not args.no_step,
        n_points_metric=args.metric_points,
        cadquery_timeout=args.timeout
    )
    
    # è¿è¡Œbenchmark
    runner = BenchmarkRunner(config)
    runner.run_benchmark()


if __name__ == "__main__":
    main()
