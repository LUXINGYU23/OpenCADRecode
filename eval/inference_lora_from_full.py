#!/usr/bin/env python3
"""
CAD-Recode LoRAå¾®è°ƒæ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºæµ‹è¯•ä»fullæ¨¡å‹å¾®è°ƒåçš„LoRAæ¨¡å‹æ•ˆæœ
"""

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import warnings
import torch
import numpy as np
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils import inference_single_sample, get_cached_point_cloud, load_data_index
from models import CADRecode
from transformers import AutoTokenizer, AutoConfig
from peft import PeftModel

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


def load_lora_model(lora_model_path: str, use_merged: bool = True):
    """åŠ è½½LoRAå¾®è°ƒæ¨¡å‹"""
    
    if use_merged:
        # å°è¯•åŠ è½½åˆå¹¶åçš„æ¨¡å‹
        merged_path = Path(lora_model_path) / "merged_model"
        if merged_path.exists():
            print(f"ğŸ”„ Loading merged model from: {merged_path}")
            
            # åŠ è½½åˆ†è¯å™¨
            tokenizer = AutoTokenizer.from_pretrained(
                str(merged_path),
                pad_token='<|im_end|>',
                padding_side='left',
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            model = CADRecode.from_pretrained(
                str(merged_path),
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
            
            print(f"âœ… Successfully loaded merged LoRA model")
            return model, tokenizer
    
    # åŠ è½½LoRAæƒé‡
    print(f"ğŸ”„ Loading LoRA model from: {lora_model_path}")
    
    # é¦–å…ˆç¡®å®šåŸºç¡€æ¨¡å‹è·¯å¾„
    config_file = Path(lora_model_path) / "config.yaml"
    if config_file.exists():
        import yaml
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
        base_model_name = config.get('base_model_name', 'checkpoints_qwen3_sft')
    else:
        base_model_name = 'checkpoints_qwen3_sft'  # é»˜è®¤è·¯å¾„
    
    print(f"ğŸ”„ Base model: {base_model_name}")
    
    # åŠ è½½åˆ†è¯å™¨
    if os.path.exists(base_model_name):
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_name,
            pad_token='<|im_end|>',
            padding_side='left',
            trust_remote_code=True
        )
    else:
        # ä»LoRAç›®å½•åŠ è½½
        tokenizer = AutoTokenizer.from_pretrained(
            lora_model_path,
            pad_token='<|im_end|>',
            padding_side='left',
            trust_remote_code=True
        )
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    if os.path.exists(base_model_name):
        model = CADRecode.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    else:
        raise ValueError(f"Base model not found: {base_model_name}")
    
    # åŠ è½½LoRAæƒé‡
    model = PeftModel.from_pretrained(model, lora_model_path)
    
    print(f"âœ… Successfully loaded LoRA model")
    return model, tokenizer


def batch_inference(model, tokenizer, data_root: str, output_dir: str, 
                   split: str = "val", max_samples: int = 10,
                   max_new_tokens: int = 768):
    """æ‰¹é‡æ¨ç†"""
    
    print(f"Starting batch inference on {split} split...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    code_dir = output_dir / "generated_code"
    stl_dir = output_dir / "generated_stl"
    results_dir = output_dir / "results"
    
    code_dir.mkdir(exist_ok=True)
    stl_dir.mkdir(exist_ok=True)
    results_dir.mkdir(exist_ok=True)
    
    # åŠ è½½æ•°æ®ç´¢å¼•
    data_index = load_data_index(Path(data_root), split)
    
    if max_samples > 0:
        data_index = data_index[:max_samples]
    
    print(f"Processing {len(data_index)} samples...")
    
    results = []
    device = next(model.parameters()).device
    
    for i, item in enumerate(data_index):
        try:
            # è·å–æ ·æœ¬ID
            sample_id = item.get("sample_id", item.get("id", f"sample_{i}"))
            print(f"Processing sample {i+1}/{len(data_index)}: {sample_id}")
            
            # åŠ è½½ç‚¹äº‘
            point_cloud = get_cached_point_cloud(Path(data_root), sample_id, 256)
            if point_cloud is None:
                print(f"âš ï¸  No point cloud found for {sample_id}, skipping...")
                continue
            
            # æ¨ç†ç”Ÿæˆä»£ç 
            generated_code = inference_single_sample(
                model, tokenizer, point_cloud, max_new_tokens=max_new_tokens
            )
            
            # ä¿å­˜ç”Ÿæˆçš„ä»£ç 
            code_file = code_dir / f"{sample_id}.py"
            with open(code_file, 'w', encoding='utf-8') as f:
                f.write(generated_code)
            
            # å°è¯•æ‰§è¡Œä»£ç ç”ŸæˆSTL
            stl_generated = False
            stl_file = None
            stl_error = None
            
            try:
                # æ‰§è¡ŒCadQueryä»£ç 
                exec_globals = {}
                exec(generated_code, exec_globals)
                
                # å¯»æ‰¾ç»“æœå˜é‡ï¼ˆé€šå¸¸æ˜¯'r'ï¼‰
                if 'r' in exec_globals:
                    result_obj = exec_globals['r']
                    
                    # å¯¼å‡ºSTL
                    stl_file = stl_dir / f"{sample_id}.stl"
                    result_obj.val().exportStl(str(stl_file))
                    stl_generated = True
                    stl_file = str(stl_file)
                    print(f"âœ… STL generated: {stl_file}")
                else:
                    stl_error = "No result variable 'r' found in generated code"
                    
            except Exception as e:
                stl_error = f"Error executing CadQuery code: {str(e)}"
                print(f"âŒ STL generation failed: {stl_error}")
            
            # åŠ è½½çœŸå®ä»£ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            ground_truth_code = ""
            code_path = item.get("code_path")
            if code_path and Path(code_path).exists():
                with open(code_path, 'r', encoding='utf-8') as f:
                    ground_truth_code = f.read()
            
            # è®°å½•ç»“æœ
            result = {
                "sample_id": sample_id,
                "generated_code": generated_code,
                "timestamp": datetime.now().isoformat(),
                "model_path": str(model.peft_config['default'].base_model_name_or_path if hasattr(model, 'peft_config') else 'merged_model'),
                "code_file": str(code_file),
                "stl_generated": stl_generated,
                "stl_file": stl_file,
                "stl_error": stl_error,
                "ground_truth_code": ground_truth_code
            }
            
            results.append(result)
            
        except Exception as e:
            print(f"âŒ Error processing sample {sample_id}: {e}")
            continue
    
    # ä¿å­˜ç»“æœ
    results_file = results_dir / "inference_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç»Ÿè®¡ç»“æœ
    total_samples = len(results)
    successful_stl = sum(1 for r in results if r["stl_generated"])
    success_rate = successful_stl / total_samples if total_samples > 0 else 0
    
    print(f"\nğŸ“Š Inference Results:")
    print(f"Total samples: {total_samples}")
    print(f"Successful STL generation: {successful_stl}")
    print(f"Success rate: {success_rate:.2%}")
    print(f"Results saved to: {results_file}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CAD-Recode LoRA Model Inference")
    parser.add_argument("--model_path", type=str, required=True, help="LoRA model path")
    parser.add_argument("--data_root", type=str, default="data", help="Data root directory")
    parser.add_argument("--output_dir", type=str, default="inference_results_lora", help="Output directory")
    parser.add_argument("--split", type=str, default="val", choices=["train", "val"], help="Data split")
    parser.add_argument("--max_samples", type=int, default=10, help="Maximum samples to process (0 for all)")
    parser.add_argument("--max_new_tokens", type=int, default=768, help="Maximum new tokens to generate")
    parser.add_argument("--use_merged", action="store_true", help="Use merged model if available")
    
    args = parser.parse_args()
    
    print("="*50)
    print("CAD-Recode LoRA Model Inference")
    print("="*50)
    print(f"Model path: {args.model_path}")
    print(f"Data root: {args.data_root}")
    print(f"Output dir: {args.output_dir}")
    print(f"Split: {args.split}")
    print(f"Max samples: {args.max_samples}")
    print(f"Use merged: {args.use_merged}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_lora_model(args.model_path, use_merged=args.use_merged)
    model = model.to(device)
    model.eval()
    
    # æ‰¹é‡æ¨ç†
    results = batch_inference(
        model=model,
        tokenizer=tokenizer,
        data_root=args.data_root,
        output_dir=args.output_dir,
        split=args.split,
        max_samples=args.max_samples,
        max_new_tokens=args.max_new_tokens
    )
    
    print("ğŸ‰ Inference completed!")


if __name__ == "__main__":
    main()
