#!/usr/bin/env python3
"""
CAD-Recode è®­ç»ƒè„šæœ¬ - åŸºäºè®ºæ–‡ä¼˜åŒ–ç‰ˆ
æ”¯æŒä»ç‚¹äº‘ç”ŸæˆCadQueryä»£ç çš„å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒ

åŸºäºCAD-Recodeè®ºæ–‡çš„å…³é”®ä¼˜åŒ–ï¼š
1. åŠ¨æ€ç‚¹äº‘ç”Ÿæˆä¸ç¼“å­˜
2. æ•°æ®å¢å¼ºï¼ˆé«˜æ–¯å™ªå£°ï¼Œ50%æ¦‚ç‡ï¼Œstd=0.01ï¼‰
3. Test-timeé‡‡æ ·æ¨ç†ï¼ˆç”Ÿæˆ10ç»„ä»£ç ï¼Œé€‰æ‹©æœ€ä½³ï¼‰
4. ä»£ç æç¤ºæ”¯æŒï¼ˆimport cadquery as cqï¼‰
5. ä¼˜åŒ–è®­ç»ƒå‚æ•°ï¼ˆ10ä¸‡æ­¥ï¼Œbatch_size=18ï¼Œlr=2e-4ç­‰ï¼‰

ä½¿ç”¨æ–¹æ³•:
python train_cad_recode.py --config configs/train_config.yaml
"""

import os
import sys
import json
import yaml
import argparse
import logging
import warnings
import signal
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
import math
import random
from tqdm import tqdm

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR

import transformers
from transformers import (
    AutoTokenizer, 
    AutoConfig,
    Qwen2Config,
    Qwen2ForCausalLM, 
    Qwen2Model, 
    PreTrainedModel,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers.trainer_utils import get_last_checkpoint

import tempfile
import cadquery as cq

# å¯¼å…¥SwanLabç”¨äºå®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–
try:
    import swanlab
    from swanlab.integration.huggingface import SwanLabCallback
    SWANLAB_AVAILABLE = True
except ImportError:
    SWANLAB_AVAILABLE = False
    print("Warning: SwanLab not installed. Install with: pip install swanlab")

# å¯¼å…¥OpenCascadeç›¸å…³æ¨¡å—ç”¨äºSTEPæ–‡ä»¶å¤„ç†
try:
    from OCC.Core.STEPControl import STEPControl_Reader
    from OCC.Core.IFSelect import IFSelect_RetDone
    from OCC.Extend.DataExchange import write_stl_file
    OCC_AVAILABLE = True
except ImportError:
    OCC_AVAILABLE = False

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# è®¾ç½®éšæœºç§å­
def set_seed(seed: int = 42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# æ—¥å¿—è®¾ç½®
def setup_logger(log_file: Optional[str] = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger('CADRecode_Training')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

# æ¨¡å‹ç»„ä»¶å®šä¹‰ (æ¥è‡ªdemo.ipynb)
class FourierPointEncoder(nn.Module):
    """å‚…é‡Œå¶ç‚¹äº‘ç¼–ç å™¨"""
    def __init__(self, hidden_size: int):
        super().__init__()
        frequencies = 2.0 ** torch.arange(8, dtype=torch.float32)
        self.register_buffer('frequencies', frequencies, persistent=False)
        self.projection = nn.Linear(51, hidden_size)  # 3 + 8*3*2 = 51

    def forward(self, points: torch.Tensor) -> torch.Tensor:
        # points: [batch_size, n_points, 3]
        x = points
        # è®¡ç®—å‚…é‡Œå¶ç‰¹å¾
        x = (x.unsqueeze(-1) * self.frequencies).view(*x.shape[:-1], -1)
        x = torch.cat((points, x.sin(), x.cos()), dim=-1)
        x = self.projection(x)
        return x


class CADRecode(Qwen2ForCausalLM):
    """CAD-Recodeå¤šæ¨¡æ€æ¨¡å‹"""
    def __init__(self, config):
        PreTrainedModel.__init__(self, config)
        self.model = Qwen2Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        
        # ä¸´æ—¶åˆ‡æ¢åˆ°float32æ¥åˆå§‹åŒ–ç‚¹äº‘ç¼–ç å™¨
        original_dtype = torch.get_default_dtype()
        torch.set_default_dtype(torch.float32)
        self.point_encoder = FourierPointEncoder(config.hidden_size)
        torch.set_default_dtype(original_dtype)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                point_cloud: Optional[torch.Tensor] = None,
                position_ids: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[torch.FloatTensor]] = None,
                inputs_embeds: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                use_cache: Optional[bool] = None,
                output_attentions: Optional[bool] = None,
                output_hidden_states: Optional[bool] = None,
                return_dict: Optional[bool] = None,
                cache_position: Optional[torch.Tensor] = None) -> CausalLMOutputWithPast:
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # åœ¨æ²¡æœ‰past_key_valuesæˆ–è€…æ˜¯æ–°çš„åºåˆ—æ—¶ï¼Œèåˆç‚¹äº‘å’Œæ–‡æœ¬åµŒå…¥
        if past_key_values is None or past_key_values.get_seq_length() == 0:
            assert inputs_embeds is None
            inputs_embeds = self.model.embed_tokens(input_ids)
            
            if point_cloud is not None:
                # ç¼–ç ç‚¹äº‘
                point_embeds = self.point_encoder(point_cloud)
                if point_embeds.dtype != inputs_embeds.dtype:
                    point_embeds = point_embeds.to(inputs_embeds.dtype)
                
                # å°†ç‚¹äº‘åµŒå…¥æ’å…¥åˆ°æŒ‡å®šä½ç½® (attention_mask == -1çš„ä½ç½®)
                point_mask = (attention_mask == -1)
                if point_mask.any():
                    inputs_embeds[point_mask] = point_embeds.reshape(-1, point_embeds.shape[-1])
                    attention_mask[point_mask] = 1
            
            input_ids = None
            position_ids = None

        # é€šè¿‡Qwen2æ¨¡å‹
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position)

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # è®¡ç®—è¯­è¨€å»ºæ¨¡æŸå¤±
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions)

    def prepare_inputs_for_generation(self, *args, **kwargs):
        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)
        if 'point_cloud' in kwargs:
            model_inputs['point_cloud'] = kwargs['point_cloud']
        return model_inputs


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½® - åŸºäºCAD-Recodeè®ºæ–‡ä¼˜åŒ–"""
    # æ¨¡å‹é…ç½®
    base_model_name: str = "Qwen/Qwen2-1.5B"
    model_save_path: str = "./checkpoints"
    
    # æ•°æ®é…ç½®
    train_data_path: str = "./data/train"
    val_data_path: str = "./data/val"
    max_seq_length: int = 1024
    n_points: int = 256
    
    # è®­ç»ƒé…ç½® - åŸºäºè®ºæ–‡ä¼˜åŒ–
    max_training_steps: int = 100000  # è®ºæ–‡ä¸­çš„10ä¸‡æ¬¡è¿­ä»£
    batch_size: int = 4              # è®ºæ–‡æ¨èbatch size
    gradient_accumulation_steps: int = 4  # è°ƒæ•´ä»¥é€‚åº”GPUå†…å­˜
    learning_rate: float = 2e-4       # è®ºæ–‡ä¸­çš„0.0002
    weight_decay: float = 0.01        # è®ºæ–‡è®¾ç½®
    warmup_steps: int = 1000          # è®ºæ–‡ä¸­çš„1åƒæ¬¡warmup
    
    # æ•°æ®å¢å¼ºé…ç½® - åŸºäºè®ºæ–‡
    noise_std: float = 0.01           # é«˜æ–¯å™ªå£°æ ‡å‡†å·®
    noise_probability: float = 0.5    # æ·»åŠ å™ªå£°çš„æ¦‚ç‡
    
    # Test-timeç­–ç•¥é…ç½®
    num_inference_samples: int = 10   # æ¨ç†æ—¶ç”Ÿæˆæ ·æœ¬æ•°
    use_test_time_sampling: bool = True
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer: str = "adamw"
    scheduler: str = "linear_warmup"  # çº¿æ€§warmup + ä½™å¼¦è¡°å‡
    
    # ç³»ç»Ÿé…ç½®
    num_workers: int = 0  # é¿å…å¤šè¿›ç¨‹åµŒå¥—é—®é¢˜
    save_steps: int = 4000           # ä¿å­˜é—´éš”ï¼Œå¿…é¡»æ˜¯eval_stepsçš„å€æ•°
    eval_steps: int = 2000           # è¯„ä¼°é—´éš”
    logging_steps: int = 100
    seed: int = 42
    
    # ç¡¬ä»¶é…ç½®
    device: str = "auto"
    mixed_precision: str = "bf16"     # è®ºæ–‡ä½¿ç”¨çš„ç²¾åº¦
    
    # è®­ç»ƒç­–ç•¥é…ç½®
    use_code_prompt: bool = True      # ä½¿ç”¨ä»£ç æç¤º
    code_prompt: str = "import cadquery as cq\n"  # åˆå§‹ä»£ç token
    
    def __post_init__(self):
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # è®¡ç®—æ€»epochæ•°ï¼ˆåŸºäºæ­¥æ•°ï¼‰
        if hasattr(self, 'train_dataset_size') and self.train_dataset_size > 0:
            effective_batch_size = self.batch_size * self.gradient_accumulation_steps
            self.num_epochs = max(1, self.max_training_steps * effective_batch_size // self.train_dataset_size)
        else:
            # å¦‚æœæ²¡æœ‰æ•°æ®é›†å¤§å°ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.num_epochs = 10


class CADDataset(Dataset):
    """CADæ•°æ®é›†ç±» - åŠ¨æ€ç”Ÿæˆç‚¹äº‘ç‰ˆæœ¬"""
    
    def __init__(self, data_path: str, tokenizer, config: TrainingConfig, split: str = "train"):
        self.data_path = Path(data_path)
        self.tokenizer = tokenizer
        self.config = config
        self.split = split
        self.logger = logging.getLogger('CADRecode_Training')
        
        # ç‚¹äº‘ç¼“å­˜ç›®å½•
        self.cache_dir = self.data_path / "point_cloud_cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # é”™è¯¯æ ·æœ¬è·Ÿè¸ª
        self.error_samples_file = self.data_path / "error_samples.json"
        self.error_samples = self._load_error_samples()
        
        # åŠ è½½æ•°æ®ç´¢å¼•
        self.data_list = self._load_data_index()
        
        # è¿‡æ»¤é”™è¯¯æ ·æœ¬
        self._filter_error_samples()
        
        self.logger.info(f"Loaded {len(self.data_list)} samples for {split} split")
        if len(self.error_samples) > 0:
            self.logger.info(f"Skipping {len(self.error_samples)} known error samples")
    
    def _load_error_samples(self) -> set:
        """åŠ è½½å·²çŸ¥çš„é”™è¯¯æ ·æœ¬åˆ—è¡¨"""
        if self.error_samples_file.exists():
            try:
                with open(self.error_samples_file, 'r') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()
    
    def _save_error_samples(self):
        """ä¿å­˜é”™è¯¯æ ·æœ¬åˆ—è¡¨"""
        try:
            with open(self.error_samples_file, 'w') as f:
                json.dump(list(self.error_samples), f)
        except Exception as e:
            self.logger.warning(f"Failed to save error samples: {e}")
    
    def _filter_error_samples(self):
        """è¿‡æ»¤æ‰å·²çŸ¥çš„é”™è¯¯æ ·æœ¬"""
        if self.error_samples:
            original_count = len(self.data_list)
            self.data_list = [item for item in self.data_list 
                            if item["sample_id"] not in self.error_samples]
            filtered_count = original_count - len(self.data_list)
            if filtered_count > 0:
                self.logger.info(f"Filtered out {filtered_count} known error samples")
    
    def _mark_sample_as_error(self, sample_id: str):
        """å°†æ ·æœ¬æ ‡è®°ä¸ºé”™è¯¯æ ·æœ¬"""
        if sample_id not in self.error_samples:
            self.error_samples.add(sample_id)
            self._save_error_samples()
            self.logger.warning(f"Marked sample {sample_id} as error sample")
    
    def _load_data_index(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®ç´¢å¼•æ–‡ä»¶"""
        index_file = self.data_path / f"{self.split}_index.json"
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            self.logger.error(f"Index file not found: {index_file}")
            return []
    
    def _execute_cadquery_code_safe(self, py_code: str, timeout: int = 10) -> Optional[np.ndarray]:
        """å®‰å…¨æ‰§è¡ŒCadQueryä»£ç å¹¶ç”Ÿæˆç‚¹äº‘ - å¼ºåŒ–è¿›ç¨‹éš”ç¦»ç‰ˆæœ¬"""
        import subprocess
        import tempfile
        import signal
        import pickle
        import base64
        
        def write_execution_script(code: str, result_file: str, sample_id: str) -> str:
            """åˆ›å»ºç‹¬ç«‹çš„æ‰§è¡Œè„šæœ¬"""
            script_content = f'''#!/usr/bin/env python3
import sys
import os
import contextlib
import io
import tempfile
import signal
import pickle
import traceback

# è®¾ç½®ä¿¡å·å¤„ç†ï¼Œé˜²æ­¢æ— é™æŒ‚èµ·
def timeout_handler(signum, frame):
    sys.exit(124)  # è¶…æ—¶é€€å‡ºç 

signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm({timeout - 1})  # ç•™1ç§’ç»™cleanup

try:
    # å¯¼å…¥æ‰€éœ€æ¨¡å—
    import cadquery as cq
    import trimesh
    import numpy as np
    import torch
    from pytorch3d.ops import sample_farthest_points
    
    # é‡å®šå‘æ‰€æœ‰è¾“å‡º
    with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):
        try:
            # ä»£ç æ‰§è¡Œ
            code = {repr(code)}
            exec_globals = {{
                'cadquery': cq,
                'cq': cq,
                '__builtins__': __builtins__
            }}
            
            exec(code, exec_globals)
            
            if 'r' in exec_globals and exec_globals['r'] is not None:
                compound = exec_globals['r'].val()
                
                # ç”Ÿæˆä¸´æ—¶STEPæ–‡ä»¶
                temp_step_path = f"/tmp/temp_model_{{os.getpid()}}_{sample_id}_{{hash(code) % 10000}}.step"
                
                try:
                    # å¯¼å‡ºSTEPæ–‡ä»¶
                    cq.exporters.export(compound, temp_step_path)
                    
                    # è½¬æ¢ä¸ºç‚¹äº‘
                    if temp_step_path.lower().endswith('.stl'):
                        mesh = trimesh.load_mesh(temp_step_path)
                    else:
                        try:
                            mesh = trimesh.load_mesh(temp_step_path, force='mesh')
                        except:
                            # tessellation fallback
                            mesh_data = compound.tessellate(0.1)
                            if len(mesh_data[0]) > 0:
                                vertices = np.array([(v.x, v.y, v.z) for v in mesh_data[0]])
                                faces = np.array(mesh_data[1])
                                mesh = trimesh.Trimesh(vertices=vertices, faces=faces)
                            else:
                                sys.exit(1)
                    
                    # æ ‡å‡†åŒ–ç½‘æ ¼
                    if not mesh.is_empty and mesh.vertices is not None and len(mesh.vertices) > 0:
                        # ç§»åŠ¨åˆ°åŸç‚¹
                        mesh.apply_translation(-(mesh.bounds[0] + mesh.bounds[1]) / 2.0)
                        # ç¼©æ”¾åˆ°å•ä½ç«‹æ–¹ä½“
                        if max(mesh.extents) > 0:
                            mesh.apply_scale(2.0 / max(mesh.extents))
                        
                        # é‡‡æ ·ç‚¹äº‘
                        vertices, _ = trimesh.sample.sample_surface(mesh, 8192)
                        
                        # ä½¿ç”¨æœ€è¿œç‚¹é‡‡æ ·
                        if len(vertices) >= 256:
                            vertices_tensor = torch.tensor(vertices, dtype=torch.float32).unsqueeze(0)
                            _, ids = sample_farthest_points(vertices_tensor, K=256)
                            ids = ids[0].numpy()
                            selected_points = vertices[ids]
                        else:
                            indices = np.random.choice(len(vertices), 256, replace=True)
                            selected_points = vertices[indices]
                        
                        # ä¿å­˜ç»“æœ
                        with open("{result_file}", "wb") as f:
                            pickle.dump(selected_points.astype(np.float32), f)
                        
                        sys.exit(0)  # æˆåŠŸ
                    else:
                        sys.exit(2)  # ç©ºç½‘æ ¼
                        
                except Exception as e:
                    sys.exit(3)  # ç½‘æ ¼å¤„ç†é”™è¯¯
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_step_path):
                        os.remove(temp_step_path)
            else:
                sys.exit(4)  # æ²¡æœ‰ç»“æœ
                
        except Exception as e:
            sys.exit(5)  # ä»£ç æ‰§è¡Œé”™è¯¯
            
except Exception as e:
    sys.exit(6)  # å¯¼å…¥æˆ–å…¶ä»–é”™è¯¯
finally:
    signal.alarm(0)  # å–æ¶ˆalarm
'''
            return script_content
        
        try:
            sample_id = abs(hash(py_code)) % 100000
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºç»“æœä¼ é€’
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as script_file:
                script_path = script_file.name
                script_content = write_execution_script(py_code, script_path + '.result', sample_id)
                script_file.write(script_content)
            
            result_file = script_path + '.result'
            
            try:
                # ä½¿ç”¨subprocessæ‰§è¡Œè„šæœ¬ï¼Œå®Œå…¨éš”ç¦»
                result = subprocess.run(
                    [sys.executable, script_path],
                    timeout=timeout,
                    capture_output=True,
                    text=True,
                    cwd='/tmp',  # åœ¨tmpç›®å½•æ‰§è¡Œ
                    env={**os.environ, 'PYTHONPATH': '/tmp'}  # é™åˆ¶ç¯å¢ƒ
                )
                
                # æ£€æŸ¥æ‰§è¡Œç»“æœ
                if result.returncode == 0 and os.path.exists(result_file):
                    try:
                        with open(result_file, 'rb') as f:
                            point_cloud = pickle.load(f)
                        return point_cloud
                    except Exception as e:
                        self.logger.debug(f"Failed to load result: {e}")
                        return None
                else:
                    # è®°å½•é”™è¯¯ä»£ç 
                    error_messages = {
                        1: "Empty tessellation",
                        2: "Empty mesh",
                        3: "Mesh processing error",
                        4: "No result variable",
                        5: "Code execution error",
                        6: "Import/system error",
                        124: "Timeout",
                        -9: "Killed",
                        -11: "Segmentation fault"
                    }
                    error_msg = error_messages.get(result.returncode, f"Unknown error {result.returncode}")
                    self.logger.debug(f"CadQuery execution failed: {error_msg}")
                    return None
                    
            except subprocess.TimeoutExpired:
                self.logger.debug(f"CadQuery execution timeout after {timeout}s")
                return None
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                for temp_file in [script_path, result_file]:
                    if os.path.exists(temp_file):
                        try:
                            os.remove(temp_file)
                        except:
                            pass
                            
        except Exception as e:
            self.logger.debug(f"Error in CadQuery execution setup: {e}")
            return None
                
        except Exception as e:
            self.logger.debug(f"Process execution failed: {e}")
            return None
    
    def _step_to_point_cloud(self, step_file_path: str) -> Optional[np.ndarray]:
        """å°†STEPæ–‡ä»¶è½¬æ¢ä¸ºç‚¹äº‘"""
        try:
            import trimesh
            from pytorch3d.ops import sample_farthest_points
            import torch
            
            if not OCC_AVAILABLE:
                # å¦‚æœæ²¡æœ‰OpenCascadeï¼Œå°è¯•ç›´æ¥åŠ è½½STLæ ¼å¼
                if step_file_path.lower().endswith('.stl'):
                    mesh = trimesh.load_mesh(step_file_path)
                else:
                    return None
            else:
                # ä½¿ç”¨OpenCascadeå¤„ç†STEPæ–‡ä»¶
                with tempfile.NamedTemporaryFile(suffix=".stl", delete=False) as tmp_file:
                    temp_stl_path = tmp_file.name
                
                try:
                    # è¯»å–STEPæ–‡ä»¶
                    step_reader = STEPControl_Reader()
                    status = step_reader.ReadFile(step_file_path)
                    
                    if status == IFSelect_RetDone:
                        step_reader.TransferRoots()
                        shape = step_reader.Shape()
                        
                        # å¯¼å‡ºä¸ºSTL
                        write_stl_file(shape, temp_stl_path)
                        
                        # ä½¿ç”¨trimeshåŠ è½½STL
                        mesh = trimesh.load_mesh(temp_stl_path)
                    else:
                        return None
                        
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_stl_path):
                        os.remove(temp_stl_path)
            
            # æ ‡å‡†åŒ–ç½‘æ ¼
            if not mesh.is_empty and mesh.vertices is not None:
                # ç§»åŠ¨åˆ°åŸç‚¹
                mesh.apply_translation(-(mesh.bounds[0] + mesh.bounds[1]) / 2.0)
                # ç¼©æ”¾åˆ°å•ä½ç«‹æ–¹ä½“
                if max(mesh.extents) > 0:
                    mesh.apply_scale(2.0 / max(mesh.extents))
                
                # é‡‡æ ·ç‚¹äº‘
                vertices, _ = trimesh.sample.sample_surface(mesh, 8192)
                
                # ä½¿ç”¨æœ€è¿œç‚¹é‡‡æ ·
                if len(vertices) >= self.config.n_points:
                    vertices_tensor = torch.tensor(vertices, dtype=torch.float32).unsqueeze(0)
                    _, ids = sample_farthest_points(vertices_tensor, K=self.config.n_points)
                    ids = ids[0].numpy()
                    selected_points = vertices[ids]
                else:
                    # å¦‚æœç‚¹æ•°ä¸å¤Ÿï¼Œè¿›è¡Œé‡å¤é‡‡æ ·
                    indices = np.random.choice(len(vertices), self.config.n_points, replace=True)
                    selected_points = vertices[indices]
                
                return selected_points.astype(np.float32)
            
            return None
            
        except Exception as e:
            self.logger.warning(f"Error converting to point cloud: {e}")
            return None
    
    def _get_point_cloud_from_code(self, code_content: str, sample_id: str) -> Optional[np.ndarray]:
        """ä»CadQueryä»£ç ç”Ÿæˆç‚¹äº‘ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼ŒåŒ…å«é”™è¯¯æ ·æœ¬è·Ÿè¸ª"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥é”™è¯¯æ ·æœ¬
        if sample_id in self.error_samples:
            return None
            
        # æ£€æŸ¥ç¼“å­˜
        cache_file = self.cache_dir / f"{sample_id}.npy"
        if cache_file.exists():
            try:
                return np.load(cache_file)
            except:
                # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤å¹¶é‡æ–°ç”Ÿæˆ
                cache_file.unlink(missing_ok=True)
        
        # ç”Ÿæˆç‚¹äº‘ï¼ˆä½¿ç”¨è¿›ç¨‹éš”ç¦»ï¼‰
        try:
            point_cloud = self._execute_cadquery_code_safe(code_content, timeout=10)
            
            if point_cloud is not None:
                # ä¿å­˜åˆ°ç¼“å­˜
                try:
                    np.save(cache_file, point_cloud)
                except Exception as e:
                    self.logger.warning(f"Failed to cache point cloud for {sample_id}: {e}")
                return point_cloud
            else:
                # æ‰§è¡Œå¤±è´¥ï¼Œæ ‡è®°ä¸ºé”™è¯¯æ ·æœ¬
                self._mark_sample_as_error(sample_id)
                return None
                
        except Exception as e:
            # è¿›ç¨‹æ‰§è¡Œå‡ºç°ä¸¥é‡é”™è¯¯ï¼ˆå¦‚æ®µé”™è¯¯ï¼‰ï¼Œæ ‡è®°ä¸ºé”™è¯¯æ ·æœ¬
            self.logger.error(f"Serious error processing sample {sample_id}: {e}")
            self._mark_sample_as_error(sample_id)
            return None
    
    def __len__(self) -> int:
        return len(self.data_list)
    
    def _apply_data_augmentation(self, point_cloud: np.ndarray) -> np.ndarray:
        """åº”ç”¨æ•°æ®å¢å¼º - åŸºäºCAD-Recodeè®ºæ–‡ç­–ç•¥"""
        if self.split == "train" and np.random.random() < self.config.noise_probability:
            # æ·»åŠ é«˜æ–¯å™ªå£°ï¼ˆæ ‡å‡†å·®0.01ï¼Œ50%æ¦‚ç‡ï¼‰
            noise = np.random.normal(0, self.config.noise_std, point_cloud.shape)
            point_cloud = point_cloud + noise.astype(np.float32)
            
            self.logger.debug(f"Applied noise augmentation with std={self.config.noise_std}")
        
        return point_cloud
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–å•ä¸ªæ•°æ®æ ·æœ¬ - ä¼˜åŒ–ç‰ˆ"""
        item = self.data_list[idx]
        sample_id = item["sample_id"]
        
        # åŠ è½½CadQueryä»£ç 
        try:
            with open(item["code_path"], 'r', encoding='utf-8') as f:
                cadquery_code = f.read().strip()
        except Exception as e:
            self.logger.error(f"Failed to load code for {sample_id}: {e}")
            return self._get_default_sample()
        
        # ç”Ÿæˆç‚¹äº‘
        point_cloud = self._get_point_cloud_from_code(cadquery_code, sample_id)
        if point_cloud is None:
            self.logger.warning(f"Failed to generate point cloud for {sample_id}, using random points")
            point_cloud = np.random.randn(self.config.n_points, 3).astype(np.float32)
        
        # åº”ç”¨æ•°æ®å¢å¼º
        point_cloud = self._apply_data_augmentation(point_cloud)
        
        # ç¡®ä¿ç‚¹äº‘å½¢çŠ¶æ­£ç¡®
        if point_cloud.shape[0] != self.config.n_points:
            if point_cloud.shape[0] > self.config.n_points:
                indices = np.random.choice(point_cloud.shape[0], self.config.n_points, replace=False)
                point_cloud = point_cloud[indices]
            else:
                indices = np.random.choice(point_cloud.shape[0], self.config.n_points, replace=True)
                point_cloud = point_cloud[indices]
        
        # æ„å»ºæç¤ºæ¨¡æ¿ - æ·»åŠ ä»£ç æç¤º
        if self.config.use_code_prompt:
            prompt = "<|im_start|>" + self.config.code_prompt
            full_text = prompt + cadquery_code + "<|endoftext|>"
        else:
            prompt = "<|im_start|>"
            full_text = prompt + cadquery_code + "<|endoftext|>"
        
        # åˆ†è¯
        try:
            tokenized = self.tokenizer(
                full_text,
                truncation=True,
                max_length=self.config.max_seq_length,
                padding=False,
                return_tensors="pt"
            )
            input_ids = tokenized["input_ids"].squeeze(0)
        except Exception as e:
            self.logger.error(f"Tokenization failed for {sample_id}: {e}")
            return self._get_default_sample()
        
        # åˆ›å»ºattention_maskï¼Œç‚¹äº‘ä½ç½®æ ‡è®°ä¸º-1
        attention_mask = torch.ones_like(input_ids)
        
        # ä¸ºç‚¹äº‘é¢„ç•™ä½ç½® (åœ¨promptä¹‹å‰)
        point_mask = torch.full((self.config.n_points,), -1, dtype=torch.long)
        prompt_start_token = self.tokenizer("<|im_start|>")["input_ids"][0]
        
        # æ‰¾åˆ°promptå¼€å§‹ä½ç½®
        prompt_start_indices = (input_ids == prompt_start_token).nonzero(as_tuple=True)[0]
        if len(prompt_start_indices) > 0:
            prompt_start_idx = prompt_start_indices[0].item()
            has_prompt = True
        else:
            prompt_start_idx = -1  # è¡¨ç¤ºæ²¡æœ‰æ‰¾åˆ°prompt
            has_prompt = False
            
        # æ’å…¥ç‚¹äº‘æ©ç 
        input_ids_with_points = torch.cat([
            torch.full((self.config.n_points,), self.tokenizer.pad_token_id, dtype=torch.long),
            input_ids
        ])
        attention_mask_with_points = torch.cat([
            point_mask,
            attention_mask
        ])
        
        # åˆ›å»ºlabels (åªå¯¹ä»£ç éƒ¨åˆ†è®¡ç®—æŸå¤±)
        labels = input_ids_with_points.clone()
        labels[:self.config.n_points] = -100  # ç‚¹äº‘éƒ¨åˆ†ä¸è®¡ç®—æŸå¤±
        
        # æ‰¾åˆ°å®é™…ä»£ç å¼€å§‹ä½ç½®ï¼ˆè·³è¿‡promptï¼‰
        if self.config.use_code_prompt:
            # å¦‚æœä½¿ç”¨ä»£ç æç¤ºï¼Œè·³è¿‡importè¯­å¥
            code_prompt_tokens = self.tokenizer(self.config.code_prompt)["input_ids"]
            code_start_offset = len(code_prompt_tokens)
        else:
            code_start_offset = 1  # åªè·³è¿‡<|im_start|>
        
        if has_prompt:
            code_start_idx = self.config.n_points + prompt_start_idx + code_start_offset
            labels[:code_start_idx] = -100
        
        return {
            "input_ids": input_ids_with_points,
            "attention_mask": attention_mask_with_points,
            "labels": labels,
            "point_cloud": torch.tensor(point_cloud, dtype=torch.float32)
        }
    
    def _get_default_sample(self) -> Dict[str, torch.Tensor]:
        """è·å–é»˜è®¤æ ·æœ¬ï¼ˆç”¨äºé”™è¯¯æƒ…å†µï¼‰"""
        # ç”Ÿæˆæœ€å°åŒ–çš„é»˜è®¤æ•°æ®
        default_text = "<|im_start|>r = cq.Workplane().box(1, 1, 1)<|endoftext|>"
        tokenized = self.tokenizer(
            default_text,
            truncation=True,
            max_length=self.config.max_seq_length,
            padding=False,
            return_tensors="pt"
        )
        
        input_ids = tokenized["input_ids"].squeeze(0)
        
        # æ·»åŠ ç‚¹äº‘ä½ç½®
        point_mask = torch.full((self.config.n_points,), -1, dtype=torch.long)
        input_ids_with_points = torch.cat([
            torch.full((self.config.n_points,), self.tokenizer.pad_token_id, dtype=torch.long),
            input_ids
        ])
        attention_mask_with_points = torch.cat([
            point_mask,
            torch.ones_like(input_ids)
        ])
        
        labels = input_ids_with_points.clone()
        labels[:self.config.n_points + 1] = -100  # ç‚¹äº‘å’Œpromptä¸è®¡ç®—æŸå¤±
        
        return {
            "input_ids": input_ids_with_points,
            "attention_mask": attention_mask_with_points,
            "labels": labels,
            "point_cloud": torch.randn(self.config.n_points, 3, dtype=torch.float32)
        }


class CADDataCollator:
    """CADæ•°æ®æ•´ç†å™¨"""
    
    def __init__(self, tokenizer, pad_token_id: Optional[int] = None):
        self.tokenizer = tokenizer
        self.pad_token_id = pad_token_id or tokenizer.pad_token_id
    
    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """å®‰å…¨çš„æ•°æ®æ•´ç†ï¼Œå¸¦é”™è¯¯å¤„ç†"""
        try:
            # è¿‡æ»¤æ— æ•ˆçš„features
            valid_features = []
            for feature in features:
                if (feature is not None and 
                    "input_ids" in feature and 
                    "attention_mask" in feature and 
                    "labels" in feature and 
                    "point_cloud" in feature):
                    valid_features.append(feature)
            
            if not valid_features:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆfeaturesï¼Œè¿”å›é»˜è®¤batch
                return self._get_default_batch()
            
            # è·å–æœ€å¤§åºåˆ—é•¿åº¦
            max_length = max(f["input_ids"].size(0) for f in valid_features)
            
            batch_input_ids = []
            batch_attention_mask = []
            batch_labels = []
            batch_point_clouds = []
            
            for feature in valid_features:
                try:
                    input_ids = feature["input_ids"]
                    attention_mask = feature["attention_mask"]
                    labels = feature["labels"]
                    point_cloud = feature["point_cloud"]
                    
                    # å¡«å……åºåˆ—
                    padding_length = max_length - input_ids.size(0)
                    if padding_length > 0:
                        input_ids = torch.cat([
                            input_ids,
                            torch.full((padding_length,), self.pad_token_id, dtype=input_ids.dtype)
                        ])
                        attention_mask = torch.cat([
                            attention_mask,
                            torch.zeros(padding_length, dtype=attention_mask.dtype)
                        ])
                        labels = torch.cat([
                            labels,
                            torch.full((padding_length,), -100, dtype=labels.dtype)
                        ])
                    
                    batch_input_ids.append(input_ids)
                    batch_attention_mask.append(attention_mask)
                    batch_labels.append(labels)
                    batch_point_clouds.append(point_cloud)
                    
                except Exception as e:
                    # å¦‚æœå•ä¸ªfeatureå¤„ç†å¤±è´¥ï¼Œè·³è¿‡å®ƒ
                    continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ‰¹æ¬¡æ•°æ®
            if not batch_input_ids:
                return self._get_default_batch()
            
            return {
                "input_ids": torch.stack(batch_input_ids),
                "attention_mask": torch.stack(batch_attention_mask),
                "labels": torch.stack(batch_labels),
                "point_cloud": torch.stack(batch_point_clouds)
            }
            
        except Exception as e:
            # å¦‚æœæ•´ä¸ªæ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œè¿”å›é»˜è®¤æ‰¹æ¬¡
            import logging
            logger = logging.getLogger('CADRecode_Training')
            logger.warning(f"Error in data collator: {e}, returning default batch")
            return self._get_default_batch()
    
    def _get_default_batch(self) -> Dict[str, torch.Tensor]:
        """è¿”å›é»˜è®¤çš„å®‰å…¨æ‰¹æ¬¡"""
        batch_size = 1
        seq_len = 128
        n_points = 256
        
        return {
            "input_ids": torch.full((batch_size, seq_len), self.pad_token_id, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len, dtype=torch.long),
            "labels": torch.full((batch_size, seq_len), -100, dtype=torch.long),
            "point_cloud": torch.randn(batch_size, n_points, 3, dtype=torch.float32)
        }


class CADTrainer(Trainer):
    """è‡ªå®šä¹‰Trainerç±» - å¢å¼ºæ—¥å¿—è®°å½•å’Œé”™è¯¯å¤„ç†"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_history = {
            'train_loss': [],
            'eval_loss': [],
            'learning_rate': [],
            'steps': []
        }
        self.error_count = 0
        self.max_consecutive_errors = 5
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """è®¡ç®—æŸå¤± - å…¼å®¹transformersæ–°ç‰ˆæœ¬å¹¶å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            labels = inputs.get("labels")
            if labels is not None:
                # ç§»é™¤-100æ ‡è®°çš„ä½ç½®ï¼ˆä¸å‚ä¸æŸå¤±è®¡ç®—ï¼‰
                valid_mask = (labels != -100)
                if not valid_mask.any():
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œè¿”å›é›¶æŸå¤±
                    loss = torch.tensor(0.0, device=labels.device, requires_grad=True)
                    if return_outputs:
                        # åˆ›å»ºè™šæ‹Ÿè¾“å‡º
                        outputs = type('MockOutputs', (), {
                            'loss': loss,
                            'logits': torch.zeros(labels.shape + (model.config.vocab_size,), device=labels.device)
                        })()
                        return (loss, outputs)
                    return loss
            
            outputs = model(**inputs)
            loss = outputs.loss if hasattr(outputs, 'loss') else outputs.get('loss')
            
            if loss is None:
                # å¦‚æœæ¨¡å‹æ²¡æœ‰è¿”å›æŸå¤±ï¼Œæ‰‹åŠ¨è®¡ç®—
                logits = outputs.logits if hasattr(outputs, 'logits') else outputs.get('logits')
                if logits is not None and labels is not None:
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
                else:
                    loss = torch.tensor(0.0, device=inputs.get('input_ids', torch.tensor([])).device, requires_grad=True)
            
            # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(loss) or torch.isinf(loss):
                self.error_count += 1
                self.logger.warning(f"Invalid loss detected (NaN/Inf), error count: {self.error_count}")
                loss = torch.tensor(0.1, device=loss.device, requires_grad=True)  # ä½¿ç”¨å°çš„é»˜è®¤æŸå¤±
                
                if self.error_count >= self.max_consecutive_errors:
                    self.logger.error(f"Too many consecutive errors ({self.error_count}), may need to check data quality")
            else:
                self.error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
            
            return (loss, outputs) if return_outputs else loss
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Error in compute_loss: {e}, error count: {self.error_count}")
            
            # è¿”å›å®‰å…¨çš„é»˜è®¤æŸå¤±
            device = inputs.get('input_ids', torch.tensor([])).device
            loss = torch.tensor(0.1, device=device, requires_grad=True)
            
            if return_outputs:
                # åˆ›å»ºè™šæ‹Ÿè¾“å‡ºä»¥é¿å…åç»­é”™è¯¯
                outputs = type('MockOutputs', (), {
                    'loss': loss,
                    'logits': torch.zeros((1, 1, getattr(model.config, 'vocab_size', 32000)), device=device)
                })()
                return (loss, outputs)
            
            return loss
    
    def log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None:
        """å¢å¼ºçš„æ—¥å¿—è®°å½•åŠŸèƒ½ - å…¼å®¹æ–°ç‰ˆtransformers"""
        # è°ƒç”¨çˆ¶ç±»çš„logæ–¹æ³•ï¼Œä¼ é€’æ‰€æœ‰å‚æ•°
        if start_time is not None:
            super().log(logs, start_time)
        else:
            super().log(logs)
        
        # è®°å½•åˆ°å†å²
        if 'train_loss' in logs:
            self.loss_history['train_loss'].append(logs['train_loss'])
            self.loss_history['steps'].append(self.state.global_step)
        
        if 'eval_loss' in logs:
            self.loss_history['eval_loss'].append(logs['eval_loss'])
        
        if 'learning_rate' in logs:
            self.loss_history['learning_rate'].append(logs['learning_rate'])
        
        # è¯¦ç»†æ—¥å¿—æ ¼å¼
        step = self.state.global_step
        epoch = self.state.epoch
        
        log_msg = f"Step {step:6d} | Epoch {epoch:.2f}"
        
        if 'train_loss' in logs:
            log_msg += f" | Train Loss: {logs['train_loss']:.6f}"
        
        if 'eval_loss' in logs:
            log_msg += f" | Eval Loss: {logs['eval_loss']:.6f}"
        
        if 'learning_rate' in logs:
            log_msg += f" | LR: {logs['learning_rate']:.2e}"
        
        # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            memory_used = torch.cuda.max_memory_allocated() / 1024**3  # GB
            log_msg += f" | GPU Mem: {memory_used:.1f}GB"
        
        # è®°å½•åˆ°logger
        logger = logging.getLogger('CADRecode_Training')
        logger.info(log_msg)
        
        # æ¯1000æ­¥ä¿å­˜æŸå¤±å†å²
        if step % 1000 == 0:
            self.save_loss_history()
    
    def save_loss_history(self):
        """ä¿å­˜æŸå¤±å†å²åˆ°æ–‡ä»¶"""
        import json
        history_file = Path(self.args.output_dir) / "loss_history.json"
        try:
            with open(history_file, 'w') as f:
                json.dump(self.loss_history, f, indent=2)
        except Exception as e:
            logger = logging.getLogger('CADRecode_Training')
            logger.warning(f"Failed to save loss history: {e}")
    
    def on_evaluate(self, args, state, control, model=None, **kwargs):
        """è¯„ä¼°æ—¶çš„å›è°ƒ"""
        super().on_evaluate(args, state, control, model, **kwargs)
        
        # åˆ›å»ºå³æ—¶å¯è§†åŒ–
        if state.global_step % 5000 == 0:  # æ¯5000æ­¥ç”Ÿæˆä¸€æ¬¡å›¾è¡¨
            try:
                self.create_loss_plot()
            except Exception as e:
                logger = logging.getLogger('CADRecode_Training')
                logger.warning(f"Failed to create loss plot: {e}")
    
    def create_loss_plot(self):
        """Create loss curve plot"""
        try:
            import matplotlib.pyplot as plt
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
            
            # Training loss
            if self.loss_history['train_loss']:
                steps = self.loss_history['steps'][-len(self.loss_history['train_loss']):]
                ax1.plot(steps, self.loss_history['train_loss'], 'b-', alpha=0.7)
                ax1.set_title('Training Loss')
                ax1.set_xlabel('Steps')
                ax1.set_ylabel('Loss')
                ax1.grid(True, alpha=0.3)
            
            # Learning rate
            if self.loss_history['learning_rate']:
                steps = self.loss_history['steps'][-len(self.loss_history['learning_rate']):]
                ax2.plot(steps, self.loss_history['learning_rate'], 'orange')
                ax2.set_title('Learning Rate')
                ax2.set_xlabel('Steps')
                ax2.set_ylabel('Learning Rate')
                ax2.set_yscale('log')
                ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plot_path = Path(self.args.output_dir) / f"loss_plot_step_{self.state.global_step}.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            logger = logging.getLogger('CADRecode_Training')
            logger.info(f"ğŸ“Š Loss plot saved: {plot_path}")
            
        except ImportError:
            pass  # Skip if matplotlib is not available


def create_model_and_tokenizer(config: TrainingConfig) -> Tuple[CADRecode, AutoTokenizer]:
    """åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆä¿®å¤æ³¨æ„åŠ›æœºåˆ¶è­¦å‘Šï¼‰"""
    logger = logging.getLogger('CADRecode_Training')
    
    # åŠ è½½åˆ†è¯å™¨
    logger.info(f"Loading tokenizer from {config.base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(
        config.base_model_name,
        pad_token='<|im_end|>',
        padding_side='left',
        trust_remote_code=True
    )
    
    # åŠ è½½æ¨¡å‹é…ç½®å¹¶ä¿®å¤æ»‘åŠ¨çª—å£æ³¨æ„åŠ›é—®é¢˜
    model_config = AutoConfig.from_pretrained(config.base_model_name, trust_remote_code=True)
    
    # ç¦ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä»¥é¿å…å®ç°è­¦å‘Š
    if hasattr(model_config, 'sliding_window'):
        model_config.sliding_window = None
        logger.info("Disabled sliding window attention to avoid implementation issues")
    
    # å¼ºåˆ¶ä½¿ç”¨ 'eager' (é»˜è®¤) å®ç°ä»¥é¿å… Qwen2+FlashAttention+å³å¡«å……çš„bug
    # attn_implementation = "eager"
    # logger.warning("Flash Attention has been forcibly disabled. Using 'eager' attention implementation.")
    
    # æ ¹æ®ç”¨æˆ·è¯·æ±‚ï¼Œé‡æ–°å¯ç”¨Flash Attention 2ä»¥èŠ‚çœå†…å­˜
    attn_implementation = "flash_attention_2"
    logger.info("Using 'flash_attention_2' attention implementation as requested to save memory.")

    try:
        model = CADRecode.from_pretrained(
            config.base_model_name,
            torch_dtype=torch.bfloat16 if config.mixed_precision == "bf16" else torch.float16,
            trust_remote_code=True
        )
        
        # å¤åˆ¶é¢„è®­ç»ƒæƒé‡
        model.model.load_state_dict(base_model.model.state_dict(), strict=False)
        model.lm_head.load_state_dict(base_model.lm_head.state_dict(), strict=False)
        
        logger.info("Successfully loaded pretrained weights")
        del base_model  # é‡Šæ”¾å†…å­˜
    except Exception as e:
        logger.warning(f"Could not load pretrained weights: {e}")
    
    return model, tokenizer


def train_model(config: TrainingConfig):
    """ä¸»è®­ç»ƒå‡½æ•° - åŸºäºCAD-Recodeè®ºæ–‡ä¼˜åŒ–"""
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.model_save_path, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(config.model_save_path, "training.log")
    logger = setup_logger(log_file)
    
    logger.info("Starting CAD-Recode training with optimized strategy")
    logger.info(f"Training strategy based on CAD-Recode paper:")
    logger.info(f"  - Max training steps: {config.max_training_steps}")
    logger.info(f"  - Batch size: {config.batch_size}")
    logger.info(f"  - Learning rate: {config.learning_rate}")
    logger.info(f"  - Warmup steps: {config.warmup_steps}")
    logger.info(f"  - Data augmentation: noise_std={config.noise_std}, prob={config.noise_probability}")
    logger.info(f"Configuration: {config}")
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = create_model_and_tokenizer(config)
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("Loading datasets...")
    train_dataset = CADDataset(config.train_data_path, tokenizer, config, "train")
    val_dataset = CADDataset(config.val_data_path, tokenizer, config, "val")
    
    # æ›´æ–°é…ç½®ä¸­çš„æ•°æ®é›†å¤§å°ï¼ˆç”¨äºè®¡ç®—epochæ•°ï¼‰
    config.train_dataset_size = len(train_dataset)
    config.__post_init__()
    
    logger.info(f"Dataset sizes: train={len(train_dataset)}, val={len(val_dataset)}")
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    data_collator = CADDataCollator(tokenizer)
    
    # è®¡ç®—è®­ç»ƒå‚æ•°
    effective_batch_size = config.batch_size * config.gradient_accumulation_steps
    total_steps = config.max_training_steps
    estimated_epochs = max(1, total_steps * effective_batch_size // len(train_dataset))
    
    logger.info(f"Training parameters:")
    logger.info(f"  - Effective batch size: {effective_batch_size}")
    logger.info(f"  - Total training steps: {total_steps}")
    logger.info(f"  - Estimated epochs: {estimated_epochs}")
    
    # è®¾ç½®è®­ç»ƒå‚æ•° - åŸºäºè®ºæ–‡ä¼˜åŒ–
    training_args = TrainingArguments(
        output_dir=config.model_save_path,
        num_train_epochs=1,  # Set epochs to 1, control training with max_steps
        max_steps=config.max_training_steps,
        per_device_train_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        warmup_steps=config.warmup_steps,
        weight_decay=config.weight_decay,
        learning_rate=config.learning_rate,
        bf16=config.mixed_precision == 'bf16',
        fp16=config.mixed_precision == 'fp16',
        logging_steps=config.logging_steps,
        save_steps=config.save_steps,
        eval_steps=config.eval_steps,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True if config.val_data_path else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        ddp_find_unused_parameters=False,
        # gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
        gradient_checkpointing_kwargs={'use_reentrant': False},
        dataloader_num_workers=config.num_workers,
        save_total_limit=5,
        report_to=["none"],  # ä½¿ç”¨SwanLabå›è°ƒè€Œä¸æ˜¯å†…ç½®æŠ¥å‘Šå·¥å…·
        logging_dir=str(Path(config.model_save_path) / "logs"),
    )

    # å‡†å¤‡å›è°ƒå‡½æ•°
    callbacks = []
    
    # æ·»åŠ SwanLabå›è°ƒç”¨äºå®éªŒè·Ÿè¸ª
    if SWANLAB_AVAILABLE:
        try:
            swanlab_callback = SwanLabCallback(
                project="CAD-Recode",
                experiment_name=f"cad-recode-training-{config.base_model_name.replace('/', '-')}",
                description="CAD-Recode training with optimized strategy based on paper",
                config={
                    "model_name": config.base_model_name,
                    "batch_size": config.batch_size,
                    "learning_rate": config.learning_rate,
                    "max_training_steps": config.max_training_steps,
                    "gradient_accumulation_steps": config.gradient_accumulation_steps,
                    "n_points": config.n_points,
                    "max_seq_length": config.max_seq_length,
                    "noise_std": config.noise_std,
                    "noise_probability": config.noise_probability,
                    "mixed_precision": config.mixed_precision,
                }
            )
            callbacks.append(swanlab_callback)
            logger.info("SwanLab callback added for experiment tracking")
        except Exception as e:
            logger.warning(f"Failed to initialize SwanLab callback: {e}")
    
    # åˆ›å»º Trainer
    trainer = CADTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=callbacks,  # æ·»åŠ å›è°ƒå‡½æ•°
    )
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æŸ¥ç‚¹å¯ä»¥æ¢å¤
    checkpoint = get_last_checkpoint(config.model_save_path)
    if checkpoint:
        logger.info(f"Resuming training from checkpoint: {checkpoint}")
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("Starting training with CAD-Recode optimized strategy...")
    logger.info(f"Expected training time: ~12 hours (based on paper, single H100)")
    
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("Saving final model...")
    trainer.save_model()
    tokenizer.save_pretrained(config.model_save_path)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_path = os.path.join(config.model_save_path, "training_config.yaml")
    with open(config_path, 'w') as f:
        yaml.dump(config.__dict__, f, default_flow_style=False)
    
    logger.info("Training completed!")
    logger.info(f"Final training loss: {train_result.training_loss}")
    logger.info(f"Total training steps: {train_result.global_step}")
    
    return trainer, train_result


def test_time_sampling_inference(model, tokenizer, point_cloud: np.ndarray, 
                                config: TrainingConfig, 
                                max_new_tokens: int = 768) -> str:
    """Test-timeé‡‡æ ·ç­–ç•¥æ¨ç† - åŸºäºCAD-Recodeè®ºæ–‡"""
    if not config.use_test_time_sampling:
        # æ ‡å‡†æ¨ç†
        return single_inference(model, tokenizer, point_cloud, max_new_tokens)
    
    logger = logging.getLogger('CADRecode_Training')
    logger.info(f"Using test-time sampling with {config.num_inference_samples} samples")
    
    candidates = []
    
    # ç”Ÿæˆå¤šä¸ªå€™é€‰ä»£ç 
    for i in range(config.num_inference_samples):
        try:
            code = single_inference(model, tokenizer, point_cloud, max_new_tokens)
            if code and code.strip():
                candidates.append(code)
        except Exception as e:
            logger.warning(f"Failed to generate candidate {i}: {e}")
            continue
    
    if not candidates:
        logger.error("No valid candidates generated")
        return "r = cq.Workplane().box(1, 1, 1)"  # é»˜è®¤ä»£ç 
    
    if len(candidates) == 1:
        return candidates[0]
    
    # è¯„ä¼°å€™é€‰ä»£ç ï¼Œé€‰æ‹©å‡ ä½•è·ç¦»æœ€å°çš„
    best_code = candidates[0]
    best_score = float('inf')
    
    for code in candidates:
        try:
            # è®¡ç®—Chamferè·ç¦»
            score = evaluate_code_geometry(code, point_cloud)
            if score < best_score:
                best_score = score
                best_code = code
        except Exception as e:
            logger.warning(f"Failed to evaluate candidate: {e}")
            continue
    
    logger.info(f"Selected best candidate with score: {best_score}")
    return best_code


def single_inference(model, tokenizer, point_cloud: np.ndarray, 
                    max_new_tokens: int = 768) -> str:
    """å•æ¬¡æ¨ç†"""
    # å‡†å¤‡è¾“å…¥ (åŸºäºdemo.ipynb)
    input_ids = [tokenizer.pad_token_id] * len(point_cloud) + [tokenizer('<|im_start|>')['input_ids'][0]]
    attention_mask = [-1] * len(point_cloud) + [1]
    
    with torch.no_grad():
        batch_ids = model.generate(
            input_ids=torch.tensor(input_ids).unsqueeze(0).to(model.device),
            attention_mask=torch.tensor(attention_mask).unsqueeze(0).to(model.device),
            point_cloud=torch.tensor(point_cloud.astype(np.float32)).unsqueeze(0).to(model.device),
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=True,  # å¯ç”¨é‡‡æ ·
            temperature=0.8,  # æ§åˆ¶éšæœºæ€§
            top_p=0.9,       # nucleusé‡‡æ ·
        )
    
    # è§£ç ç”Ÿæˆçš„ä»£ç 
    py_string = tokenizer.batch_decode(batch_ids)[0]
    begin = py_string.find('<|im_start|>') + 12
    end = py_string.find('<|endoftext|>')
    py_string = py_string[begin: end]
    
    return py_string


def evaluate_code_geometry(code: str, target_point_cloud: np.ndarray, 
                          timeout: int = 5) -> float:
    """è¯„ä¼°ç”Ÿæˆä»£ç çš„å‡ ä½•è´¨é‡ï¼ˆChamferè·ç¦»ï¼‰"""
    try:
        import trimesh
        from scipy.spatial import cKDTree
        
        def target_func(code, result_queue):
            try:
                # æ‰§è¡Œä»£ç ç”ŸæˆCADæ¨¡å‹
                exec_globals = {'cq': cq}
                exec(code, exec_globals)
                
                if 'r' in exec_globals:
                    compound = exec_globals['r'].val()
                    
                    # å¯¼å‡ºä¸ºä¸´æ—¶STEPæ–‡ä»¶
                    with tempfile.NamedTemporaryFile(suffix=".step", delete=False) as tmp_file:
                        temp_step_path = tmp_file.name
                    
                    try:
                        cq.exporters.export(compound, temp_step_path)
                        
                        # è½¬æ¢ä¸ºç‚¹äº‘
                        generated_pc = step_to_point_cloud_for_eval(temp_step_path, len(target_point_cloud))
                        if generated_pc is not None:
                            # è®¡ç®—Chamferè·ç¦»
                            cd = compute_chamfer_distance(target_point_cloud, generated_pc)
                            result_queue.put((True, cd))
                        else:
                            result_queue.put((False, float('inf')))
                    finally:
                        if os.path.exists(temp_step_path):
                            os.remove(temp_step_path)
                else:
                    result_queue.put((False, float('inf')))
                    
            except Exception as e:
                result_queue.put((False, float('inf')))
        
        # ä½¿ç”¨è¿›ç¨‹æ‰§è¡Œï¼ˆé¿å…CadQueryå†…å­˜æ³„æ¼ï¼‰
        result_queue = Queue()
        process = Process(target=target_func, args=(code, result_queue))
        process.start()
        process.join(timeout)
        
        if process.is_alive():
            process.terminate()
            process.join()
            return float('inf')
        
        if not result_queue.empty():
            success, score = result_queue.get()
            return score if success else float('inf')
        
        return float('inf')
        
    except Exception as e:
        return float('inf')


def step_to_point_cloud_for_eval(step_file_path: str, n_points: int = 256) -> Optional[np.ndarray]:
    """ç”¨äºè¯„ä¼°çš„STEPè½¬ç‚¹äº‘å‡½æ•°"""
    try:
        import trimesh
        from pytorch3d.ops import sample_farthest_points
        import torch
        
        if not OCC_AVAILABLE:
            return None
        
        # ä½¿ç”¨OpenCascadeå¤„ç†STEPæ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".stl", delete=False) as tmp_file:
            temp_stl_path = tmp_file.name
        
        try:
            step_reader = STEPControl_Reader()
            status = step_reader.ReadFile(step_file_path)
            
            if status == IFSelect_RetDone:
                step_reader.TransferRoots()
                shape = step_reader.Shape()
                write_stl_file(shape, temp_stl_path)
                mesh = trimesh.load_mesh(temp_stl_path)
                
                # æ ‡å‡†åŒ–ç½‘æ ¼
                if not mesh.is_empty and mesh.vertices is not None:
                    mesh.apply_translation(-(mesh.bounds[0] + mesh.bounds[1]) / 2.0)
                    if max(mesh.extents) > 0:
                        mesh.apply_scale(2.0 / max(mesh.extents))
                    
                    # é‡‡æ ·ç‚¹äº‘
                    vertices, _ = trimesh.sample.sample_surface(mesh, 8192)
                    if len(vertices) >= n_points:
                        vertices_tensor = torch.tensor(vertices, dtype=torch.float32).unsqueeze(0)
                        _, ids = sample_farthest_points(vertices_tensor, K=n_points)
                        ids = ids[0].numpy()
                        return vertices[ids].astype(np.float32)
            
            return None
            
        finally:
            if os.path.exists(temp_stl_path):
                os.remove(temp_stl_path)
                
    except Exception as e:
        return None


def compute_chamfer_distance(pc1: np.ndarray, pc2: np.ndarray) -> float:
    """è®¡ç®—ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´çš„Chamferè·ç¦»"""
    try:
        from scipy.spatial import cKDTree
        
        # ç¡®ä¿ç‚¹äº‘å½¢çŠ¶ä¸€è‡´
        if pc1.shape[0] != pc2.shape[0]:
            min_points = min(pc1.shape[0], pc2.shape[0])
            if pc1.shape[0] > min_points:
                indices = np.random.choice(pc1.shape[0], min_points, replace=False)
                pc1 = pc1[indices]
            if pc2.shape[0] > min_points:
                indices = np.random.choice(pc2.shape[0], min_points, replace=False)
                pc2 = pc2[indices]
        
        # è®¡ç®—Chamferè·ç¦»
        tree1 = cKDTree(pc1)
        tree2 = cKDTree(pc2)
        
        dist1, _ = tree1.query(pc2, k=1)
        dist2, _ = tree2.query(pc1, k=1)
        
        chamfer_dist = np.mean(np.square(dist1)) + np.mean(np.square(dist2))
        return float(chamfer_dist)
        
    except Exception as e:
        return float('inf')


def create_optimized_model_with_inference(config: TrainingConfig):
    """åˆ›å»ºæ”¯æŒä¼˜åŒ–æ¨ç†çš„æ¨¡å‹"""
    model, tokenizer = create_model_and_tokenizer(config)
    
    # æ·»åŠ æ¨ç†æ–¹æ³•åˆ°æ¨¡å‹
    def inference_method(point_cloud: np.ndarray, max_new_tokens: int = 768) -> str:
        return test_time_sampling_inference(model, tokenizer, point_cloud, config, max_new_tokens)
    
    model.optimized_inference = inference_method
    return model, tokenizer


def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨ä»¥ä¼˜é›…å¤„ç†å´©æºƒ"""
    def signal_handler(signum, frame):
        if signum == signal.SIGSEGV:
            print(f"\n[CRITICAL] Segmentation fault detected! Attempting graceful shutdown...")
            logger = logging.getLogger('CADRecode_Training')
            logger.error(f"Segmentation fault detected at frame: {frame}")
            
            # å°è¯•ä¿å­˜å½“å‰çŠ¶æ€
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("Cleared CUDA cache")
            except:
                pass
            
            # è®°å½•é”™è¯¯ä¿¡æ¯
            logger.error("Training terminated due to segmentation fault")
            sys.exit(1)
        elif signum == signal.SIGINT:
            print(f"\n[INFO] Received interrupt signal, shutting down gracefully...")
            sys.exit(0)
        elif signum == signal.SIGTERM:
            print(f"\n[INFO] Received termination signal, shutting down gracefully...")
            sys.exit(0)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGSEGV, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)


def main():
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    parser = argparse.ArgumentParser(description="CAD-Recodeè®­ç»ƒè„šæœ¬")
    parser.add_argument("--config", type=str, default=None,
                       help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„ (YAMLæ ¼å¼)")
    parser.add_argument("--train_data_path", type=str, default="./data/train",
                       help="è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--val_data_path", type=str, default="./data/val",
                       help="éªŒè¯æ•°æ®è·¯å¾„")
    parser.add_argument("--model_save_path", type=str, default="./checkpoints",
                       help="æ¨¡å‹ä¿å­˜è·¯å¾„")
    parser.add_argument("--base_model_name", type=str, default="Qwen/Qwen2-1.5B",
                       help="åŸºç¡€æ¨¡å‹åç§°")
    parser.add_argument("--num_epochs", type=int, default=10,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = setup_logger()
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    if args.config and os.path.exists(args.config):
        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        with open(args.config, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        # è¿‡æ»¤æ‰ä¸å±äºTrainingConfigçš„å­—æ®µ
        from dataclasses import fields
        valid_fields = {field.name for field in fields(TrainingConfig)}
        filtered_config = {k: v for k, v in config_dict.items() if k in valid_fields}
        
        logger.info(f"Loaded config with fields: {list(filtered_config.keys())}")
        config = TrainingConfig(**filtered_config)
    else:
        # ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»º
        config = TrainingConfig(
            train_data_path=args.train_data_path,
            val_data_path=args.val_data_path,
            model_save_path=args.model_save_path,
            base_model_name=args.base_model_name,
            num_epochs=args.num_epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            device=args.device
        )
    
    # è®­ç»ƒæ¨¡å‹
    try:
        # è®¾ç½®éšæœºç§å­
        set_seed(config.seed)
        
        # è®¾ç½®è®¾å¤‡ä¿¡æ¯
        if torch.cuda.is_available():
            logger.info(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            torch.cuda.empty_cache()
        
        # å¼€å§‹è®­ç»ƒ
        logger.info("Starting CAD-Recode training with robust error handling...")
        
        trainer, train_result = train_model(config)
        
        # æ˜¾ç¤ºè®­ç»ƒå®Œæˆä¿¡æ¯
        print("\n" + "="*60)
        print("TRAINING COMPLETED SUCCESSFULLY!")
        print("="*60)
        print(f"Final model saved to: {config.model_save_path}")
        print(f"Total training steps: {train_result.global_step}")
        print(f"Final training loss: {train_result.training_loss:.4f}")
        print("\nTo run inference, use:")
        print("python demo_optimized_strategy.py --model_path <path_to_model>")
        
    except KeyboardInterrupt:
        logger.warning("Training interrupted by user")
        print("\n[INFO] Training interrupted by user. Cleaning up...")
        
        # æ¸…ç†CUDAç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("Cleared CUDA cache")
        
        return 1
        
    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            logger.error(f"CUDA out of memory error: {e}")
            print("\n[ERROR] GPU out of memory! Try reducing batch_size or max_seq_length")
            
            # æ¸…ç†CUDAç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info("Cleared CUDA cache")
            
        else:
            logger.error(f"Runtime error during training: {e}")
            print(f"\n[ERROR] Runtime error: {e}")
            import traceback
            traceback.print_exc()
        
        return 1
        
    except Exception as e:
        logger.error(f"Unexpected error during training: {e}")
        print(f"\n[ERROR] Training failed: {e}")
        import traceback
        traceback.print_exc()
        
        # æ¸…ç†CUDAç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("Cleared CUDA cache")
        
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
